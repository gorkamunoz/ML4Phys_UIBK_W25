[
  {
    "objectID": "course/linear_models/logistic_regression.html#accuracy",
    "href": "course/linear_models/logistic_regression.html#accuracy",
    "title": "Logistic regression",
    "section": "7.1 Accuracy",
    "text": "7.1 Accuracy\nThe most typical, and perhaps natural metric when dealing with classification problems is the accuracy, which we define as:\n\\[ \\text{acc}= \\frac{\\# \\text{correct predictions}}{\\#\\text{dataset}},\\]\ni.e.¬†the ratio between the number of correct predictions and the number of elements in our training set. Compared to the BCE, a value of this metric is quite easy to undestand: close to 0 is really bad, close to 1 is really good.\n\nveca, vecb, vecl = np.array(trackers['a']), np.array(trackers['b']), np.array(trackers['loss'])\nmask = np.arange(0,len(veca),100)\nveca, vecb, vecl = veca [mask], vecb[mask], vecl[mask]\n\n\n\n\n\n\n\nExercise\n\n\n\nWrite the function accuracy(x,y,a,b), which returns the accuracy for a given dataset and for the parameters \\(a\\) and \\(b\\). Choose the label with the following rule: \\(0\\) if \\(\\sigma_i<0.5\\) else \\(1\\).\n\n\n\n### Your Code Here!\ndef accuracy(x,y,a,b):\n    \n    return acc\n\nNow that we can calculate the accuracy, let‚Äôs track it through training. Let‚Äôs us the save veca and vecb from above to avoid retraining:\n\nvec_acc = np.zeros_like(veca)\n\nfor i,(a,b) in enumerate(zip(veca,vecb)):\n    vec_acc[i] = accuracy(x,y,a,b)\n\nFigure¬†5 shows the value of the Loss function and the accuracy during the training. It is interesting to note that while the Loss function is a smooth function, the accuracy is not smooth and that the variations of Loss function do not directly correspond to the variations of the accuracy. It is therefore important to check not only the Loss function but also the figure of merit of the problem, that can be for example the accuracy.\n\n\nFigure code\n# Create figure with secondary y-axis\nfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\n# Add traces\nfig.add_trace(\n    go.Scatter(y=vecl, name=\"Loss\"),\n    secondary_y=False,\n)\n\nfig.add_trace(\n    go.Scatter(y=vec_acc, name=\"Accuracy\"),\n    secondary_y=True,\n)\n\n\n# Set x-axis title\nfig.update_xaxes(title_text=\"iterations\")\n\n# Set y-axes titles\nfig.update_yaxes(title_text=\"Loss\", secondary_y=False)\nfig.update_yaxes(title_text=\"Accuracy\", secondary_y=True)\n\nfig.show()\n\n\n\n\n                                                    \nFigure¬†5: Loss function vs Accuracy"
  },
  {
    "objectID": "course/linear_models/logistic_regression.html#confusion-matrix",
    "href": "course/linear_models/logistic_regression.html#confusion-matrix",
    "title": "Logistic regression",
    "section": "7.2 Confusion matrix",
    "text": "7.2 Confusion matrix\nWhile the accuracy is quite understandable, it may not give us enough information to understand why our model is actually failing. For instance, why is the previous accuracy not 1? Where is our model making a mistake? The confusion matrix is a very handy method for this. Before introducing it, we need to introduce few terms, the following table helps us for that:\n\n\n\n\nPositive (Prediction)\nNegative (Prediction)\n\n\n\n\nPositive (Ground Truth)\nTrue Positive (TP)\nFalse negative (FN)\n\n\nNegative (Ground Truth)\nFalse positive (FP)\nTrue Negative (TN)\n\n\n\nNow the confusion matrix is just the previous table, but in matrix form:\n\\[\nC = \\begin{bmatrix}\n\\text{TP} & \\text{FN} \\\\\n\\text{FP} & \\text{TN}\n\\end{bmatrix}\n\\]\nLet us now construct the confusion matrix for our model.\n\nyp = sigmoid(x, a, b)\nyp[yp>0.5] = 1\nyp[yp<=0.5] = 0\ncm = confusion_matrix(y,yp)\n\nFigure¬†6 shows the confustion matrix for the logistic regression.\n\n\nFigure code\nX, Y = [\"Alive(P)\", \"Dead(P)\"], [\"Alive(GT)\", \"Dead(GT)\"]\nfig = px.imshow(cm, x=X, y=Y, text_auto=True,color_continuous_scale='Blues')\n\n\nfig.show()\n\n\n\n\n                                                    \nFigure¬†6: Confusion matrix for the logistic regression\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nHow do you interpret the previous confusion metrics? What is the model doing wrong?\n\n\nWe readily observe that the accuracy correponds to the trace of this matrix normalized by the number of individuals.\n\nprint(f'{np.trace(cm)/np.sum(cm):.3f}')\n\n0.714"
  },
  {
    "objectID": "course/linear_models/logistic_regression.html#other-metrics",
    "href": "course/linear_models/logistic_regression.html#other-metrics",
    "title": "Logistic regression",
    "section": "7.3 Other metrics",
    "text": "7.3 Other metrics\nThere exist many other metrics that can be calculated from the previous. In particular, the following are commonly used:\n\n\n\n\n\n\n\n\nMetric\nFormula\nIntuition\n\n\n\n\nPrecision\n\\(\\frac{TP}{TP+FP}\\)\nMeasures how many of the predicted positives are truly positive (focuses on correctness)\n\n\nRecall\n\\(\\frac{TP}{TP+FN}\\)\nMeasures how many of the actual positives are correctly detected (focuses on completeness)\n\n\nF1 score\n\\(\\frac{2\\text{Recall}\\,\\text{Precision}}{\\text{Recall}+\\text{Precision}}\\)\nBalances both by taking their harmonic mean, rewarding models that are both precise and sensitive\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCompute the different metrics for the trained logistic regression model. Bonus: compute it as a function of the training epochs.\n\n\n\n### Your Code Here!\n\n\n\nFigure code\n# Create figure with secondary y-axis\nfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\n# Add traces\nfig.add_trace(\n    go.Scatter(y=vecl, name=\"Loss\"),\n    secondary_y=False,\n)\n\nfig.add_trace(\n    go.Scatter(y=vec_prec, name=\"Precision\", visible = 'legendonly'),\n    secondary_y=True,\n)\n\nfig.add_trace(\n    go.Scatter(y=vec_rec, name=\"Recall\", visible = 'legendonly'),\n    secondary_y=True,\n)\n\nfig.add_trace(\n    go.Scatter(y=vec_f1, name=\"F1-score\", visible = 'legendonly'),\n    secondary_y=True,\n)\n\n\n# Set x-axis title\nfig.update_xaxes(title_text=\"iterations\")\n\n# Set y-axes titles\nfig.update_yaxes(title_text=\"Loss\", secondary_y=False)\nfig.update_yaxes(title_text=\"Accuracy\", secondary_y=True)\n\nfig.show()\n\n\n\n\n                                                    \nFigure¬†7: Loss function vs other metrics\n\n\n\nLet us perform the same analysis on another dataset, the brest cancer dataset of scikit learn.\n\nx1, y1 =load_breast_cancer(return_X_y=True)\nx1 = x1[:,3]\n\nprint(x1[-10:])\nprint(y1[-10:])\n\n[ 403.5  600.4  386.   716.9 1347.  1479.  1261.   858.1 1265.   181. ]\n[1 1 1 0 0 0 0 0 0 1]\n\n\n\nx1 = x1.reshape([np.size(x1),1])\n\nclf = LogisticRegression().fit(x1,y1)\nyp1 = clf.predict(x1)\n\ncm = confusion_matrix(y1,yp1)\n\n\n\nFigure code\nX1, Y1 = [\"Malignous(P)\", \"Benign(P)\"], [\"Malignous(GT)\", \"Benign(GT)\"]\nfig = px.imshow(cm, x=X1, y=Y1, text_auto=True,color_continuous_scale='Blues')\n\n\nfig.show()\n\n\n\n\n                                                    \nFigure¬†8: Confusion matrix for the logistic regression of the brest cancer\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCompute the different metrics for the trained logistic regression model."
  },
  {
    "objectID": "course/linear_models/linear_regression.html",
    "href": "course/linear_models/linear_regression.html",
    "title": "Linear regression",
    "section": "",
    "text": "1 The task\nIn this introductory notebook, we discuss our first learning algorithm to perform a regression task. Given a dataset \\(\\{\\mathbf{x},\\mathbf{y}\\}\\) of \\(n\\) points, we would like to find the line \\(y'=\\mathbf{w}^{T} \\mathbf{x} + \\mathbf{b}\\) that best fits the data. Therefore, let us start by generating such a dataset for the one dimnesional case. We do so by taking the line \\(y=a^*x+b^*\\) and adding gaussian noise to \\(y\\). We have prepared a small package lectures_ml with functionalities to do these tasks easily.\n\na_true, b_true = 1.5, 1 \nx, y = noisy_line(a_true, b_true, noise=[0,2])\n\n\n\n\n\n\n\nDocumentation and source code\n\n\n\nYou can access the documentation of any function by pressing the tab key or by adding a ? after the function. You can also see the source code by adding ?? after the function. If you want them to appear in a cell of the notebook, you can use the function nbdev.showdoc() for the documentation and lectures_ml.utils.show_code().\n\n\nFigure¬†1 shows the dataset \\(\\{x,y\\}\\). As expected, the dataset follows the linear relation dispersion (in red) but with some noise.\n\n\n\nCode\nfig = go.Figure()\nfig.add_scatter(x=x, y=y, mode=\"markers\", name='data',\n                hovertemplate='x:%{x:.2f}'\n                +'<br>y:%{y:.2f}</br><extra></extra>')\nx1 = np.array([x.min(),x.max()])\ny1 = a_true*x1+b_true\nfig.add_scatter(x=x1, y=y1, mode=\"lines\",name='Ground Truth')\nfig.update_layout(width=800,height=400,xaxis={'title':'x'},yaxis={'title':'y'})\nfig.show()\n\n\n\n\n        \n        \n        \n(a) Line with Gaussian noise\n\n\n\n\n\n                                                    \n(b) ?(caption)\n\n\nFigure¬†1: ?(caption)\n\n\n\n\n2 Learning as an optimization problem\nThe goal of the learning task is to find the slope and the intercept of the line directly from the data. Therefore, we have to define a suitable model to solve the task with the given data. In general, the model is a function of the input data, \\(f(\\mathbf{x})\\), whose output is interpreted as a prediction for the input data. We start by declaring a certain parametrization of a model (function), e.g., \\(f(\\mathbf{x}) = \\mathbf{w}^{T} \\mathbf{x} + \\mathbf{b}\\), with \\(\\theta \\supset \\{\\mathbf{w}, \\mathbf{b}\\}\\) denoting the model parameters. Then, all possible parametrizations of this function form the set of functions, i.e., the hypothesis class. Given that both \\(x\\) and \\(y\\) are one-dimensional in our example, let‚Äôs consider \\(f_\\theta(\\mathbf{x}) = a x + b\\) where \\(a\\) and \\(b\\) are real numbers too.\n\n\n\n\n\n\nImportant\n\n\n\nMachines ‚Äò‚Äôlearn‚Äô‚Äô by minimizing a loss function of the training data, i.e., all the data accessible to the ML model during the learning process. The minimization is done by tuning the parameters of the model. We need to choose the loss function according to the objective task, although there is certain freedom on how to do it. In general, the loss function compares the model predictions or a developed solution against the reality or expectations. Therefore, learning becomes an optimization problem.\n\n\nHere, we use the terms of loss, error, and cost functions 1 interchangeably following Ref. (Goodfellow, Bengio, and Courville 2016). Popular examples of loss functions include the mean square error and the cross entropy, used for supervised regression and classification 2 problems.\n\n\n3 The loss function: Mean square error\nHaving a model, we now have to define a loss function for our regression task. For this case, we choose the mean square error, defined as \\[MSE=\\frac{1}{N}\\sum_{i=1}^{N}(y_i'-y_i)^2.\\] Such a loss measures the mean vertical distance between the dataset and the line \\(y'=w_1 x + w_0\\) (see Figure¬†2).\n\n\n\nFigure¬†2: Mean Square error\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere is not a unique loss function suitable for our task. We could have chosen other losses such as, e.g., the Mean Absolute Error (MAE) or the Root Mean Squared Error (RMSE). The choice of the loss really depends on the problem and the dataset.\n\n\nLet us now study the loss function in terms of its two parameters \\(\\{a,b\\}\\) for our dataset \\(\\{x,y\\}\\). Figure¬†3 shows the contour plot of the logarithm of loss function in terms of \\(a\\) and \\(b\\). We can clearly see that the minimum appears at the expected values of the line we generated in the previous section.\n\n\nCode generating the data of the figure\nvec_a = np.arange(-5,5,0.1)\nvec_b = np.arange(-5,5,0.1)\nmatz, matzg = np.zeros((vec_a.size,vec_b.size)), np.zeros((vec_a.size,vec_b.size,2))\nvec = np.zeros((vec_a.size*vec_b.size,3))\n\nfor i, a1 in enumerate(vec_a):\n    for j, b1 in enumerate(vec_b):\n        matz[i,j] = MSE(x,y,lambda x:a1*x+b1)\n        matzg[i,j,:] = grad_MSE_lr(x,y,dict(a=a1,b=b1))\n\n\n\n\nCode\nfig = go.Figure()\n\nfig.add_contour(z=np.log(matz),x=vec_b, y=vec_a,hovertemplate=\n                    'a:%{y:.2f}'\n                    +'<br>b:%{x:.2f}</br>'\n                    +'f:%{z:.2f}<extra></extra>')\nfig.add_scatter(x=[b_true],y=[a_true], marker_color='White')\n\n\nd = dict(width=600,\n         height=600,\n         xaxis={'title':'b'},\n         yaxis={'title':'a'}\n       )\n\nfig.update_layout(d)\nfig.show()\n\n\n\n\n                                                    \nFigure¬†3: \\(MSE(a,b)\\)\n\n\n\n\n\n4 Finding the minimum of the loss function\nIn the case of the mean square error, we can derive analytically the optimal values of \\(a\\) and \\(b\\). To this end, we start by writing the gradients \\[\n\\begin{align}\n&\\partial_a MSE=\\frac{2}{N}\\sum_{i=1}^{N}(y_i'-y_i)x_i\\\\\n&\\partial_b MSE=\\frac{2}{N}\\sum_{i=1}^{N}(y_i'-y_i).\n\\end{align}\n\\]\nThis leads to the linear system of equations for \\(a\\) and \\(b\\) when the gradients vanish \\[\n\\begin{align}\n&a \\sum_{i=1}^N x_i^2+b \\sum_{i=1}^N x_i - \\sum_{i=1}^N y_i x_i =0\\\\\n&a \\sum_{i=1}^N x_i+b N -\\sum_{i=1}^N y_i =0\n\\end{align}\n\\]\nWe can easily solve this system of equation to find\n\\[\n\\begin{align}\n& b = \\bar{y} - a \\bar{x}\\\\\n& a = \\frac{\\sum_{i=1}^N (x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^N(x_i-\\bar{x})^2},\n\\end{align}\n\\] where \\(\\bar{x}\\) (\\(\\bar{y}\\)) stands for the mean of \\(x\\) (\\(y\\)). As this problem is convex, we have found the unique global minimum.\n\n\n\n\n\n\nExercise\n\n\n\nImplement a function linear_regression_analytic(x,y) to compute the analytical optimal values for the slope and intercept given a dataset with samples x and y, such as the one we have created above.\n\n\n\n### Your Code Here!\ndef linear_regression_analytic(x,y):\n    pass\n\n\nestimate_a, estimate_b = linear_regression_analytic(x,y)\n\nprint(f'a={estimate_a:.3f}\\nb={estimate_b:.3f}')\n\nWe have just performed our first learning task!\n\n\n5 Gradient Descent\nIn general, we do not have a tractable closed expression for the optimal parameters and we need to solve the optimization task through other means. Here, we introduce gradient-based approaches, which, despite not being needed for this task, it will allow us to introduce important concepts that will appear in a more abstract form in neural networks.\nLet us first study the gradients. Figure¬†4 shows the gradients of the MSE with respect to \\(a\\) and \\(b\\). The values of \\(a\\) and \\(b\\) of the line lie in the zero contour lines of the gradients.\n\n\nCode\nfor i in range(2):\n    mat = matzg[:, :, i]\n    vmax = np.abs(mat).max()  # symmetric range around 0\n\n    fig = go.Figure()\n    fig.add_contour(\n        z=mat,\n        x=vec_b,\n        y=vec_a,\n        colorscale='RdBu',   # diverging colormap centered on zero\n        zmin=-vmax,\n        zmax=vmax,\n        colorbar_title=\"Value\"\n    )\n\n    fig.add_scatter(x=[b_true], y=[a_true], marker_color='white')\n    fig.update_layout(\n        xaxis_title='b',\n        yaxis_title='a'\n    )\n    fig.show()\n\n\n\n\n\n\n\n                                                    \n(a) \\(\\partial_a MSE(a,b)\\)\n\n\n\n\n\n                                                    \n(b) \\(\\partial_b MSE(a,b)\\)\n\n\n\nFigure¬†4: Gradient of \\(MSE(a,b)\\)\n\n\n\nWe can now perform a gradient optimization. The simplest one is the gradient descent algorithm (often called steepest descent algorithm). This iterative algorithms works as follows:\n\n\n\n\n\n\nPseudocode\n\n\n\n\nChoose an initial condtion for the paramaters: \\(a_0\\) and \\(b_0\\)\nChoose a step size \\(\\eta\\)\nRepeat:\n\nCompute the gradients \\(\\partial_a MSE\\) and \\(\\partial_b MSE\\)\nUpdate the parameters in the opposite direction of the gradient \\[\\begin{aligned}\n&a_{i+1}=a_i-\\eta \\, \\partial_a MSE\\\\\n&b_{i+1}=b_i-\\eta \\, \\partial_b MSE\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nImplement the previous pseucode code to find the minimum of \\(f(x)=x^2\\). This convex function has a unique global minimum at \\(x=0\\) and we can compute its gradient analitically.\n\n\n\n# Here are the functions we will use\ndef f(x): return x**2\ndef grad_f(x): return 2*x\n\nGiven the initial \\(x_0\\), perform perform n_iter iterations of the gradient descent algorithm.\n\n### Your Code Here!\ndef gd_step(x0, grad_func):\n    pass\n\n\n\nCode\n# Solution\ndef gd_step(x0, grad_func):\n    x1 = x0 - eta* grad_func(x0)\n    return x1\n\n\nOnce you have your gradient step ready, put it to the text by creating a loop that performs the pseudocode higher up. Keep track of the values of \\(x\\) and \\(f(x)\\) to see how they evolve. Do 20 iterations of GD.\n\n#### Your Code Here!\n\nLet us now come back to our linear regression problem. We consider n_ini random initial values for our parameters and run the gradient descent algortihm. Rather than writing the whole algorithm again, we use the gradient_descent function from the lectures_ml library.\n\nn_ini = 5\nveca0 = np.random.uniform(low=vec_a[1], high=vec_a[-2], size=n_ini)\nvecb0 = np.random.uniform(low=vec_b[1], high=vec_b[-2], size=n_ini)\n\nll = dict(loss=MSE, grads=grad_MSE_lr, fun=line)\n\ndf = pd.DataFrame(columns=['a','b','label','value'])\nfor i in range(n_ini):\n    pini = dict(a=veca0[i],b=vecb0[i])\n    trackers = gradient_descent(x, y, pini, ll, niter=int(1E4), eta=1E-3)\n    df1 = pd.DataFrame(data={'a':trackers['a'],'b':trackers['b'],'label':f'traj {i+1}','value':trackers['loss']})\n    df = pd.concat([d.dropna(axis=1, how=\"all\") for d in (df, df1)])\n\nFigure¬†5 depicts the loss functions in terms of the epochs for the different trajectories. The initial value of the loss function strongly varies depending on the initial conditions.However, we observe that the steepest descent algorithm drives rapidly the parameters towards the minimum.\n\n\nCode\nfig = px.scatter(df, y='value',animation_frame='label')\n\nfig[\"layout\"].pop(\"updatemenus\") # optional, drop animation buttons\nfig.update_layout(xaxis_title='epochs',yaxis_title='Loss')\nfig.show()\n\n\n\n\n                                                    \nFigure¬†5: Loss function for the different initial conditions\n\n\n\nIn ML it is usually much illustrative to see the evolution of the loss function in a log-scale:\n\n\nCode\nfig = px.scatter(df, y='value',animation_frame='label')\n\nfig[\"layout\"].pop(\"updatemenus\") # optional, drop animation buttons\nfig.update_layout(xaxis_title='epochs',yaxis_title='Loss',\n                 yaxis_type='log', xaxis_type='log' )\nfig.show()\n\n\n\n\n                                                    \nFigure¬†6: Loss function for the different initial conditions\n\n\n\nFigure¬†7 shows the trajectories in the parameter space.\n\n\nCode\nfig = go.Figure()\nfig.add_contour(z=np.log(matz),x=vec_b, y=vec_a,\n                hovertemplate=\n                    'a:%{y:.2f}'\n                    +'<br>b:%{x:.2f}</br>'\n                    +'f:%{z:.2f}<extra></extra>')\n\nhovertemplate ='a:%{a}'+'b:%{b}<extra></extra>'\nfor i in range(n_ini):\n    visible = True if i == 0 else 'legendonly'\n    newdf = df[df.label == f'traj {i+1}']\n    fig.add_scatter(x=newdf.b, y=newdf.a, name=f'traj {i+1}',text=newdf.value,\n                    hovertemplate=\n                    'a:%{y:.2f}'\n                    +'<br>b:%{x:.2f}</br>'\n                    +'f:%{text:.2f}<extra></extra>', visible=visible)\n    \nlegend=dict(\n    yanchor=\"top\",\n    y=1.3,\n    xanchor=\"left\",\n    x=0.1\n    )\nd = dict(width=800,\n         height=600,\n         xaxis={'title':'b'},\n         yaxis={'title':'a'},\n         legend = legend\n        )\n\nfig.update_layout(d)\nfig.show()\n\n\n\n\n                                                    \nFigure¬†7: Gradient descent for \\(n_{ini}\\) initial conditions \\(a_0\\) and \\(b_0\\).\n\n\n\n\n\n6 Choosing a Learning rate\nChoosing a learning rate has an impact on convergence to the minimum, as depicted in Figure¬†8.\n\nIf the learning rate is too small, the training needs many epochs.\nThe right learning rate allows for a fast convergence to a minimum and needs to be found.\nIf the learning rate is too large, optimization can take you away from the minimum (you ``overshoot‚Äô‚Äô).\n\n\n\n\nFigure¬†8: Choice of the learning rate\n\n\nLet us first illustrate the latter on the parabola example.\n\ntreshold = 1E-6 # Minimum difference between f_t and f_t+1 at which we stop the iterations\nimax = int(1E4) # Maximum number of iterations\n\n# Initial guess\nx0 = 2\n\n# Learning rate\neta = 1E-3\n\n# Saving the info\nvecx, vecf = [x0], [f(x0)]\n\nx1=x0\ni = 0\ndl = 10\nwhile dl>treshold and i<imax:\n    i = i+1\n    x1 =  x1 - eta* grad_f(x1)\n    vecx.append(x1)\n    vecf.append(f(x1))\n    dl = np.abs(vecf[-1]-vecf[-2])\n    if vecf[-1]>1000.: break\n\n\n\nCode\nfig = go.Figure()\n\nx1 = np.arange(-2.5,2.51,0.01)\ny1 = x1**2\nfig.add_scatter(x=x1, y=y1, mode=\"lines\",name='Parabola',marker_color='#EF553B')\n\n\nfig.add_scatter(x=vecx, y=vecf, mode=\"lines+markers\", name='GD',\n                hovertemplate='x:%{x:.2f}'\n                +'<br>y:%{y:.2f}</br><extra></extra>',marker_color='#636EFA',marker_size=8)\n\nfig.update_layout(width=800,height=400,xaxis={'title':'x'},yaxis={'title':'f(x)'},title=f'number of iterations to reach the threshold {treshold:.0e}: {i}')\nfig.show()\n\n\n\n\n                                                    \nFigure¬†9: Gradient descent on a parabola\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nRerun the last experiment for \\(\\eta=10^{-3},10^{-1},1.1\\). What do you see?\n\n\nWe now perform a similar analysis for the linear regression problem. To this end, we choose a vector of learning rates vec_eta for the same initial condition and we apply the steepest descent algorithm.\n\n\nCode\nvec_eta = [1E-4,1E-3,1E-2,2E-2,3E-2,5E-2,1E-1]\nn_ini = len(vec_eta)\n\npini = dict(a=-1.8, b=1)\n\ndf = pd.DataFrame(columns=['a','b','label','value'])\n\nfor i in range(n_ini):\n    trackers = gradient_descent(x, y, pini, ll, niter=int(1E4),eta=vec_eta[i])\n    df1 = pd.DataFrame(data={'a':trackers['a'],'b':trackers['b'],'label':f'traj {i+1}','eta':vec_eta[i],'value':trackers['loss']})\n    df = pd.concat([d.dropna(axis=1, how=\"all\") for d in (df, df1)])\n\n\n\n\nCode\nfig = go.Figure()\nfig.add_contour(z=np.log(matz),x=vec_b, y=vec_a,\n                hovertemplate=\n                    'a:%{y:.2f}'\n                    +'<br>b:%{x:.2f}</br>'\n                    +'f:%{z:.2f}<extra></extra>')\n\nhovertemplate ='a:%{a}'+'b:%{b}<extra></extra>'\nfor i in range(n_ini):\n    visible = 'legendonly'\n    newdf = df[df.label == f'traj {i+1}']\n    fig.add_scatter(x=newdf.b, y=newdf.a, name=f'eta = {vec_eta[i]}',text=newdf.value,\n                    hovertemplate=\n                    'a:%{y:.2f}'\n                    +'<br>b:%{x:.2f}</br>'\n                    +'f:%{text:.2f}<extra></extra>',\n                    visible=visible)\n    \nlegend=dict(\n    yanchor=\"top\",\n    y=1.3,\n    xanchor=\"left\",\n    x=0.01\n    )\nd = dict(width=800,\n         height=600,\n         xaxis={'title':'b'},\n         yaxis={'title':'a'},\n         legend = legend,\n         xaxis_range=[vec_b[1], vec_b[-1]],       \n         yaxis_range=[vec_a[1], vec_a[-1]]\n        )\n\nfig.update_layout(d)\nfig.show()\n\n\n\n\n                                                    \nFigure¬†10: Effect of the choice of the learning rate\n\n\n\n\n\n7 Non-convex problems\nFor convex cases as the one above, the gradient descent algorithm is guaranteed to converge to the global minimum for sufficiently small \\(\\eta\\). For non-convex problems, it can instead get stuck on local minima. Indeed, in practical ML trainings, we hardly ever reach the global optimum, but it is usually sufficient to reach a local one that is close enough. Let‚Äôs see a visual example of this:\n\ndef f_nc(x): \n    return (x+1)**2*(x-2)**2 + 2*x\n\ndef grad_f_nc(x): \n    return 2*(x+1)*(x-2)*(2*x-1) + 0.2\n\nWe now proceed to do the same descent from two different points in the parameter space:\n\nn_iter = 20\neta = 1E-2\n\n# Point one: converges to local minima\nx0 = 2.5\nvecx = np.zeros(n_iter+1)\nvecf = np.zeros(n_iter+1)\n\nvecx[0] = x0\nvecf[0] = f_nc(x0)\n\nfor i in np.arange(n_iter):\n    vecx[i+1] = gd_step(vecx[i], grad_f_nc)  \n    vecf[i+1] = f_nc(vecx[i+1])\n\n# Point two: converges to global minima\nx0 = -1.4\nvecx_div = np.zeros(n_iter+1)\nvecf_div = np.zeros(n_iter+1)\n\nvecx_div[0] = x0\nvecf_div[0] = f_nc(x0)\n\nfor i in np.arange(n_iter):\n    vecx_div[i+1] = gd_step(vecx_div[i], grad_f_nc) \n    vecf_div[i+1] = f_nc(vecx_div[i+1])\n\n\n\nCode\nfig = go.Figure()\n\nx1 = np.arange(-2,3,0.01)\ny1 = f_nc(x1)\nfig.add_scatter(x=x1, y=y1, mode=\"lines\",name='Parabola',marker_color='#EF553B')\n\n\nfig.add_scatter(x=vecx, y=vecf, mode=\"markers\", name='GD local minima',\n                hovertemplate='x:%{x:.2f}'\n                +'<br>y:%{y:.2f}</br><extra></extra>',marker_color='#636EFA',marker_size=8)\n\nfig.add_scatter(x=vecx_div, y=vecf_div, mode=\"markers\", name='GD true minima',\n                hovertemplate='x:%{x:.2f}'\n                +'<br>y:%{y:.2f}</br><extra></extra>',marker_color='#2ECC71',marker_size=8)\n\nfig.update_layout(width=800,height=400,xaxis={'title':'x'},yaxis={'title':'f(x)'})\nfig.show()\n\n\n\n\n                                                    \nFigure¬†11: Gradient descent on a parabola\n\n\n\nThis showcases the importance, in non-convex cases, which are most of ML cases, to perform multiple random initializations of our model / training, because:\n\nWe may have not found the correct solution because of an ‚Äúunlucky‚Äù start.\nWe may have found the correct solution by luck, and restarting the training does not find it again. We refer here then to the ‚Äúrobustness‚Äù of the model. A robust model can function under any conditions.\n\n\n\n8 Stochastic Gradient Descent\nThe gradient descent algorithm requires to pass through the whole training set to compute the gradient. However, in some cases, this can be quite costly. Imagine, for example, the case of linear regression with many variables and many training examples. To overcome this limitation, computer scientists have designed a stochastic alternative to gradient descent: the stochastic gradient descent (SGD).\n\n\n\n\n\n\nNote\n\n\n\nWhile stochastic gradient descent is not very relevant for the case of the linear regression with two parameters, it will become very important in the case of neural networks. We here take the simplicity of the loss landscape of such model to illustrate the main properties of stochastic gradient descent.\n\n\nThe main idea behind stochastic gradient descent is to approximate the loss function of the training set by the gradient of a single or just few training samples. While, each gradient step is a relatively bad approximation, the random walk followed by the aglorithm eventually converges to the direction of the steepest descent. This can be intuitively seen by noting that the mean of the gradient of several training points is pointing towards the steepest descent.\nWe now have two extreme cases: the gradient descent algorithm with no stochasticiy and the stochastic gradient descent with full stochasticity. This version of the stochastic gradient descent can be very unstable and take extremely long times to converge. Thus, it is desirable to find a middle ground: minibacth gradient descent. In this case, rather than taking the gradient over a single training example, we consider a batch size \\(BS\\), i.e.¬†the number of training samples in the stochastic gradient descent loop. This way, we obtain a better estimate of the gradient while preserving some of its stochasticity.\nThe pseudocode looks like:\n\n\n\n\n\n\nPseudocode\n\n\n\n\nChoose an initial condtion for the paramaters: ùëé0 and ùëè0\nChoose a learning rate \\(\\eta\\) and batch size \\(BS\\)\nRepeat until convergence:\n\nShuffle the training set\nIterate over every batch:\n\nIterate over every sample in batch Compute loss for every sample\nCompute gradient based on average loss in batch\nUpdate params as in GD\n\\[\\begin{aligned}\n      &a_{i+1}=a_i-\\eta \\, \\partial_a MSE\\\\\n      &b_{i+1}=b_i-\\eta \\, \\partial_b MSE\n  \\end{aligned}\\]\n\n\n\n\n\nWe illustrate the stochastic gradient descent with the following code snippet for the same initial condition and for a minibatch of size BS=20.\n\nn_ini = 5\npini = dict(a=2, b=1)\n\ndf = pd.DataFrame(columns=['a','b','label','value','niter'])\n\n# Let's first consider the gradient descent as before\ntrackers = gradient_descent(x, y, pini, ll, niter=int(1E3))\ndf1 = pd.DataFrame(data={'a':trackers['a'],'b':trackers['b'],'label':f'GD','value':trackers['loss'],'niter':np.arange(len(trackers['a']))})\ndf = pd.concat([d.dropna(axis=1, how=\"all\") for d in (df, df1)])\n\n# And now consider instead SGD\nfor i in range(n_ini):\n    trackers = sgd(x,y, pini, ll, niter=int(1E2), bs = 20)\n    df1 = pd.DataFrame(data={'a':trackers['a'],'b':trackers['b'],'niter':np.arange(len(trackers['a'])),'label':f'traj {i+1}','value':trackers['loss']})\n    df = pd.concat([d.dropna(axis=1, how=\"all\") for d in (df, df1)])\n\n\n\nCode\nfig = px.line(df, y='value', markers=True, animation_frame='label')\n\nfig[\"layout\"].pop(\"updatemenus\") # optional, drop animation buttons\nfig.update_layout(xaxis_title='iterations', yaxis_title='Loss')\nfig.show()\n\n\n\n\n                                                    \nFigure¬†12: Loss of the gradient descent and the stochastic gradient descent for different shufflings\n\n\n\nFigure¬†12 depcits the loss function of the gradient descent and the stochastic gradient descent algorithm for different shufflings. While both algorithms converge to a similar value of the Loss function, we can nicely observe the fluctuations coming from the stochasticity of the minibatches3. The latter can be also seen in Figure¬†13. It is interesting to notice in that last figure that the stochastic gradient descent fuctuates more in the \\(a\\)-direction. This fact is well known for SGD and can be improved with more avolved algorithms such as momentum, nesterov or Adam.\n\n\nCode\namin, amax = df.a.min()*0.8,df.a.max()*1.1\nbmin, bmax = df.b.min()*0.8,df.b.max()*1.1\nn = 100\nvec_a = np.arange(amin, amax,(amax-amin)/n)\nvec_b = np.arange(bmin, bmax,(bmax-bmin)/n)\nmatz = np.zeros((vec_a.size,vec_b.size))\n\nfor i, a1 in enumerate(vec_a):\n    for j, b1 in enumerate(vec_b):\n        params = dict(a=a1, b=b1)\n        matz[i,j] = MSE(x,y,line,params)\n        \nfig = go.Figure()\nfig.add_contour(z=np.log(matz),x=vec_b, y=vec_a,\n                hovertemplate=\n                    'a:%{y:.2f}'\n                    +'<br>b:%{x:.2f}</br>'\n                    +'f:%{z:.2f}<extra></extra>')\n\nhovertemplate ='a:%{a}'+'b:%{b}<extra></extra>'\nfor i in range(n_ini):\n    visible = True if i == 0 else 'legendonly'\n    newdf = df[df.label == f'traj {i+1}']\n    fig.add_scatter(x=newdf.b, y=newdf.a, name=f'traj {i+1}',text=newdf.value, mode='lines+markers',\n                    hovertemplate=\n                    'a:%{y:.2f}'\n                    +'<br>b:%{x:.2f}</br>'\n                    +'f:%{text:.2f}<extra></extra>',\n                    visible=visible)\n    \nnewdf = df[df.label == f'GD']\nfig.add_scatter(x=newdf.b, y=newdf.a, name=f'GD',text=newdf.value,\n                mode='lines', line={'dash': 'dash','color':'White'},\n                hovertemplate=\n                    'a:%{y:.2f}'\n                    +'<br>b:%{x:.2f}</br>'\n                    +'f:%{text:}<extra></extra>')\n\nlegend=dict(\n    yanchor=\"top\",\n    y=1.3,\n    xanchor=\"left\",\n    x=0.01\n    )\nd = dict(width=800,\n         height=600,\n         xaxis={'title':'b'},\n         yaxis={'title':'a'},\n         legend = legend\n        )\n\nfig.update_layout(d)\nfig.show()\n\n\n\n\n                                                    \nFigure¬†13: Loss landscape of the gradient descent and the stochastic gradient descent for different shufflings\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nRerun the last experiment with different minibatch sizes. What do you see?\n\n\nWe finish this section by observing how the line adjust to our dataset in terms of the iterations for the GD and SGD. The results are presented in Figure¬†14 for the gradient descent.\n\n\nCode generating the data of the figure\ni =1\nlabel = 'GD'#f'traj {i+1}' #change it if you want to see the SGD trajectory\nx1 = np.array([x.min(),x.max()])\nnewdf = df[df.label == label]\na, b, mse = newdf.a.to_numpy(), newdf.b.to_numpy(), newdf.value.to_numpy()\ny1 = np.einsum('i,j->ij',a,x1)+np.tile(b,(2,1)).T\n\n\n\n\nCode\nframes = [go.Frame(data=[go.Scatter(x=x1, y=y1[i,:],mode='lines')],layout=go.Layout(title_text=f'step:{i}, MSE:{mse[i]:.2f}')) for i in range(a.size)]\nbuttons = [dict(label=\"Play\",method=\"animate\",\n                args=[None, {\"frame\": {\"duration\": 100, \"redraw\": True},\n                             \"fromcurrent\": True, \n                             \"transition\": {\"duration\": 300,\"easing\": \"quadratic-in-out\"}}]),\n           dict(label=\"Pause\",method=\"animate\",\n                args=[[None], {\"frame\": {\"duration\": 0, \"redraw\": False},\"mode\": \"immediate\",\"transition\": {\"duration\": 0}}]),\n          dict(label=\"Restart\",method=\"animate\",\n                args=[None])]\n\nFig = go.Figure(\n    data=[go.Scatter(x=x1, y= y1[0,:],mode='lines',name = 'line'),\n          go.Scatter(x=x, y=y, mode=\"markers\", name='data',\n                hovertemplate='x:%{x:.2f}'\n                +'<br>y:%{y:.2f}</br><extra></extra>')],\n    layout=go.Layout(\n        xaxis=dict(range=[x.min()-2, x.max()+2], autorange=False),       \n        yaxis=dict(range=[y.min()-2, y.max()+2], autorange=False),\n        updatemenus=[dict(\n            type=\"buttons\",\n            buttons=buttons)]\n    ),\n    frames= frames\n)\n\nFig.show()\n\n\n\n\n                                                    \nFigure¬†14: Animation of t\n\n\n\n\n\n\n\n\n\nReferences\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. The MIT Press. https://doi.org/10.5555/3086952.\n\nFootnotes\n\n\nThe literature also uses the terms of criterion or cost, error, or objective functions. Their definitions are not very strict. Following (Goodfellow, Bengio, and Courville 2016): ‚Äò‚ÄôThe function we want to minimize or maximize is called the objective function, or criterion. When we are minimizing it, we may also call it the cost function, loss function, or error function. In this book, we use these terms interchangeably, though some machine learning publications assign special meaning to some of these term‚Äô‚Äô. For example, loss function may be defined for a single data point, the cost or error function may be a sum of loss functions, so check the definitions used in each paper.‚Ü©Ô∏é\nFor classification, a~more intuitive measure of the performance could be, e.g., accuracy, which is the ratio between the number of correctly classified examples and the data set size. Note, however, that gradient-based optimization requires measures of performance that are smooth and differentiable. These conditions distinguish loss functions from evaluation metrics such as accuracy, recall, precision, etc.‚Ü©Ô∏é\nBeware that the notion of iteration is different for gradient descent and for stochastic gradient descent. For the former, an iteration corresponds to an epoch (the whole training set), while for the latter it corresponds to a minibatch.‚Ü©Ô∏é"
  },
  {
    "objectID": "course/linear_models/polynomial_fit.html",
    "href": "course/linear_models/polynomial_fit.html",
    "title": "Polynomial fit",
    "section": "",
    "text": "1 Fitting a noisy polynomial curve\nWe now consider the task of fitting a polynomial curve with noise. Despite dealing with higher order polynomials than before, this problem can also be rewritten as a linear regression task. Let us first generate a dataset with the help of the function noisy_curve, which maps \\(x\\) to a polynomial of degree \\(d\\) \\(f(x)=\\mathbf{w}^T\\mathbf{x}+\\text{noise}\\) and where \\(\\mathbf{x}=(x^0,x^1,\\ldots,x^d)\\).\n\ncoeffs = [2., 1., 0., 1.]\nx, y = noisy_curve(coeffs, interval=[-3., 1.5], noise=[0., 2.])\n\nFigure¬†1 shows the generated data with the ground truth.\n\n\n\nFigure code\nfig = go.Figure()\nfig.add_scatter(x=x, y=y, mode=\"markers\", name='data',\n                hovertemplate='x:%{x:.2f}'\n                +'<br>y:%{y:.2f}</br><extra></extra>')\nx1 = np.linspace(x.min(),x.max(),num=50)\nx1, y1 = noisy_curve(coeffs,x=x1)\nfig.add_scatter(x=x1, y=y1, mode=\"lines\",name='Ground Truth')\nfig.update_layout(width=800,height=400,xaxis_title='x',yaxis_title='f(x)')\nfig.show()\n\n\n\n\n        \n        \n        \n(a) Parabola with Gaussian noise\n\n\n\n\n\n                                                    \n(b) ?(caption)\n\n\nFigure¬†1: ?(caption)\n\n\nAs in the previous section, we choose the mean square error for the loss function.\n\n\n2 Polynomial fit as multivariable linear regression\nThe polynomial regression can be seen as a linear regression of multiple variables with the help of the following trick. Let us rewrite \\(f(x)=\\sum_{i=0}^d w_i x^i\\) as \\(f(x)=\\mathbf{w}^T\\mathbf{x}\\), where \\(\\mathbf{x}=(x^0, x^1, \\ldots, x^d)\\). Now we can use this vectorial form, together with the fact that \\(f(x)\\) is linear w.r.t to \\(\\mathbf{x}\\) (while being non-linear w.r.t. to \\(\\mathbf{x}\\) to draw an analogy with linear regression. If we consider for instance the mean square error loss:\n\\[\\begin{aligned}\nMSE &= \\sum_{i=1}^N (\\mathbf{w}^T\\mathbf{x}_i-y_i)^2\\\\\n&= \\parallel \\mathbf{y}-X\\mathbf{w}\\parallel^2\\\\\n&=(\\mathbf{y}-X\\mathbf{w})^T(\\mathbf{y}-X\\mathbf{w}),\n\\end{aligned}\\]\nwhere\n\\[ X=\n\\begin{pmatrix}\n1 & x_1^1 & \\ldots & x_1^d\\\\\n1 & x_2^1 & \\ldots & x_2^d \\\\\n\\vdots& \\vdots & \\vdots & \\vdots\\\\\n1 & x_N^1 & \\ldots & x_N^d\n\\end{pmatrix} .\\]\nWe now take the derivative with respect to all the weights \\(w\\) and set it to \\(0\\). We therefore find the estimator\n\\[\\mathbf{w} =(X^TX)^{-1}X^T\\mathbf{y}.\\]\n\n\n\n\n\n\nExercise\n\n\n\nImplement a exact_poly_fit(x, y, degree) that numerically computes the weights w with the expression above. You can use np.linalg.inv to do the matrix inversion.\n\n\n\n# Your code here\ndef exact_poly_fit(x,y,degree):\n    \n    return w\n\nLet us run the algorithm for our dataset. We will fit a 3rd degree polynomial and compare the resulting parameters to the original ones.\n\nw_best = exact_poly_fit(x,y,3)\np1, p2 = dict(coeffs=w_best), dict(coeffs=coeffs)\nprint (np.array([w_best,coeffs]))\nprint (f'MSE Best parameters: {MSE(x,y,curve,params=p1):.3f}')\nprint(f'MSE Original parameters: {MSE(x,y,curve,params=p2):.3f}')\n\nThe algorithm does a fairly good job. It is quite interesting to have a look at the mean square error on this dataset. The best parameters have a lower loss than the actual true parameters! We will come back to this point later ;)\n\n\n3 Stochastic Gradient Descent\nJust like we did with the linear regression, we can optimize our model parameters with a gradient-based method. Let us see what we obtain with the stochastic gradient descent algorithm for the poynomial fit. Figure¬†2 shows the fit after optimization.\n\ncoeffs0 = np.random.normal(loc=0,scale=0.1,size=4)\n\nll = dict(loss=MSE, grads=grad_MSE_pr, fun=curve)\n\npini = dict(coeffs=coeffs0)\n\ndf = pd.DataFrame(columns=['coeffs','value'])\n\ntrackers = sgd(x,y, pini, ll, eta=1E-5, niter=int(1E4))\ndf1 = pd.DataFrame(data={'coeffs':trackers['coeffs'],'value':trackers['loss']})\ndf = pd.concat([d.dropna(axis=1, how=\"all\") for d in (df, df1)])\n\nprint(f'final Loss:{df[\"value\"].iloc[-1]:3f}')\nprint (df[\"coeffs\"].iloc[-1])\nprint(coeffs)\n\nfinal Loss:4.043960\n[1.28389833 0.55562468 0.44173774 1.18165629]\n[2.0, 1.0, 0.0, 1.0]\n\n\n\n\nFigure code\ncc = df[\"coeffs\"].iloc[-1]\n\nfig = go.Figure()\nfig.add_scatter(x=x, y=y, mode=\"markers\", name='data',\n                hovertemplate='x:%{x:.2f}'\n                +'<br>y:%{y:.2f}</br><extra></extra>')\nx1 = np.linspace(x.min(),x.max(),num=50)\ny1 = curve(x1,cc)\nfig.add_scatter(x=x1, y=y1, mode=\"lines\",name='Fit')\nfig.update_layout(width=800,height=400,xaxis_title='x',yaxis_title='f(x)')\nfig.show()\n\n\n\n\n                                                    \nFigure¬†2: Polynomial fit of the data\n\n\n\nFigure¬†3 shows how the algotihm adjusts the polynomial curve during the optimization.\n\n\nFigure code\nstep = 100\nx1 = np.linspace(x.min(),x.max(),num=50)\n\nframes = [go.Frame(data=[go.Scatter(x=x1, y=curve(x1,df[\"coeffs\"].iloc[i*step]),mode='lines')],layout=go.Layout(title_text=f'step:{i*step}, MSE:{df[\"value\"].iloc[i*step]:.2f}')) for i in range(len(df)//step)]\n\nbuttons = [dict(label=\"Play\",method=\"animate\",\n                args=[None, {\"frame\": {\"duration\": 100, \"redraw\": True},\n                             \"fromcurrent\": True, \n                             \"transition\": {\"duration\": 300,\"easing\": \"quadratic-in-out\"}}]),\n           dict(label=\"Pause\",method=\"animate\",\n                args=[[None], {\"frame\": {\"duration\": 0, \"redraw\": False},\"mode\": \"immediate\",\"transition\": {\"duration\": 0}}]),\n          dict(label=\"Restart\",method=\"animate\",\n                args=[None,{\"frame\": {\"duration\": 100, \"redraw\": True}}])]\n\nFig = go.Figure(\n    data=[go.Scatter(x=x1, y= curve(x1,df[\"coeffs\"].iloc[0]),mode='lines',name = 'line',\n                     hovertemplate='x:%{x:.2f}'+'<br>y:%{y:.2f}</br><extra></extra>'),\n          go.Scatter(x=x, y=y, mode=\"markers\", name='data',\n                hovertemplate='x:%{x:.2f}'\n                +'<br>y:%{y:.2f}</br><extra></extra>')],\n    layout=go.Layout(\n        xaxis=dict(range=[x.min()-2, x.max()+2], autorange=False),       \n        yaxis=dict(range=[y.min()-2, y.max()+2], autorange=False),\n        updatemenus=[dict(\n            type=\"buttons\",\n            buttons=buttons)]\n    ),\n    frames= frames\n)\nFig.update_layout(xaxis_title='x',yaxis_title='f(x)')\nFig.show()\n\n\n\n\n                                                    \nFigure¬†3: Animation of the optimization\n\n\n\n\n\n4 Overfitting\nUp until now, we have not discussed a very important hyper-parameter in this problem: the degree of the polynomail. In particular, we have fixed the degree to be the one of the original function. However, this is typically unknown in practice and we either rely on educated guesses or we resort to perform various fits for different degrees and keep the best one (but what is the best one?! We‚Äôll see). To this end, we use the polyfit subroutine of numpy, which is much more stable than the exact_poly_fit we prepared.\n\nvec_cc = []\nmse_t = []\nmse_v = []\n\nnpoly = 20\nndata = 50\nfor i in np.arange(1,npoly):\n    vec_cc.append(polyfit(x[:ndata],y[:ndata],deg=i))\n    p1, p2 = dict(coeffs=vec_cc[i-1]), dict(coeffs=vec_cc[i-1])\n    mse_t.append(MSE(x[:ndata], y[:ndata],curve,params=p1))\n    mse_v.append(MSE(x[ndata:], y[ndata:],curve,params=p2))\n\nFigure¬†4 shows the loss function over the training data as function of the polynomial degree. At a first glance, it looks like a higher degree gives rise to better loss. However, in Figure¬†5, we can really see the overfitting of the higher order polynomials. This can be detected by dividing the training set into two subsets: the training set and the validation set. We train the algorithm using data exclusively from the training set and, then, we evaluate its performance on the validation set. The validation set contains new data for our model, which allows us to assess how well our algorithm generalizes to unseen data. We can see that we are overfitting to the training set when we see that the loss in the validation set stagnates or increases. Turn on the orange line in Figure¬†4 to see it!\n\n\nFigure code\nfig = go.Figure()\nfig.add_scatter(x=np.arange(1,npoly), y=mse_t, mode='lines+markers', name='training')\nfig.add_scatter(x=np.arange(1,npoly), y=mse_v, mode='lines+markers', visible='legendonly', name='validation')\nfig.update_layout(yaxis_range=[0,20],xaxis_title='degree',yaxis_title='Loss')\n\n\n\n\n                                                    \nFigure¬†4: Training loss with respect to the degree of the polynomial\n\n\n\n\n\nFigure code\nfig = go.Figure()\nfig.add_scatter(x=x[:ndata], y=y[:ndata], mode=\"markers\", name='data',\n                hovertemplate='x:%{x:.2f}'\n                +'<br>y:%{y:.2f}</br><extra></extra>')\nx1 = np.linspace(x.min(),x.max(),num=50)\n\npoly = [1, 2, 3, 4, 6, 8, 10, 19]\nfor i,k in enumerate(poly):\n    visible = True if k == 0 else 'legendonly'\n    x1, y1 = noisy_curve(vec_cc[k-1],x=x1)\n    fig.add_scatter(x=x1, y=y1, mode=\"lines\",name=f'{k}th degree', visible=visible)\nfig.update_layout(width=800, height=400, yaxis_range=[y.min(),y.max()], xaxis_title='x', yaxis_title='f(x)')\nfig.show()\n\n\n\n\n                                                    \nFigure¬†5: Training loss with respect to the degree of the polynomial\n\n\n\nA typical instance for overfitting appears when we have many free parameters compared to the number of data points. Indeed, we can achieve a zero training loss with some algorithms when we have as many parameters as data points. However, this usually comes at the expense of extreme overfitting and poor generalization.\n\n\n\n\n\n\nExercise\n\n\n\nIn the experiment above, we have used 50 data points to fit up to a 19th-degree polynomial. Run the same procedure with increasingly less data, e.g., set ndata to 30, 20 and 10, and observe what happens with the resulting curves. Do we see overfitting for lower degree polynomials? Do you think these models would provide a reasonable prediction if we drew a new data sample form the same experiment?\n\n\nWe will now discuss two strategies to prevent overfitting.\n\n\n5 More data\nAs we have seen right above, the relative number of our model parameters compared to the amount of data that we have is a key factor for overfitting. If having less data makes our model more prone to overfitting, having more data naturally helps us mitigate it.\nTherefore, let us generate more samples.\n\nnsamples = int(1E3)\nxn, yn = noisy_curve(coeffs, interval = [-3,1.5], noise=[0.,2], nsamples=nsamples)\n\nWe then perform the split between the training set and validation set and compute the loss function for both sets.\n\nvec_cc, mse_t,mse_v  = [], [], []\n\nnpoly = 20\nndata = int(0.8*nsamples) #We set 80% of the data for the training and 20% for the validation\n\nfor i in np.arange(1,npoly):\n    vec_cc.append(polyfit(xn[:ndata],yn[:ndata],deg=i))\n    p1, p2 = dict(coeffs=vec_cc[i-1]), dict(coeffs=vec_cc[i-1])\n    mse_t.append(MSE(xn[:ndata], yn[:ndata], curve, params=p1))\n    mse_v.append(MSE(xn[ndata:], yn[ndata:], curve, params=p2))\n\nFigure¬†6 shows the comparison between the trainng loss and the validation loss for different degrees. We observe a much better behavior than in the previous case with small dataset. This is also confirmed in Figure¬†7 where we can clearly see the advantage of using a larger dataset.\n\n\nFigure code\nfig = go.Figure()\nfig.add_scatter(x=np.arange(1,npoly), y=mse_t, mode='lines+markers', name='training')\nfig.add_scatter(x=np.arange(1,npoly), y=mse_v, mode='lines+markers', visible='legendonly', name='validation')\nfig.update_layout(yaxis_range=[0,10], xaxis_title='degree', yaxis_title='Loss')\n\n\n\n\n                                                    \nFigure¬†6: Training loss with respect to the degree of the polynomial for a larger dataset\n\n\n\n\n\nFigure code\nfig = go.Figure()\nfig.add_scatter(x=xn[:ndata], y=yn[:ndata], mode=\"markers\", name='data',\n                hovertemplate='x:%{x:.2f}'\n                +'<br>y:%{y:.2f}</br><extra></extra>')\nx1 = np.linspace(x.min(),x.max(),num=50)\n\npoly = [1, 2, 3, 4, 6, 8, 10, 19]\nfor i,k in enumerate(poly):\n    visible = True if k == 0 else 'legendonly'\n    x1, y1 = noisy_curve(vec_cc[k-1],x=x1)\n    fig.add_scatter(x=x1, y=y1, mode=\"lines\",name=f'{k}th degree', visible=visible)\nfig.update_layout(width=800, height=400, yaxis_range=[y.min(),y.max()],xaxis_title='x',yaxis_title='f(x)')\nfig.show()\n\n\n\n\n                                                    \nFigure¬†7: Polynomial fit for the different degrees\n\n\n\n\n\n6 Regularization\nAnother way to avoid overfitting is regularization. The idea here is to add a term to the loss function that prevents the weights to behave in an ill-deined way. An example of such regularization is the \\(l_2\\) regularization which consists in adding to the loss function the term \\(\\alpha\\parallel \\mathbf{w} \\parallel^2\\). Intituitively, this parabolic term avoids to have exploding weights. The regression with such regularization is called Ridge regression.\n\n\n\n\n\n\nExercise\n\n\n\nFor the analytical solution, show that the regularization term gives rise to the following solution\n\\[w =(X^TX+2\\alpha \\mathbb{1})^{-1}X^T\\mathbf{y}.\\]\n\n\n\n\n\n\n\n\nExercise\n\n\n\nFor the gradient descent algorithm, show that the regularization leads to the update rule for each weight \\(w\\)\n\\[ w_{i+1} = (1-2\\, \\alpha\\,\\eta) w_i - \\eta \\partial_w MSE \\]\n\n\nWe here use an implementation of scikit learn. To this end, we define a function that creates the matrix \\(X\\).\n\ndef poly_cond(x, n):\n    matx = np.zeros((x.size,n))\n    for i,k in enumerate(range(1,n+1)):\n        matx[:,i] = x**k\n    return matx\n\nWe then perform the Ridge regression for the polynomials of different degrees.\n\nalpha = 0.5\n\nvec_cc, mse_t, mse_v = [], [], []\n\nnpoly = 20\nndata = 50\nfor i in np.arange(1,npoly):\n    matx = poly_cond(xn[:ndata],i)\n    reg = linear_model.Ridge(alpha=alpha)\n    reg.fit(matx,yn[:ndata])\n    c = np.insert(reg.coef_,0,reg.intercept_)\n    vec_cc.append(c)\n    p1, p2 = dict(coeffs=vec_cc[i-1]), dict(coeffs=vec_cc[i-1])\n    mse_t.append(MSE(xn[:ndata], yn[:ndata], curve, params=p1))\n    mse_v.append(MSE(xn[ndata:], yn[ndata:], curve, params=p2))\n\nFigure¬†8 shows the Loss funtion in terms of the degree of the polynomials. We can now see that the validation curve behaves much better for higher polymomials. The latter is also confirmed with Figure¬†9, which shows smaller behaviors for higher polynomials.\n\n\nFigure code\nfig = go.Figure()\nfig.add_scatter(x=np.arange(1,npoly), y=mse_t, mode='lines+markers', name='training')\nfig.add_scatter(x=np.arange(1,npoly), y=mse_v, mode='lines+markers', visible='legendonly', name='validation')\nfig.update_layout(yaxis_range=[0,10],xaxis_title='Loss',yaxis_title='degree')\n\n\n\n\n                                                    \nFigure¬†8: Polynomial fit for the different degrees\n\n\n\n\n\nFigure code\nfig = go.Figure()\nfig.add_scatter(x=x[:ndata], y=y[:ndata], mode=\"markers\", name='data',\n                hovertemplate='x:%{x:.2f}'\n                +'<br>y:%{y:.2f}</br><extra></extra>')\nx1 = np.linspace(x.min(),x.max(),num=50)\n\npoly = [1, 2, 3, 4, 6, 8, 10, 19]\nfor i,k in enumerate(poly):\n    visible = True if k == 0 else 'legendonly'\n    x1, y1 = noisy_curve(vec_cc[k-1],x=x1)\n    fig.add_scatter(x=x1, y=y1, mode=\"lines\",name=f'{k}th degree', visible=visible)\nfig.update_layout(width=800, height=400, yaxis_range=[yn.min()-1,yn.max()+1])\nfig.show()\n\n\n\n\n                                                    \nFigure¬†9: Polynoms for the different degrees\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nBalancing two loss terms: change the value of \\(\\alpha\\) from 0 to 100 and perform the same analysis. For a fixed degree (e.g.¬†3), plot the value of the MSE as a function of \\(\\alpha\\) for both the training and the validation dataset. What do you see?\n\n\n\n# Your code here"
  },
  {
    "objectID": "course/index.html",
    "href": "course/index.html",
    "title": "Structure of the lectures",
    "section": "",
    "text": "flowchart TD\nsubgraph one[ ]\n A(linear models)--> A1(Linear regression)\n A --> A2(Polynomial Regression)\n A --> A3(Logistic Regression)\n A --> A4(Perceptron)\nend\nstyle one fill:#82c4c3,stroke-width:0px;\nclassDef boxes stroke-width:0px;\nclass A,A1,A2,A3,A4 boxes;\n\n\n\n\n\n\n\n\n\n\n\n\nflowchart TD\nsubgraph three[ ]\n C(A Probabilistic View on Machine Learning)--> C1(Review Probability)\n C-->C2(Likelihood)\n C-->C3(Kullback Leibler Divergence)\n C-->C4(Linear Regression) \nend\nstyle three fill:#f6d887,stroke-width:0px;\nclassDef boxes stroke-width:0px;\nclass C,C1,C2,C3,C4 boxes;\n\n\n\n\n\n\n\n\n\n\n\n\nflowchart TD\nsubgraph two[ ]\n B(Neural Networks)--> B1(Perceptron)\n B-->B2(Deep Neural Networks)\n B-->B3(Automatic Differentiation)\nend\nstyle two fill:#82c4c3,stroke-width:0px;\nclassDef boxes stroke-width:0px;\nclass B,B1,B2,B3,B4 boxes;"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning in Classical and Quantum Physics",
    "section": "",
    "text": "Welcome to the webpage of the ML for Physics Course at the UIBK. Here you will find most resources of the course, from explanatory notebooks to code snippets that will help us explore the wild world of ML."
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Machine Learning in Classical and Quantum Physics",
    "section": "Course description",
    "text": "Course description\nThis course gives an introduction to machine learning and deep learning: starting from linear linear models al the way up to state of the art generative models. The material covers the following topics:\n\nWhat is actually machine learning?\nBasics of ML: From linear models to logistic regression\nML applications: from computer vision to Physics\nBasics of deep learning: from neural networks to Transformers\nUnsupervised learning and interpretable ML\nReinforcement Learning\nGenerative modelling: from Boltzmann machines to diffusion models\n\nThe course combines theory and practice in the form of jupyter notebooks with python. We make extensive use of specific librairies such as numpy, PyTorch and fastai."
  },
  {
    "objectID": "index.html#evaluation",
    "href": "index.html#evaluation",
    "title": "Machine Learning in Classical and Quantum Physics",
    "section": "Evaluation",
    "text": "Evaluation\nHomeworks (50%)\nThere will be 3 homeworks, each equally contributing to the final mark. To know more about the homeworks, visit the Codabench platform. The day after the submission of each of the homeworks, your group will do a short presentation (5 mins.) about the methods you developed. The mark will calculated from your performance above the baseline (60%), the revision of the code + your presentations (35%) and an extra 5% based on your ranking‚Äôs position (first position gets the full 5% :), and we decrease linearly).\n\nImportant: to get accepted in the competition, your Codabench username must end with ‚Äú_UIBK25‚Äù.\n\nFinal Project (30%)\nThe last weeks of the course you will work in groups on a final project, the topic of which will be made public at later stages. You will present your findings in a 20 minutes presentation in the last days of the course. The topics of the final projects will be decided later, based on the number of course participants.\nExam (20%)\nA short written exam, reviewing the main concepts taught in the course."
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "Machine Learning in Classical and Quantum Physics",
    "section": "Resources",
    "text": "Resources\n\nSome of the content of this course has been adapted from the book Machine Learning for the Quantum Sciences by A. Dawid et al., which serves as a gentle introduction to ML but also to its applications in quantum sciences.\nThe book Neural Networks and Deep Learning by Nielsen offers a nice hands-on introduction to the world of ML\nIf you are already fluent in Python, the course Practical Deep Learning for Coders is for you. Indeed, we will extensively use some of the tools developed therein, as for instance the library fastai.\nFor the Reinforcement Learning part of this course, the book Reinforcement Learning: An Introduction is the go-to resource"
  },
  {
    "objectID": "index.html#previous-contributors",
    "href": "index.html#previous-contributors",
    "title": "Machine Learning in Classical and Quantum Physics",
    "section": "Previous contributors",
    "text": "Previous contributors\nPart of the content of this course was originally developed for by Borja Requena, Alexandre Dauphin, Marcin P≈Çodzie≈Ñ and Paolo Stornati for the Master in Quantum Science and Technology Barcelona. The original course content can be found here."
  }
]