[
  {
    "objectID": "course/linear_models/logistic_regression.html#accuracy",
    "href": "course/linear_models/logistic_regression.html#accuracy",
    "title": "Logistic regression",
    "section": "7.1 Accuracy",
    "text": "7.1 Accuracy\nThe most typical, and perhaps natural metric when dealing with classification problems is the accuracy, which we define as:\n\\[ \\text{acc}= \\frac{\\# \\text{correct predictions}}{\\#\\text{dataset}},\\]\ni.e. the ratio between the number of correct predictions and the number of elements in our training set. Compared to the BCE, a value of this metric is quite easy to undestand: close to 0 is really bad, close to 1 is really good.\n\nveca, vecb, vecl = np.array(trackers['a']), np.array(trackers['b']), np.array(trackers['loss'])\nmask = np.arange(0,len(veca),100)\nveca, vecb, vecl = veca [mask], vecb[mask], vecl[mask]\n\n\n\n\n\n\n\nExercise\n\n\n\nWrite the function accuracy(x,y,a,b), which returns the accuracy for a given dataset and for the parameters \\(a\\) and \\(b\\). Choose the label with the following rule: \\(0\\) if \\(\\sigma_i<0.5\\) else \\(1\\).\n\n\n\n### Your Code Here!\ndef accuracy(x,y,a,b):\n    \n    return acc\n\n\n\nSolution\ndef accuracy(x,y,a,b):\n    yp = sigmoid(x, a, b)\n    yp[yp>0.5] = 1\n    yp[yp<=0.5] = 0\n    return np.sum(yp==y)/y.size\n\n\nNow that we can calculate the accuracy, let’s track it through training. Let’s us the save veca and vecb from above to avoid retraining:\n\nvec_acc = np.zeros_like(veca)\n\nfor i,(a,b) in enumerate(zip(veca,vecb)):\n    vec_acc[i] = accuracy(x,y,a,b)\n\nFigure 5 shows the value of the Loss function and the accuracy during the training. It is interesting to note that while the Loss function is a smooth function, the accuracy is not smooth and that the variations of Loss function do not directly correspond to the variations of the accuracy. It is therefore important to check not only the Loss function but also the figure of merit of the problem, that can be for example the accuracy.\n\n\nFigure code\n# Create figure with secondary y-axis\nfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\n# Add traces\nfig.add_trace(\n    go.Scatter(y=vecl, name=\"Loss\"),\n    secondary_y=False,\n)\n\nfig.add_trace(\n    go.Scatter(y=vec_acc, name=\"Accuracy\", visible = 'legendonly'),\n    secondary_y=True,\n)\n\n\n# Set x-axis title\nfig.update_xaxes(title_text=\"iterations\")\n\n# Set y-axes titles\nfig.update_yaxes(title_text=\"Loss\", secondary_y=False)\nfig.update_yaxes(title_text=\"Accuracy\", secondary_y=True)\n\nfig.show()\n\n\n\n\n                                                    \nFigure 5: Loss function vs Accuracy"
  },
  {
    "objectID": "course/linear_models/logistic_regression.html#confusion-matrix",
    "href": "course/linear_models/logistic_regression.html#confusion-matrix",
    "title": "Logistic regression",
    "section": "7.2 Confusion matrix",
    "text": "7.2 Confusion matrix\nWhile the accuracy is quite understandable, it may not give us enough information to understand why our model is actually failing. For instance, why is the previous accuracy not 1? Where is our model making a mistake? The confusion matrix is a very handy method for this. Before introducing it, we need to introduce few terms, the following table helps us for that:\n\n\n\n\nPositive (Prediction)\nNegative (Prediction)\n\n\n\n\nPositive (Ground Truth)\nTrue Positive (TP)\nFalse negative (FN)\n\n\nNegative (Ground Truth)\nFalse positive (FP)\nTrue Negative (TN)\n\n\n\nNow the confusion matrix is just the previous table, but in matrix form:\n\\[\nC = \\begin{bmatrix}\n\\text{TP} & \\text{FN} \\\\\n\\text{FP} & \\text{TN}\n\\end{bmatrix}\n\\]\nLet us now construct the confusion matrix for our model.\n\nyp = sigmoid(x, a, b)\nyp[yp>0.5] = 1\nyp[yp<=0.5] = 0\ncm = confusion_matrix(y,yp)\n\nFigure 6 shows the confustion matrix for the logistic regression.\n\n\nFigure code\nX, Y = [\"Alive(P)\", \"Dead(P)\"], [\"Alive(GT)\", \"Dead(GT)\"]\nfig = px.imshow(cm, x=X, y=Y, text_auto=True,color_continuous_scale='Blues')\n\n\nfig.show()\n\n\n\n\n                                                    \nFigure 6: Confusion matrix for the logistic regression\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nHow do you interpret the previous confusion metrics? What is the model doing wrong?\n\n\nWe readily observe that the accuracy correponds to the trace of this matrix normalized by the number of individuals.\n\nprint(f'{np.trace(cm)/np.sum(cm):.3f}')\n\n0.714"
  },
  {
    "objectID": "course/linear_models/logistic_regression.html#other-metrics",
    "href": "course/linear_models/logistic_regression.html#other-metrics",
    "title": "Logistic regression",
    "section": "7.3 Other metrics",
    "text": "7.3 Other metrics\nThere exist many other metrics that can be calculated from the previous. In particular, the following are commonly used:\n\n\n\n\n\n\n\n\nMetric\nFormula\nIntuition\n\n\n\n\nPrecision\n\\(\\frac{TP}{TP+FP}\\)\nMeasures how many of the predicted positives are truly positive (focuses on correctness)\n\n\nRecall\n\\(\\frac{TP}{TP+FN}\\)\nMeasures how many of the actual positives are correctly detected (focuses on completeness)\n\n\nF1 score\n\\(\\frac{2\\text{Recall}\\,\\text{Precision}}{\\text{Recall}+\\text{Precision}}\\)\nBalances both by taking their harmonic mean, rewarding models that are both precise and sensitive\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCompute the different metrics for the trained logistic regression model. Bonus: compute it as a function of the training epochs.\n\n\n\n### Your Code Here!\n\n\n\nSolution\nprecision = cm[0,0]/(cm[0,0]+cm[1,0])\n\nrecall = cm[0,0]/(cm[0,0]+cm[0,1])\n\nF1_score = 2*recall*precision/(precision+recall)\n\nprint(f'Precision: {precision:.3f}')\n\nprint(f'Recall: {recall:.3f}')\n\nprint(f'F1 score: {F1_score:.3f}')\n\n\n\n\nSolution Bonus\nvec_prec = np.zeros_like(veca)\nvec_rec = np.zeros_like(veca)\nvec_f1 = np.zeros_like(veca)\n\nfor i,(a,b) in enumerate(zip(veca,vecb)):\n\n    yp = sigmoid(x, a, b)\n    yp[yp>0.5] = 1\n    yp[yp<=0.5] = 0\n    cm = confusion_matrix(y,yp)\n\n    if (cm.diagonal() == 0).any():\n        continue\n\n    vec_prec[i] = cm[0,0]/(cm[0,0]+cm[1,0])\n\n    vec_rec[i] = cm[0,0]/(cm[0,0]+cm[0,1])\n    \n    vec_f1[i] = 2*vec_prec[i]*vec_rec[i]/(vec_prec[i]+vec_rec[i])\n\n\n\n\nFigure code\n# Create figure with secondary y-axis\nfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\n# Add traces\nfig.add_trace(\n    go.Scatter(y=vecl, name=\"Loss\"),\n    secondary_y=False,\n)\n\nfig.add_trace(\n    go.Scatter(y=vec_prec, name=\"Precision\", visible = 'legendonly'),\n    secondary_y=True,\n)\n\nfig.add_trace(\n    go.Scatter(y=vec_rec, name=\"Recall\", visible = 'legendonly'),\n    secondary_y=True,\n)\n\nfig.add_trace(\n    go.Scatter(y=vec_f1, name=\"F1-score\", visible = 'legendonly'),\n    secondary_y=True,\n)\n\n\n# Set x-axis title\nfig.update_xaxes(title_text=\"iterations\")\n\n# Set y-axes titles\nfig.update_yaxes(title_text=\"Loss\", secondary_y=False)\nfig.update_yaxes(title_text=\"Accuracy\", secondary_y=True)\n\nfig.show()\n\n\n\n\n                                                    \nFigure 7: Loss function vs other metrics\n\n\n\nLet us perform the same analysis on another dataset, the brest cancer dataset of scikit learn.\n\nx1, y1 =load_breast_cancer(return_X_y=True)\nx1 = x1[:,3]\n\nprint(x1[-10:])\nprint(y1[-10:])\n\n[ 403.5  600.4  386.   716.9 1347.  1479.  1261.   858.1 1265.   181. ]\n[1 1 1 0 0 0 0 0 0 1]\n\n\n\nx1 = x1.reshape([np.size(x1),1])\n\nclf = LogisticRegression().fit(x1,y1)\nyp1 = clf.predict(x1)\n\ncm = confusion_matrix(y1,yp1)\n\n\n\nFigure code\nX1, Y1 = [\"Malignous(P)\", \"Benign(P)\"], [\"Malignous(GT)\", \"Benign(GT)\"]\nfig = px.imshow(cm, x=X1, y=Y1, text_auto=True,color_continuous_scale='Blues')\n\n\nfig.show()\n\n\n\n\n                                                    \nFigure 8: Confusion matrix for the logistic regression of the brest cancer\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCompute the different metrics for the trained logistic regression model.\n\n\n\n\nSolution\nprecision = cm[0,0]/(cm[0,0]+cm[1,0])\n\nrecall = cm[0,0]/(cm[0,0]+cm[0,1])\n\nF1_score = 2*recall*precision/(precision+recall)\n\nprint(f'Precision: {precision:.3f}')\n\nprint(f'Recall: {recall:.3f}')\n\nprint(f'F1 score: {F1_score:.3f}')\n\n\nPrecision: 0.892\nRecall: 0.783\nF1 score: 0.834"
  },
  {
    "objectID": "course/linear_models/linear_regression.html",
    "href": "course/linear_models/linear_regression.html",
    "title": "Linear regression",
    "section": "",
    "text": "1 The task\nIn this introductory notebook, we discuss our first learning algorithm to perform a regression task. Given a dataset \\(\\{\\mathbf{x},\\mathbf{y}\\}\\) of \\(n\\) points, we would like to find the line \\(y'=\\mathbf{w}^{T} \\mathbf{x} + \\mathbf{b}\\) that best fits the data. Therefore, let us start by generating such a dataset for the one dimnesional case. We do so by taking the line \\(y=a^*x+b^*\\) and adding gaussian noise to \\(y\\). We have prepared a small package lectures_ml with functionalities to do these tasks easily.\n\na_true, b_true = 1.5, 1 \nx, y = noisy_line(a_true, b_true, noise=[0,2])\n\n\n\n\n\n\n\nDocumentation and source code\n\n\n\nYou can access the documentation of any function by pressing the tab key or by adding a ? after the function. You can also see the source code by adding ?? after the function. If you want them to appear in a cell of the notebook, you can use the function nbdev.showdoc() for the documentation and lectures_ml.utils.show_code().\n\n\nFigure 1 shows the dataset \\(\\{x,y\\}\\). As expected, the dataset follows the linear relation dispersion (in red) but with some noise.\n\n\n\nCode\nfig = go.Figure()\nfig.add_scatter(x=x, y=y, mode=\"markers\", name='data',\n                hovertemplate='x:%{x:.2f}'\n                +'<br>y:%{y:.2f}</br><extra></extra>')\nx1 = np.array([x.min(),x.max()])\ny1 = a_true*x1+b_true\nfig.add_scatter(x=x1, y=y1, mode=\"lines\",name='Ground Truth')\nfig.update_layout(width=800,height=400,xaxis={'title':'x'},yaxis={'title':'y'})\nfig.show()\n\n\n\n\n        \n        \n        \n(a) Line with Gaussian noise\n\n\n\n\n\n                                                    \n(b) ?(caption)\n\n\nFigure 1: ?(caption)\n\n\n\n\n2 Learning as an optimization problem\nThe goal of the learning task is to find the slope and the intercept of the line directly from the data. Therefore, we have to define a suitable model to solve the task with the given data. In general, the model is a function of the input data, \\(f(\\mathbf{x})\\), whose output is interpreted as a prediction for the input data. We start by declaring a certain parametrization of a model (function), e.g., \\(f(\\mathbf{x}) = \\mathbf{w}^{T} \\mathbf{x} + \\mathbf{b}\\), with \\(\\theta \\supset \\{\\mathbf{w}, \\mathbf{b}\\}\\) denoting the model parameters. Then, all possible parametrizations of this function form the set of functions, i.e., the hypothesis class. Given that both \\(x\\) and \\(y\\) are one-dimensional in our example, let’s consider \\(f_\\theta(\\mathbf{x}) = a x + b\\) where \\(a\\) and \\(b\\) are real numbers too.\n\n\n\n\n\n\nImportant\n\n\n\nMachines ‘’learn’’ by minimizing a loss function of the training data, i.e., all the data accessible to the ML model during the learning process. The minimization is done by tuning the parameters of the model. We need to choose the loss function according to the objective task, although there is certain freedom on how to do it. In general, the loss function compares the model predictions or a developed solution against the reality or expectations. Therefore, learning becomes an optimization problem.\n\n\nHere, we use the terms of loss, error, and cost functions 1 interchangeably following Ref. (Goodfellow, Bengio, and Courville 2016). Popular examples of loss functions include the mean square error and the cross entropy, used for supervised regression and classification 2 problems.\n\n\n3 The loss function: Mean square error\nHaving a model, we now have to define a loss function for our regression task. For this case, we choose the mean square error, defined as \\[MSE=\\frac{1}{N}\\sum_{i=1}^{N}(y_i'-y_i)^2.\\] Such a loss measures the mean vertical distance between the dataset and the line \\(y'=w_1 x + w_0\\) (see Figure 2).\n\n\n\nFigure 2: Mean Square error\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere is not a unique loss function suitable for our task. We could have chosen other losses such as, e.g., the Mean Absolute Error (MAE) or the Root Mean Squared Error (RMSE). The choice of the loss really depends on the problem and the dataset.\n\n\nLet us now study the loss function in terms of its two parameters \\(\\{a,b\\}\\) for our dataset \\(\\{x,y\\}\\). Figure 3 shows the contour plot of the logarithm of loss function in terms of \\(a\\) and \\(b\\). We can clearly see that the minimum appears at the expected values of the line we generated in the previous section.\n\n\nCode generating the data of the figure\nvec_a = np.arange(-5,5,0.1)\nvec_b = np.arange(-5,5,0.1)\nmatz, matzg = np.zeros((vec_a.size,vec_b.size)), np.zeros((vec_a.size,vec_b.size,2))\nvec = np.zeros((vec_a.size*vec_b.size,3))\n\nfor i, a1 in enumerate(vec_a):\n    for j, b1 in enumerate(vec_b):\n        matz[i,j] = MSE(x,y,lambda x:a1*x+b1)\n        matzg[i,j,:] = grad_MSE_lr(x,y,dict(a=a1,b=b1))\n\n\n\n\nCode\nfig = go.Figure()\n\nfig.add_contour(z=np.log(matz),x=vec_b, y=vec_a,hovertemplate=\n                    'a:%{y:.2f}'\n                    +'<br>b:%{x:.2f}</br>'\n                    +'f:%{z:.2f}<extra></extra>')\nfig.add_scatter(x=[b_true],y=[a_true], marker_color='White')\n\n\nd = dict(width=600,\n         height=600,\n         xaxis={'title':'b'},\n         yaxis={'title':'a'}\n       )\n\nfig.update_layout(d)\nfig.show()\n\n\n\n\n                                                    \nFigure 3: \\(MSE(a,b)\\)\n\n\n\n\n\n4 Finding the minimum of the loss function\nIn the case of the mean square error, we can derive analytically the optimal values of \\(a\\) and \\(b\\). To this end, we start by writing the gradients \\[\n\\begin{align}\n&\\partial_a MSE=\\frac{2}{N}\\sum_{i=1}^{N}(y_i'-y_i)x_i\\\\\n&\\partial_b MSE=\\frac{2}{N}\\sum_{i=1}^{N}(y_i'-y_i).\n\\end{align}\n\\]\nThis leads to the linear system of equations for \\(a\\) and \\(b\\) when the gradients vanish \\[\n\\begin{align}\n&a \\sum_{i=1}^N x_i^2+b \\sum_{i=1}^N x_i - \\sum_{i=1}^N y_i x_i =0\\\\\n&a \\sum_{i=1}^N x_i+b N -\\sum_{i=1}^N y_i =0\n\\end{align}\n\\]\nWe can easily solve this system of equation to find\n\\[\n\\begin{align}\n& b = \\bar{y} - a \\bar{x}\\\\\n& a = \\frac{\\sum_{i=1}^N (x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^N(x_i-\\bar{x})^2},\n\\end{align}\n\\] where \\(\\bar{x}\\) (\\(\\bar{y}\\)) stands for the mean of \\(x\\) (\\(y\\)). As this problem is convex, we have found the unique global minimum.\n\n\n\n\n\n\nExercise\n\n\n\nImplement a function linear_regression_analytic(x,y) to compute the analytical optimal values for the slope and intercept given a dataset with samples x and y, such as the one we have created above.\n\n\n\n### Your Code Here!\ndef linear_regression_analytic(x,y):\n    pass\n\n\nestimate_a, estimate_b = linear_regression_analytic(x,y)\n\nprint(f'a={estimate_a:.3f}\\nb={estimate_b:.3f}')\n\n\n\nSolution\ndef linear_regression_analytic(x,y):\n    xb, yb = np.mean(x), np.mean(y)\n    a = np.sum((x-xb)*(y-yb))/np.sum((x-xb)**2)\n    b = yb - a*xb\n    return a,b\n\n\nWe have just performed our first learning task!\n\n\n5 Gradient Descent\nIn general, we do not have a tractable closed expression for the optimal parameters and we need to solve the optimization task through other means. Here, we introduce gradient-based approaches, which, despite not being needed for this task, it will allow us to introduce important concepts that will appear in a more abstract form in neural networks.\nLet us first study the gradients. Figure 4 shows the gradients of the MSE with respect to \\(a\\) and \\(b\\). The values of \\(a\\) and \\(b\\) of the line lie in the zero contour lines of the gradients.\n\n\nCode\nfor i in range(2):\n    mat = matzg[:, :, i]\n    vmax = np.abs(mat).max()  # symmetric range around 0\n\n    fig = go.Figure()\n    fig.add_contour(\n        z=mat,\n        x=vec_b,\n        y=vec_a,\n        colorscale='RdBu',   # diverging colormap centered on zero\n        zmin=-vmax,\n        zmax=vmax,\n        colorbar_title=\"Value\"\n    )\n\n    fig.add_scatter(x=[b_true], y=[a_true], marker_color='white')\n    fig.update_layout(\n        xaxis_title='b',\n        yaxis_title='a'\n    )\n    fig.show()\n\n\n\n\n\n\n\n                                                    \n(a) \\(\\partial_a MSE(a,b)\\)\n\n\n\n\n\n                                                    \n(b) \\(\\partial_b MSE(a,b)\\)\n\n\n\nFigure 4: Gradient of \\(MSE(a,b)\\)\n\n\n\nWe can now perform a gradient optimization. The simplest one is the gradient descent algorithm (often called steepest descent algorithm). This iterative algorithms works as follows:\n\n\n\n\n\n\nPseudocode\n\n\n\n\nChoose an initial condtion for the paramaters: \\(a_0\\) and \\(b_0\\)\nChoose a step size \\(\\eta\\)\nRepeat:\n\nCompute the gradients \\(\\partial_a MSE\\) and \\(\\partial_b MSE\\)\nUpdate the parameters in the opposite direction of the gradient \\[\\begin{aligned}\n&a_{i+1}=a_i-\\eta \\, \\partial_a MSE\\\\\n&b_{i+1}=b_i-\\eta \\, \\partial_b MSE\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nImplement the previous pseucode code to find the minimum of \\(f(x)=x^2\\). This convex function has a unique global minimum at \\(x=0\\) and we can compute its gradient analitically.\n\n\n\n# Here are the functions we will use\ndef f(x): return x**2\ndef grad_f(x): return 2*x\n\nGiven the initial \\(x_0\\), perform perform n_iter iterations of the gradient descent algorithm.\n\n### Your Code Here!\ndef gd_step(x0, grad_func):\n    pass\n\n\n\nCode\n# Solution\ndef gd_step(x0, grad_func):\n    x1 = x0 - eta* grad_func(x0)\n    return x1\n\n\nOnce you have your gradient step ready, put it to the text by creating a loop that performs the pseudocode higher up. Keep track of the values of \\(x\\) and \\(f(x)\\) to see how they evolve. Do 20 iterations of GD.\n\n#### Your Code Here!\n\n\n\nCode\n# Solution\n\nn_iter = 20\n\nx0 = 2\n\neta = 1E-1\n\n# keep track of the value of X\nvecx = np.zeros(n_iter+1)\n# And aslo the value of the function\nvecf = np.zeros(n_iter+1)\n\nvecx[0] = x0\nvecf[0] = f(x0)\n\nfor i in np.arange(n_iter):\n    vecx[i+1] = gd_step(vecx[i], grad_f) \n    vecf[i+1] = f(vecx[i+1])\n\n\n\n\nCode\n# Solution\n\nfig = go.Figure()\n\nx1 = np.arange(-2.5,2.51,0.01)\ny1 = f(x1)\nfig.add_scatter(x=x1, y=y1, mode=\"lines\",name='Parabola',marker_color='#EF553B', visible='legendonly')\n\n\nfig.add_scatter(x=vecx, y=vecf, mode=\"markers\", name='GD',\n                hovertemplate='x:%{x:.2f}'\n                +'<br>y:%{y:.2f}</br><extra></extra>',marker_color='#636EFA',marker_size=8, visible='legendonly')\n\nfig.update_layout(width=800,height=400,xaxis={'title':'x'},yaxis={'title':'f(x)'})\nfig.show()\n\n\n\n\n                                                    \nFigure 5: Gradient descent on a parabola\n\n\n\nFigure 5 shows a nice convergence of the algorithm to the global minimum \\(x=0\\).\nLet us now come back to our linear regression problem. We consider n_ini random initial values for our parameters and run the gradient descent algortihm. Rather than writing the whole algorithm again, we use the gradient_descent function from the lectures_ml library.\n\nn_ini = 5\nveca0 = np.random.uniform(low=vec_a[1], high=vec_a[-2], size=n_ini)\nvecb0 = np.random.uniform(low=vec_b[1], high=vec_b[-2], size=n_ini)\n\nll = dict(loss=MSE, grads=grad_MSE_lr, fun=line)\n\ndf = pd.DataFrame(columns=['a','b','label','value'])\nfor i in range(n_ini):\n    pini = dict(a=veca0[i],b=vecb0[i])\n    trackers = gradient_descent(x, y, pini, ll, niter=int(1E4), eta=1E-3)\n    df1 = pd.DataFrame(data={'a':trackers['a'],'b':trackers['b'],'label':f'traj {i+1}','value':trackers['loss']})\n    df = pd.concat([d.dropna(axis=1, how=\"all\") for d in (df, df1)])\n\nFigure 6 depicts the loss functions in terms of the epochs for the different trajectories. The initial value of the loss function strongly varies depending on the initial conditions.However, we observe that the steepest descent algorithm drives rapidly the parameters towards the minimum.\n\n\nCode\nfig = px.scatter(df, y='value',animation_frame='label')\n\nfig[\"layout\"].pop(\"updatemenus\") # optional, drop animation buttons\nfig.update_layout(xaxis_title='epochs',yaxis_title='Loss')\nfig.show()\n\n\n\n\n                                                    \nFigure 6: Loss function for the different initial conditions\n\n\n\nIn ML it is usually much illustrative to see the evolution of the loss function in a log-scale:\n\n\nCode\nfig = px.scatter(df, y='value',animation_frame='label')\n\nfig[\"layout\"].pop(\"updatemenus\") # optional, drop animation buttons\nfig.update_layout(xaxis_title='epochs',yaxis_title='Loss',\n                 yaxis_type='log', xaxis_type='log' )\nfig.show()\n\n\n\n\n                                                    \nFigure 7: Loss function for the different initial conditions\n\n\n\nFigure 8 shows the trajectories in the parameter space.\n\n\nCode\nfig = go.Figure()\nfig.add_contour(z=np.log(matz),x=vec_b, y=vec_a,\n                hovertemplate=\n                    'a:%{y:.2f}'\n                    +'<br>b:%{x:.2f}</br>'\n                    +'f:%{z:.2f}<extra></extra>')\n\nhovertemplate ='a:%{a}'+'b:%{b}<extra></extra>'\nfor i in range(n_ini):\n    visible = True if i == 0 else 'legendonly'\n    newdf = df[df.label == f'traj {i+1}']\n    fig.add_scatter(x=newdf.b, y=newdf.a, name=f'traj {i+1}',text=newdf.value,\n                    hovertemplate=\n                    'a:%{y:.2f}'\n                    +'<br>b:%{x:.2f}</br>'\n                    +'f:%{text:.2f}<extra></extra>', visible=visible)\n    \nlegend=dict(\n    yanchor=\"top\",\n    y=1.3,\n    xanchor=\"left\",\n    x=0.1\n    )\nd = dict(width=800,\n         height=600,\n         xaxis={'title':'b'},\n         yaxis={'title':'a'},\n         legend = legend\n        )\n\nfig.update_layout(d)\nfig.show()\n\n\n\n\n                                                    \nFigure 8: Gradient descent for \\(n_{ini}\\) initial conditions \\(a_0\\) and \\(b_0\\).\n\n\n\n\n\n6 Choosing a Learning rate\nChoosing a learning rate has an impact on convergence to the minimum, as depicted in Figure 9.\n\nIf the learning rate is too small, the training needs many epochs.\nThe right learning rate allows for a fast convergence to a minimum and needs to be found.\nIf the learning rate is too large, optimization can take you away from the minimum (you ``overshoot’’).\n\n\n\n\nFigure 9: Choice of the learning rate\n\n\nLet us first illustrate the latter on the parabola example.\n\ntreshold = 1E-6 # Minimum difference between f_t and f_t+1 at which we stop the iterations\nimax = int(1E4) # Maximum number of iterations\n\n# Initial guess\nx0 = 2\n\n# Learning rate\neta = 1E-3\n\n# Saving the info\nvecx, vecf = [x0], [f(x0)]\n\nx1=x0\ni = 0\ndl = 10\nwhile dl>treshold and i<imax:\n    i = i+1\n    x1 =  x1 - eta* grad_f(x1)\n    vecx.append(x1)\n    vecf.append(f(x1))\n    dl = np.abs(vecf[-1]-vecf[-2])\n    if vecf[-1]>1000.: break\n\n\n\nCode\nfig = go.Figure()\n\nx1 = np.arange(-2.5,2.51,0.01)\ny1 = x1**2\nfig.add_scatter(x=x1, y=y1, mode=\"lines\",name='Parabola',marker_color='#EF553B')\n\n\nfig.add_scatter(x=vecx, y=vecf, mode=\"lines+markers\", name='GD',\n                hovertemplate='x:%{x:.2f}'\n                +'<br>y:%{y:.2f}</br><extra></extra>',marker_color='#636EFA',marker_size=8)\n\nfig.update_layout(width=800,height=400,xaxis={'title':'x'},yaxis={'title':'f(x)'},title=f'number of iterations to reach the threshold {treshold:.0e}: {i}')\nfig.show()\n\n\n\n\n                                                    \nFigure 10: Gradient descent on a parabola\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nRerun the last experiment for \\(\\eta=10^{-3},10^{-1},1.1\\). What do you see?\n\n\nWe now perform a similar analysis for the linear regression problem. To this end, we choose a vector of learning rates vec_eta for the same initial condition and we apply the steepest descent algorithm.\n\n\nCode\nvec_eta = [1E-4,1E-3,1E-2,2E-2,3E-2,5E-2,1E-1]\nn_ini = len(vec_eta)\n\npini = dict(a=-1.8, b=1)\n\ndf = pd.DataFrame(columns=['a','b','label','value'])\n\nfor i in range(n_ini):\n    trackers = gradient_descent(x, y, pini, ll, niter=int(1E4),eta=vec_eta[i])\n    df1 = pd.DataFrame(data={'a':trackers['a'],'b':trackers['b'],'label':f'traj {i+1}','eta':vec_eta[i],'value':trackers['loss']})\n    df = pd.concat([d.dropna(axis=1, how=\"all\") for d in (df, df1)])\n\n\n\n\nCode\nfig = go.Figure()\nfig.add_contour(z=np.log(matz),x=vec_b, y=vec_a,\n                hovertemplate=\n                    'a:%{y:.2f}'\n                    +'<br>b:%{x:.2f}</br>'\n                    +'f:%{z:.2f}<extra></extra>')\n\nhovertemplate ='a:%{a}'+'b:%{b}<extra></extra>'\nfor i in range(n_ini):\n    visible = 'legendonly'\n    newdf = df[df.label == f'traj {i+1}']\n    fig.add_scatter(x=newdf.b, y=newdf.a, name=f'eta = {vec_eta[i]}',text=newdf.value,\n                    hovertemplate=\n                    'a:%{y:.2f}'\n                    +'<br>b:%{x:.2f}</br>'\n                    +'f:%{text:.2f}<extra></extra>',\n                    visible=visible)\n    \nlegend=dict(\n    yanchor=\"top\",\n    y=1.3,\n    xanchor=\"left\",\n    x=0.01\n    )\nd = dict(width=800,\n         height=600,\n         xaxis={'title':'b'},\n         yaxis={'title':'a'},\n         legend = legend,\n         xaxis_range=[vec_b[1], vec_b[-1]],       \n         yaxis_range=[vec_a[1], vec_a[-1]]\n        )\n\nfig.update_layout(d)\nfig.show()\n\n\n\n\n                                                    \nFigure 11: Effect of the choice of the learning rate\n\n\n\n\n\n7 Non-convex problems\nFor convex cases as the one above, the gradient descent algorithm is guaranteed to converge to the global minimum for sufficiently small \\(\\eta\\). For non-convex problems, it can instead get stuck on local minima. Indeed, in practical ML trainings, we hardly ever reach the global optimum, but it is usually sufficient to reach a local one that is close enough. Let’s see a visual example of this:\n\ndef f_nc(x): \n    return (x+1)**2*(x-2)**2 + 2*x\n\ndef grad_f_nc(x): \n    return 2*(x+1)*(x-2)*(2*x-1) + 0.2\n\nWe now proceed to do the same descent from two different points in the parameter space:\n\nn_iter = 20\neta = 1E-2\n\n# Point one: converges to local minima\nx0 = 2.5\nvecx = np.zeros(n_iter+1)\nvecf = np.zeros(n_iter+1)\n\nvecx[0] = x0\nvecf[0] = f_nc(x0)\n\nfor i in np.arange(n_iter):\n    vecx[i+1] = gd_step(vecx[i], grad_f_nc)  \n    vecf[i+1] = f_nc(vecx[i+1])\n\n# Point two: converges to global minima\nx0 = -1.4\nvecx_div = np.zeros(n_iter+1)\nvecf_div = np.zeros(n_iter+1)\n\nvecx_div[0] = x0\nvecf_div[0] = f_nc(x0)\n\nfor i in np.arange(n_iter):\n    vecx_div[i+1] = gd_step(vecx_div[i], grad_f_nc) \n    vecf_div[i+1] = f_nc(vecx_div[i+1])\n\n\n\nCode\nfig = go.Figure()\n\nx1 = np.arange(-2,3,0.01)\ny1 = f_nc(x1)\nfig.add_scatter(x=x1, y=y1, mode=\"lines\",name='Parabola',marker_color='#EF553B')\n\n\nfig.add_scatter(x=vecx, y=vecf, mode=\"markers\", name='GD local minima',\n                hovertemplate='x:%{x:.2f}'\n                +'<br>y:%{y:.2f}</br><extra></extra>',marker_color='#636EFA',marker_size=8)\n\nfig.add_scatter(x=vecx_div, y=vecf_div, mode=\"markers\", name='GD true minima',\n                hovertemplate='x:%{x:.2f}'\n                +'<br>y:%{y:.2f}</br><extra></extra>',marker_color='#2ECC71',marker_size=8)\n\nfig.update_layout(width=800,height=400,xaxis={'title':'x'},yaxis={'title':'f(x)'})\nfig.show()\n\n\n\n\n                                                    \nFigure 12: Gradient descent on a parabola\n\n\n\nThis showcases the importance, in non-convex cases, which are most of ML cases, to perform multiple random initializations of our model / training, because:\n\nWe may have not found the correct solution because of an “unlucky” start.\nWe may have found the correct solution by luck, and restarting the training does not find it again. We refer here then to the “robustness” of the model. A robust model can function under any conditions.\n\n\n\n8 Stochastic Gradient Descent\nThe gradient descent algorithm requires to pass through the whole training set to compute the gradient. However, in some cases, this can be quite costly. Imagine, for example, the case of linear regression with many variables and many training examples. To overcome this limitation, computer scientists have designed a stochastic alternative to gradient descent: the stochastic gradient descent (SGD).\n\n\n\n\n\n\nNote\n\n\n\nWhile stochastic gradient descent is not very relevant for the case of the linear regression with two parameters, it will become very important in the case of neural networks. We here take the simplicity of the loss landscape of such model to illustrate the main properties of stochastic gradient descent.\n\n\nThe main idea behind stochastic gradient descent is to approximate the loss function of the training set by the gradient of a single or just few training samples. While, each gradient step is a relatively bad approximation, the random walk followed by the aglorithm eventually converges to the direction of the steepest descent. This can be intuitively seen by noting that the mean of the gradient of several training points is pointing towards the steepest descent.\nWe now have two extreme cases: the gradient descent algorithm with no stochasticiy and the stochastic gradient descent with full stochasticity. This version of the stochastic gradient descent can be very unstable and take extremely long times to converge. Thus, it is desirable to find a middle ground: minibacth gradient descent. In this case, rather than taking the gradient over a single training example, we consider a batch size \\(BS\\), i.e. the number of training samples in the stochastic gradient descent loop. This way, we obtain a better estimate of the gradient while preserving some of its stochasticity.\nThe pseudocode looks like:\n\n\n\n\n\n\nPseudocode\n\n\n\n\nChoose an initial condtion for the paramaters: 𝑎0 and 𝑏0\nChoose a learning rate \\(\\eta\\) and batch size \\(BS\\)\nRepeat until convergence:\n\nShuffle the training set\nIterate over every batch:\n\nCompute gradient based on average gradient in batch\nUpdate params as in GD\n\\[\\begin{aligned}\n      &a_{i+1}=a_i-\\eta \\, \\partial_a MSE\\\\\n      &b_{i+1}=b_i-\\eta \\, \\partial_b MSE\n  \\end{aligned}\\]\n\n\n\n\n\nWe illustrate the stochastic gradient descent with the following code snippet for the same initial condition and for a minibatch of size BS=20.\n\nn_ini = 5\npini = dict(a=2, b=1)\n\ndf = pd.DataFrame(columns=['a','b','label','value','niter'])\n\n# Let's first consider the gradient descent as before\ntrackers = gradient_descent(x, y, pini, ll, niter=int(1E3))\ndf1 = pd.DataFrame(data={'a':trackers['a'],'b':trackers['b'],'label':f'GD','value':trackers['loss'],'niter':np.arange(len(trackers['a']))})\ndf = pd.concat([d.dropna(axis=1, how=\"all\") for d in (df, df1)])\n\n# And now consider instead SGD\nfor i in range(n_ini):\n    trackers = sgd(x,y, pini, ll, niter=int(1E2), bs = 20)\n    df1 = pd.DataFrame(data={'a':trackers['a'],'b':trackers['b'],'niter':np.arange(len(trackers['a'])),'label':f'traj {i+1}','value':trackers['loss']})\n    df = pd.concat([d.dropna(axis=1, how=\"all\") for d in (df, df1)])\n\n\n\nCode\nfig = px.line(df, y='value', markers=True, animation_frame='label')\n\nfig[\"layout\"].pop(\"updatemenus\") # optional, drop animation buttons\nfig.update_layout(xaxis_title='iterations', yaxis_title='Loss')\nfig.show()\n\n\n\n\n                                                    \nFigure 13: Loss of the gradient descent and the stochastic gradient descent for different shufflings\n\n\n\nFigure 13 depcits the loss function of the gradient descent and the stochastic gradient descent algorithm for different shufflings. While both algorithms converge to a similar value of the Loss function, we can nicely observe the fluctuations coming from the stochasticity of the minibatches3. The latter can be also seen in Figure 14. It is interesting to notice in that last figure that the stochastic gradient descent fuctuates more in the \\(a\\)-direction. This fact is well known for SGD and can be improved with more avolved algorithms such as momentum, nesterov or Adam.\n\n\nCode\namin, amax = df.a.min()*0.8,df.a.max()*1.1\nbmin, bmax = df.b.min()*0.8,df.b.max()*1.1\nn = 100\nvec_a = np.arange(amin, amax,(amax-amin)/n)\nvec_b = np.arange(bmin, bmax,(bmax-bmin)/n)\nmatz = np.zeros((vec_a.size,vec_b.size))\n\nfor i, a1 in enumerate(vec_a):\n    for j, b1 in enumerate(vec_b):\n        params = dict(a=a1, b=b1)\n        matz[i,j] = MSE(x,y,line,params)\n        \nfig = go.Figure()\nfig.add_contour(z=np.log(matz),x=vec_b, y=vec_a,\n                hovertemplate=\n                    'a:%{y:.2f}'\n                    +'<br>b:%{x:.2f}</br>'\n                    +'f:%{z:.2f}<extra></extra>')\n\nhovertemplate ='a:%{a}'+'b:%{b}<extra></extra>'\nfor i in range(n_ini):\n    visible = True if i == 0 else 'legendonly'\n    newdf = df[df.label == f'traj {i+1}']\n    fig.add_scatter(x=newdf.b, y=newdf.a, name=f'traj {i+1}',text=newdf.value, mode='lines+markers',\n                    hovertemplate=\n                    'a:%{y:.2f}'\n                    +'<br>b:%{x:.2f}</br>'\n                    +'f:%{text:.2f}<extra></extra>',\n                    visible=visible)\n    \nnewdf = df[df.label == f'GD']\nfig.add_scatter(x=newdf.b, y=newdf.a, name=f'GD',text=newdf.value,\n                mode='lines', line={'dash': 'dash','color':'White'},\n                hovertemplate=\n                    'a:%{y:.2f}'\n                    +'<br>b:%{x:.2f}</br>'\n                    +'f:%{text:}<extra></extra>')\n\nlegend=dict(\n    yanchor=\"top\",\n    y=1.3,\n    xanchor=\"left\",\n    x=0.01\n    )\nd = dict(width=800,\n         height=600,\n         xaxis={'title':'b'},\n         yaxis={'title':'a'},\n         legend = legend\n        )\n\nfig.update_layout(d)\nfig.show()\n\n\n\n\n                                                    \nFigure 14: Loss landscape of the gradient descent and the stochastic gradient descent for different shufflings\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nRerun the last experiment with different minibatch sizes. What do you see?\n\n\nWe finish this section by observing how the line adjust to our dataset in terms of the iterations for the GD and SGD. The results are presented in Figure 15 for the gradient descent.\n\n\nCode generating the data of the figure\ni =1\nlabel = 'GD'#f'traj {i+1}' #change it if you want to see the SGD trajectory\nx1 = np.array([x.min(),x.max()])\nnewdf = df[df.label == label]\na, b, mse = newdf.a.to_numpy(), newdf.b.to_numpy(), newdf.value.to_numpy()\ny1 = np.einsum('i,j->ij',a,x1)+np.tile(b,(2,1)).T\n\n\n\n\nCode\nframes = [go.Frame(data=[go.Scatter(x=x1, y=y1[i,:],mode='lines')],layout=go.Layout(title_text=f'step:{i}, MSE:{mse[i]:.2f}')) for i in range(a.size)]\nbuttons = [dict(label=\"Play\",method=\"animate\",\n                args=[None, {\"frame\": {\"duration\": 100, \"redraw\": True},\n                             \"fromcurrent\": True, \n                             \"transition\": {\"duration\": 300,\"easing\": \"quadratic-in-out\"}}]),\n           dict(label=\"Pause\",method=\"animate\",\n                args=[[None], {\"frame\": {\"duration\": 0, \"redraw\": False},\"mode\": \"immediate\",\"transition\": {\"duration\": 0}}]),\n          dict(label=\"Restart\",method=\"animate\",\n                args=[None])]\n\nFig = go.Figure(\n    data=[go.Scatter(x=x1, y= y1[0,:],mode='lines',name = 'line'),\n          go.Scatter(x=x, y=y, mode=\"markers\", name='data',\n                hovertemplate='x:%{x:.2f}'\n                +'<br>y:%{y:.2f}</br><extra></extra>')],\n    layout=go.Layout(\n        xaxis=dict(range=[x.min()-2, x.max()+2], autorange=False),       \n        yaxis=dict(range=[y.min()-2, y.max()+2], autorange=False),\n        updatemenus=[dict(\n            type=\"buttons\",\n            buttons=buttons)]\n    ),\n    frames= frames\n)\n\nFig.show()\n\n\n\n\n                                                    \nFigure 15: Animation of t\n\n\n\n\n\n\n\n\n\nReferences\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. The MIT Press. https://doi.org/10.5555/3086952.\n\nFootnotes\n\n\nThe literature also uses the terms of criterion or cost, error, or objective functions. Their definitions are not very strict. Following (Goodfellow, Bengio, and Courville 2016): ‘’The function we want to minimize or maximize is called the objective function, or criterion. When we are minimizing it, we may also call it the cost function, loss function, or error function. In this book, we use these terms interchangeably, though some machine learning publications assign special meaning to some of these term’’. For example, loss function may be defined for a single data point, the cost or error function may be a sum of loss functions, so check the definitions used in each paper.↩︎\nFor classification, a~more intuitive measure of the performance could be, e.g., accuracy, which is the ratio between the number of correctly classified examples and the data set size. Note, however, that gradient-based optimization requires measures of performance that are smooth and differentiable. These conditions distinguish loss functions from evaluation metrics such as accuracy, recall, precision, etc.↩︎\nBeware that the notion of iteration is different for gradient descent and for stochastic gradient descent. For the former, an iteration corresponds to an epoch (the whole training set), while for the latter it corresponds to a minibatch.↩︎"
  },
  {
    "objectID": "course/linear_models/polynomial_fit.html",
    "href": "course/linear_models/polynomial_fit.html",
    "title": "Polynomial fit",
    "section": "",
    "text": "1 Fitting a noisy polynomial curve\nWe now consider the task of fitting a polynomial curve with noise. Despite dealing with higher order polynomials than before, this problem can also be rewritten as a linear regression task. Let us first generate a dataset with the help of the function noisy_curve, which maps \\(x\\) to a polynomial of degree \\(d\\) \\(f(x)=\\mathbf{w}^T\\mathbf{x}+\\text{noise}\\) and where \\(\\mathbf{x}=(x^0,x^1,\\ldots,x^d)\\).\n\ncoeffs = [2., 1., 0., 1.]\nx, y = noisy_curve(coeffs, interval=[-3., 1.5], noise=[0., 2.])\n\nFigure 1 shows the generated data with the ground truth.\n\n\n\nFigure code\nfig = go.Figure()\nfig.add_scatter(x=x, y=y, mode=\"markers\", name='data',\n                hovertemplate='x:%{x:.2f}'\n                +'<br>y:%{y:.2f}</br><extra></extra>')\nx1 = np.linspace(x.min(),x.max(),num=50)\nx1, y1 = noisy_curve(coeffs,x=x1)\nfig.add_scatter(x=x1, y=y1, mode=\"lines\",name='Ground Truth')\nfig.update_layout(width=800,height=400,xaxis_title='x',yaxis_title='f(x)')\nfig.show()\n\n\n\n\n        \n        \n        \n(a) Parabola with Gaussian noise\n\n\n\n\n\n                                                    \n(b) ?(caption)\n\n\nFigure 1: ?(caption)\n\n\nAs in the previous section, we choose the mean square error for the loss function.\n\n\n2 Polynomial fit as multivariable linear regression\nThe polynomial regression can be seen as a linear regression of multiple variables with the help of the following trick. Let us rewrite \\(f(x)=\\sum_{i=0}^d w_i x^i\\) as \\(f(x)=\\mathbf{w}^T\\mathbf{x}\\), where \\(\\mathbf{x}=(x^0, x^1, \\ldots, x^d)\\). Now we can use this vectorial form, together with the fact that \\(f(x)\\) is linear w.r.t to \\(\\mathbf{x}\\) (while being non-linear w.r.t. to \\(\\mathbf{x}\\) to draw an analogy with linear regression. If we consider for instance the mean square error loss:\n\\[\\begin{aligned}\nMSE &= \\sum_{i=1}^N (\\mathbf{w}^T\\mathbf{x}_i-y_i)^2\\\\\n&= \\parallel \\mathbf{y}-X\\mathbf{w}\\parallel^2\\\\\n&=(\\mathbf{y}-X\\mathbf{w})^T(\\mathbf{y}-X\\mathbf{w}),\n\\end{aligned}\\]\nwhere\n\\[ X=\n\\begin{pmatrix}\n1 & x_1^1 & \\ldots & x_1^d\\\\\n1 & x_2^1 & \\ldots & x_2^d \\\\\n\\vdots& \\vdots & \\vdots & \\vdots\\\\\n1 & x_N^1 & \\ldots & x_N^d\n\\end{pmatrix} .\\]\nWe now take the derivative with respect to all the weights \\(w\\) and set it to \\(0\\). We therefore find the estimator\n\\[\\mathbf{w} =(X^TX)^{-1}X^T\\mathbf{y}.\\]\n\n\n\n\n\n\nExercise\n\n\n\nImplement a exact_poly_fit(x, y, degree) that numerically computes the weights w with the expression above. You can use np.linalg.inv to do the matrix inversion.\n\n\n\n# Your code here\ndef exact_poly_fit(x,y,degree):\n    \n    return w\n\n\n\nSolution\ndef exact_poly_fit(x,y,degree):\n    mat = np.ones((x.size,degree+1))\n    for i in range(degree):\n        mat[:,i+1] = x**(i+1)\n    w = np.linalg.inv(np.transpose(mat)@mat)@np.transpose(mat)@y\n    return w\n\n\nLet us run the algorithm for our dataset. We will fit a 3rd degree polynomial and compare the resulting parameters to the original ones.\n\nw_best = exact_poly_fit(x,y,3)\np1, p2 = dict(coeffs=w_best), dict(coeffs=coeffs)\nprint (np.array([w_best,coeffs]))\nprint (f'MSE Best parameters: {MSE(x,y,curve,params=p1):.3f}')\nprint(f'MSE Original parameters: {MSE(x,y,curve,params=p2):.3f}')\n\nThe algorithm does a fairly good job. It is quite interesting to have a look at the mean square error on this dataset. The best parameters have a lower loss than the actual true parameters! We will come back to this point later ;)\n\n\n3 Stochastic Gradient Descent\nJust like we did with the linear regression, we can optimize our model parameters with a gradient-based method. Let us see what we obtain with the stochastic gradient descent algorithm for the poynomial fit. Figure 2 shows the fit after optimization.\n\ncoeffs0 = np.random.normal(loc=0,scale=0.1,size=4)\n\nll = dict(loss=MSE, grads=grad_MSE_pr, fun=curve)\n\npini = dict(coeffs=coeffs0)\n\ndf = pd.DataFrame(columns=['coeffs','value'])\n\ntrackers = sgd(x,y, pini, ll, eta=1E-5, niter=int(1E4))\ndf1 = pd.DataFrame(data={'coeffs':trackers['coeffs'],'value':trackers['loss']})\ndf = pd.concat([d.dropna(axis=1, how=\"all\") for d in (df, df1)])\n\nprint(f'final Loss:{df[\"value\"].iloc[-1]:3f}')\nprint (df[\"coeffs\"].iloc[-1])\nprint(coeffs)\n\nfinal Loss:4.043960\n[1.28389833 0.55562468 0.44173774 1.18165629]\n[2.0, 1.0, 0.0, 1.0]\n\n\n\n\nFigure code\ncc = df[\"coeffs\"].iloc[-1]\n\nfig = go.Figure()\nfig.add_scatter(x=x, y=y, mode=\"markers\", name='data',\n                hovertemplate='x:%{x:.2f}'\n                +'<br>y:%{y:.2f}</br><extra></extra>')\nx1 = np.linspace(x.min(),x.max(),num=50)\ny1 = curve(x1,cc)\nfig.add_scatter(x=x1, y=y1, mode=\"lines\",name='Fit')\nfig.update_layout(width=800,height=400,xaxis_title='x',yaxis_title='f(x)')\nfig.show()\n\n\n\n\n                                                    \nFigure 2: Polynomial fit of the data\n\n\n\nFigure 3 shows how the algotihm adjusts the polynomial curve during the optimization.\n\n\nFigure code\nstep = 100\nx1 = np.linspace(x.min(),x.max(),num=50)\n\nframes = [go.Frame(data=[go.Scatter(x=x1, y=curve(x1,df[\"coeffs\"].iloc[i*step]),mode='lines')],layout=go.Layout(title_text=f'step:{i*step}, MSE:{df[\"value\"].iloc[i*step]:.2f}')) for i in range(len(df)//step)]\n\nbuttons = [dict(label=\"Play\",method=\"animate\",\n                args=[None, {\"frame\": {\"duration\": 100, \"redraw\": True},\n                             \"fromcurrent\": True, \n                             \"transition\": {\"duration\": 300,\"easing\": \"quadratic-in-out\"}}]),\n           dict(label=\"Pause\",method=\"animate\",\n                args=[[None], {\"frame\": {\"duration\": 0, \"redraw\": False},\"mode\": \"immediate\",\"transition\": {\"duration\": 0}}]),\n          dict(label=\"Restart\",method=\"animate\",\n                args=[None,{\"frame\": {\"duration\": 100, \"redraw\": True}}])]\n\nFig = go.Figure(\n    data=[go.Scatter(x=x1, y= curve(x1,df[\"coeffs\"].iloc[0]),mode='lines',name = 'line',\n                     hovertemplate='x:%{x:.2f}'+'<br>y:%{y:.2f}</br><extra></extra>'),\n          go.Scatter(x=x, y=y, mode=\"markers\", name='data',\n                hovertemplate='x:%{x:.2f}'\n                +'<br>y:%{y:.2f}</br><extra></extra>')],\n    layout=go.Layout(\n        xaxis=dict(range=[x.min()-2, x.max()+2], autorange=False),       \n        yaxis=dict(range=[y.min()-2, y.max()+2], autorange=False),\n        updatemenus=[dict(\n            type=\"buttons\",\n            buttons=buttons)]\n    ),\n    frames= frames\n)\nFig.update_layout(xaxis_title='x',yaxis_title='f(x)')\nFig.show()\n\n\n\n\n                                                    \nFigure 3: Animation of the optimization\n\n\n\n\n\n4 Overfitting\nUp until now, we have not discussed a very important hyper-parameter in this problem: the degree of the polynomail. In particular, we have fixed the degree to be the one of the original function. However, this is typically unknown in practice and we either rely on educated guesses or we resort to perform various fits for different degrees and keep the best one (but what is the best one?! We’ll see). To this end, we use the polyfit subroutine of numpy, which is much more stable than the exact_poly_fit we prepared.\n\nvec_cc = []\nmse_t = []\nmse_v = []\n\nnpoly = 20\nndata = 50\nfor i in np.arange(1,npoly):\n    vec_cc.append(polyfit(x[:ndata],y[:ndata],deg=i))\n    p1, p2 = dict(coeffs=vec_cc[i-1]), dict(coeffs=vec_cc[i-1])\n    mse_t.append(MSE(x[:ndata], y[:ndata],curve,params=p1))\n    mse_v.append(MSE(x[ndata:], y[ndata:],curve,params=p2))\n\nFigure 4 shows the loss function over the training data as function of the polynomial degree. At a first glance, it looks like a higher degree gives rise to better loss. However, in Figure 5, we can really see the overfitting of the higher order polynomials. This can be detected by dividing the training set into two subsets: the training set and the validation set. We train the algorithm using data exclusively from the training set and, then, we evaluate its performance on the validation set. The validation set contains new data for our model, which allows us to assess how well our algorithm generalizes to unseen data. We can see that we are overfitting to the training set when we see that the loss in the validation set stagnates or increases. Turn on the orange line in Figure 4 to see it!\n\n\nFigure code\nfig = go.Figure()\nfig.add_scatter(x=np.arange(1,npoly), y=mse_t, mode='lines+markers', name='training')\nfig.add_scatter(x=np.arange(1,npoly), y=mse_v, mode='lines+markers', visible='legendonly', name='validation')\nfig.update_layout(yaxis_range=[0,20],xaxis_title='degree',yaxis_title='Loss')\n\n\n\n\n                                                    \nFigure 4: Training loss with respect to the degree of the polynomial\n\n\n\n\n\nFigure code\nfig = go.Figure()\nfig.add_scatter(x=x[:ndata], y=y[:ndata], mode=\"markers\", name='data',\n                hovertemplate='x:%{x:.2f}'\n                +'<br>y:%{y:.2f}</br><extra></extra>')\nx1 = np.linspace(x.min(),x.max(),num=50)\n\npoly = [1, 2, 3, 4, 6, 8, 10, 19]\nfor i,k in enumerate(poly):\n    visible = True if k == 0 else 'legendonly'\n    x1, y1 = noisy_curve(vec_cc[k-1],x=x1)\n    fig.add_scatter(x=x1, y=y1, mode=\"lines\",name=f'{k}th degree', visible=visible)\nfig.update_layout(width=800, height=400, yaxis_range=[y.min(),y.max()], xaxis_title='x', yaxis_title='f(x)')\nfig.show()\n\n\n\n\n                                                    \nFigure 5: Training loss with respect to the degree of the polynomial\n\n\n\nA typical instance for overfitting appears when we have many free parameters compared to the number of data points. Indeed, we can achieve a zero training loss with some algorithms when we have as many parameters as data points. However, this usually comes at the expense of extreme overfitting and poor generalization.\n\n\n\n\n\n\nExercise\n\n\n\nIn the experiment above, we have used 50 data points to fit up to a 19th-degree polynomial. Run the same procedure with increasingly less data, e.g., set ndata to 30, 20 and 10, and observe what happens with the resulting curves. Do we see overfitting for lower degree polynomials? Do you think these models would provide a reasonable prediction if we drew a new data sample form the same experiment?\n\n\nWe will now discuss two strategies to prevent overfitting.\n\n\n5 More data\nAs we have seen right above, the relative number of our model parameters compared to the amount of data that we have is a key factor for overfitting. If having less data makes our model more prone to overfitting, having more data naturally helps us mitigate it.\nTherefore, let us generate more samples.\n\nnsamples = int(1E3)\nxn, yn = noisy_curve(coeffs, interval = [-3,1.5], noise=[0.,2], nsamples=nsamples)\n\nWe then perform the split between the training set and validation set and compute the loss function for both sets.\n\nvec_cc, mse_t,mse_v  = [], [], []\n\nnpoly = 20\nndata = int(0.8*nsamples) #We set 80% of the data for the training and 20% for the validation\n\nfor i in np.arange(1,npoly):\n    vec_cc.append(polyfit(xn[:ndata],yn[:ndata],deg=i))\n    p1, p2 = dict(coeffs=vec_cc[i-1]), dict(coeffs=vec_cc[i-1])\n    mse_t.append(MSE(xn[:ndata], yn[:ndata], curve, params=p1))\n    mse_v.append(MSE(xn[ndata:], yn[ndata:], curve, params=p2))\n\nFigure 6 shows the comparison between the trainng loss and the validation loss for different degrees. We observe a much better behavior than in the previous case with small dataset. This is also confirmed in Figure 7 where we can clearly see the advantage of using a larger dataset.\n\n\nFigure code\nfig = go.Figure()\nfig.add_scatter(x=np.arange(1,npoly), y=mse_t, mode='lines+markers', name='training')\nfig.add_scatter(x=np.arange(1,npoly), y=mse_v, mode='lines+markers', visible='legendonly', name='validation')\nfig.update_layout(yaxis_range=[0,10], xaxis_title='degree', yaxis_title='Loss')\n\n\n\n\n                                                    \nFigure 6: Training loss with respect to the degree of the polynomial for a larger dataset\n\n\n\n\n\nFigure code\nfig = go.Figure()\nfig.add_scatter(x=xn[:ndata], y=yn[:ndata], mode=\"markers\", name='data',\n                hovertemplate='x:%{x:.2f}'\n                +'<br>y:%{y:.2f}</br><extra></extra>')\nx1 = np.linspace(x.min(),x.max(),num=50)\n\npoly = [1, 2, 3, 4, 6, 8, 10, 19]\nfor i,k in enumerate(poly):\n    visible = True if k == 0 else 'legendonly'\n    x1, y1 = noisy_curve(vec_cc[k-1],x=x1)\n    fig.add_scatter(x=x1, y=y1, mode=\"lines\",name=f'{k}th degree', visible=visible)\nfig.update_layout(width=800, height=400, yaxis_range=[y.min(),y.max()],xaxis_title='x',yaxis_title='f(x)')\nfig.show()\n\n\n\n\n                                                    \nFigure 7: Polynomial fit for the different degrees\n\n\n\n\n\n6 Regularization\nAnother way to avoid overfitting is regularization. The idea here is to add a term to the loss function that prevents the weights to behave in an ill-deined way. An example of such regularization is the \\(l_2\\) regularization which consists in adding to the loss function the term \\(\\alpha\\parallel \\mathbf{w} \\parallel^2\\). Intituitively, this parabolic term avoids to have exploding weights. The regression with such regularization is called Ridge regression.\n\n\n\n\n\n\nExercise\n\n\n\nFor the analytical solution, show that the regularization term gives rise to the following solution\n\\[w =(X^TX+2\\alpha \\mathbb{1})^{-1}X^T\\mathbf{y}.\\]\n\n\n\n\n\n\n\n\nExercise\n\n\n\nFor the gradient descent algorithm, show that the regularization leads to the update rule for each weight \\(w\\)\n\\[ w_{i+1} = (1-2\\, \\alpha\\,\\eta) w_i - \\eta \\partial_w MSE \\]\n\n\nWe here use an implementation of scikit learn. To this end, we define a function that creates the matrix \\(X\\).\n\ndef poly_cond(x, n):\n    matx = np.zeros((x.size,n))\n    for i,k in enumerate(range(1,n+1)):\n        matx[:,i] = x**k\n    return matx\n\nWe then perform the Ridge regression for the polynomials of different degrees.\n\nalpha = 0.5\n\nvec_cc, mse_t, mse_v = [], [], []\n\nnpoly = 20\nndata = 50\nfor i in np.arange(1,npoly):\n    matx = poly_cond(xn[:ndata],i)\n    reg = linear_model.Ridge(alpha=alpha)\n    reg.fit(matx,yn[:ndata])\n    c = np.insert(reg.coef_,0,reg.intercept_)\n    vec_cc.append(c)\n    p1, p2 = dict(coeffs=vec_cc[i-1]), dict(coeffs=vec_cc[i-1])\n    mse_t.append(MSE(xn[:ndata], yn[:ndata], curve, params=p1))\n    mse_v.append(MSE(xn[ndata:], yn[ndata:], curve, params=p2))\n\nFigure 8 shows the Loss funtion in terms of the degree of the polynomials. We can now see that the validation curve behaves much better for higher polymomials. The latter is also confirmed with Figure 9, which shows smaller behaviors for higher polynomials.\n\n\nFigure code\nfig = go.Figure()\nfig.add_scatter(x=np.arange(1,npoly), y=mse_t, mode='lines+markers', name='training')\nfig.add_scatter(x=np.arange(1,npoly), y=mse_v, mode='lines+markers', visible='legendonly', name='validation')\nfig.update_layout(yaxis_range=[0,10],xaxis_title='Loss',yaxis_title='degree')\n\n\n\n\n                                                    \nFigure 8: Polynomial fit for the different degrees\n\n\n\n\n\nFigure code\nfig = go.Figure()\nfig.add_scatter(x=x[:ndata], y=y[:ndata], mode=\"markers\", name='data',\n                hovertemplate='x:%{x:.2f}'\n                +'<br>y:%{y:.2f}</br><extra></extra>')\nx1 = np.linspace(x.min(),x.max(),num=50)\n\npoly = [1, 2, 3, 4, 6, 8, 10, 19]\nfor i,k in enumerate(poly):\n    visible = True if k == 0 else 'legendonly'\n    x1, y1 = noisy_curve(vec_cc[k-1],x=x1)\n    fig.add_scatter(x=x1, y=y1, mode=\"lines\",name=f'{k}th degree', visible=visible)\nfig.update_layout(width=800, height=400, yaxis_range=[yn.min()-1,yn.max()+1])\nfig.show()\n\n\n\n\n                                                    \nFigure 9: Polynoms for the different degrees\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nBalancing two loss terms: change the value of \\(\\alpha\\) from 0 to 100 and perform the same analysis. For a fixed degree (e.g. 3), plot the value of the MSE as a function of \\(\\alpha\\) for both the training and the validation dataset. What do you see?\n\n\n\n# Your code here\n\n\n\nSolution\nalphas = np.linspace(0,100,100)[1:]\n\nmse_t_alpha, mse_v_alpha = [], []\n\nnpoly = 20\nndata = 50\n\npoly_degree = 3\n\nfor alpha in alphas:\n\n    matx = poly_cond(xn[:ndata], poly_degree)\n    reg = linear_model.Ridge(alpha=alpha, max_iter = int(1E5))\n    reg.fit(matx,yn[:ndata])\n    c = np.insert(reg.coef_,0,reg.intercept_)\n\n    p1, p2 = dict(coeffs=c), dict(coeffs=c)\n    mse_t_alpha.append(MSE(xn[:ndata], yn[:ndata], curve, params=p1))\n    mse_v_alpha.append(MSE(xn[ndata:], yn[ndata:], curve, params=p2))\n\n\n\n\nFigure code\nfig = go.Figure()\nfig.add_scatter(x=alphas, y=mse_t_alpha, mode='lines', visible='legendonly', name='training')\nfig.add_scatter(x=alphas, y=mse_v_alpha, mode='lines', visible='legendonly', name='validation')\n\n\n\n\n                                                    \nFigure 10: Effect of regularization parameter \\(\\alpha\\) on the training and validation loss"
  },
  {
    "objectID": "course/applications/applications-index.html",
    "href": "course/applications/applications-index.html",
    "title": "Typical machine learning applications",
    "section": "",
    "text": "1 Recap\nSo far, we have visited the fundamentals of machine learning. We have tackled both regression and classification tasks building the algorithms from scratch. This has allowed us to introduce key machine learning concepts such as loss function, stochastic gradient descent, overfitting or regularization, which are transferrable to any machine learning task and architecture.\nBuilding a polynomial regression from scratch, we have gained intuition about what are the model parameters, how to compute their gradients and how to update them to obtain a better model. Then, with the logistic regression, we have learned the difference between a loss function and a metric. Finally, with the perceptron, we have mastered a fundamental building block of many complex machine learning architectures, as well as developed further intuition about classification tasks in multiple dimensions.\n\n\n2 Next steps\nFrom now on, we will take a more applied approach. We will use neural networks to tackle more challenging problems than what we have done so far. However, the basic principles remain the same.\nIn this lecture, we provide an overview of various prototypical machine learning tasks over different kinds of data. This will provide context for the upcomming lessons as well as (hopefully) some motivation! We will focus on three main types of data: images, text and structured data. These give raise to computer vision, natural language processing and tabular data tasks, respectively.\nSeat back and enjoy the ride!"
  },
  {
    "objectID": "course/applications/applications-tabular.html",
    "href": "course/applications/applications-tabular.html",
    "title": "Typical tasks with structured data",
    "section": "",
    "text": "Note\n\n\n\nIn this notebook we use random forests, which is a machine learning technique built upon decision trees. Furthermore, we use the fastai (Howard and Gugger 2020) library to download the data for the different tasks and easily train our models.\n\n\n\n1 Introduction\nTabular data or structured data problems are pretty common in the field of machine learning. It is the prototypical problem in which each sample is described by a certain set of features and, thus, the dataset can be layed out in a table (hence the name). The goal, then, is to predict the value of one of the columns based on the rest. Up until quite recently, tabular data problems where generally addressed with classical models based on decision trees, be it ensembles or gradient boosted machines. However, deep learning has proven quite successful on these tasks in the past years.\nWithin this field, we encounter problems of all kinds, from telling flower types apart given a feature list, to assessing whether to give a loan to a bank client. Unfortunately, tabular data problems are much less nicer to show than computer vision tasks and so this part will be less flashy than the others. In order to illustrate the process, we will address a regression problem to infer the auction prices of bulldozers that was a kaggle competition. We will solve the same problem with random forests and neural networks in order to see what differences we find with them.\n\n\n\n\n\n\nNote\n\n\n\nWe take a regression example here, but tabular data problems can also be classification tasks and all the processes shown may be applied indistinctively.\n\n\nLet’s have a look at the data.\n\nURLs\n\n\nuntar_data(URLs.PETS)\n\n\n\nCode\npath = URLs.path('bluebook')\n\ndf = pd.read_csv(path/'TrainAndValid.csv', low_memory=False)\ndf_test = pd.read_csv(path/'Test.csv', low_memory=False)\ndf.head()\n\n\nFileNotFoundError: [Errno 2] No such file or directory: '/home/gorka/.fastai/archive/bluebook/TrainAndValid.csv'\n\n\nEach bulldozer is described by 53 features that constitute the columns of the dataset.\n\n\nCode\ndf.columns\n\n\nIndex(['SalesID', 'SalePrice', 'MachineID', 'ModelID', 'datasource',\n       'auctioneerID', 'YearMade', 'MachineHoursCurrentMeter', 'UsageBand',\n       'saledate', 'fiModelDesc', 'fiBaseModel', 'fiSecondaryDesc',\n       'fiModelSeries', 'fiModelDescriptor', 'ProductSize',\n       'fiProductClassDesc', 'state', 'ProductGroup', 'ProductGroupDesc',\n       'Drive_System', 'Enclosure', 'Forks', 'Pad_Type', 'Ride_Control',\n       'Stick', 'Transmission', 'Turbocharged', 'Blade_Extension',\n       'Blade_Width', 'Enclosure_Type', 'Engine_Horsepower', 'Hydraulics',\n       'Pushblock', 'Ripper', 'Scarifier', 'Tip_Control', 'Tire_Size',\n       'Coupler', 'Coupler_System', 'Grouser_Tracks', 'Hydraulics_Flow',\n       'Track_Type', 'Undercarriage_Pad_Width', 'Stick_Length', 'Thumb',\n       'Pattern_Changer', 'Grouser_Type', 'Backhoe_Mounting', 'Blade_Type',\n       'Travel_Controls', 'Differential_Type', 'Steering_Controls'],\n      dtype='object')\n\n\nThe first thing to do is to identify our target value. In this case, it is the SalePrice column and, in fact, we want to predict the logarithm of the price, as stated in the competition. Then, these problems heavily rely on feature engineering, which consists on adding additional (smart) features that may be informative for the task. For instance, from a single date we can extract the day of the week, whether it was weekend or holidays, beginning or end of the month, etc. We could even figure out the weather if needed!\nCompetitions such as this one are won, in general, by those who can come up with the best relevant features for the task at hand.\n\n\nCode\ndep_var = 'SalePrice'\ndf[dep_var] = np.log(df[dep_var])\n\n\n\n\n\n\n\n\nExpand to learn about the training details\n\n\n\n\n\nGenerally, besides feature engineering, one of the key points in this kind of problems is properly handling categorical and numerical values as well as missing values. For instance, ProductSize is a categorical feature which takes values ‘Large’, ‘Large / Medium’, ‘Medium’, ‘Small’, ‘Mini’ and ‘Compact’. The model does not konw how to process these strings and so we convert them into numerical values assigning a number to each category. These numbers have essentially no meaning. However, given the nature of decision trees, it is convenient that ordinal categories, such as this one, are ordered so that increasing numbers, for example, represent increasing categorical sizes. Numerical values, in turn, should be properly normalized (for neural networks) and, finally, missing values are filled with the mean value of the column and a new column indicating wether it was filled or not is added.\nChoosing the right validation set is also extremely important. Given that this is a price forecasting task, we will take the latest sales within the training dataset to be our validation set.\n\n\n\n\n\nCode\nsizes = 'Large','Large / Medium','Medium','Small','Mini','Compact'\ndf['ProductSize'] = df['ProductSize'].astype('category')\ndf['ProductSize'].cat.set_categories(sizes, ordered=True, inplace=True)\n\ndf = add_datepart(df, 'saledate')\ndf_test = add_datepart(df_test, 'saledate')\n\n# Split train/validation\ncond = (df.saleYear<2011) | (df.saleMonth<10)\ntrain_idx, val_idx = np.where(cond)[0], np.where(~cond)[0]\nsplits = (list(train_idx), list(val_idx))\n\n# Handle continuous and categorical variables\nprocs = [Categorify, FillMissing]\ncont, cat = cont_cat_split(df, 1, dep_var=dep_var)\nto = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits)\n\n\n\n\n2 Random forests\nRandom forests are the go-to technique to deal with tabular data. They are extremely powerful and extremely easy to set up and train thanks to libraries like sci-kit learn.\nLet’s fit a random forest regressor to the dataset and evaluate its performance. We evaluate the root mean square error (RMSE) of the price prediction on the validation set.\n\n\nCode\ndef rf(xs, y, n_estimators=40, max_samples=200_000, max_features=0.5,\n       min_samples_leaf=5, **kwargs):\n    \"Builds and fits a `RandomForestRegressor`.\"\n    return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators,\n        max_samples=max_samples, max_features=max_features,\n        min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)\n\ndef r_mse(pred, y):    return round(math.sqrt(((pred-y)**2).mean()), 6)\ndef m_rmse(m, xs, y): return r_mse(m.predict(xs), y)\n\n\nx, y = to.train.xs, to.train.y\nvalid_x, valid_y = to.valid.xs, to.valid.y\n\nm = rf(x, y)\nm_rmse(m, valid_x, valid_y)\n\n\n0.232313\n\n\nThe RMSE is 0.23 in the logarithm of the price. Let’s see how to improve on this. Random forests are quite easy to interpret and we can see, for instance, what are the most relevant features as well as those that are redundant.\nLet’s have a look at the feature importances of the most significant ones (top 30).\n\n\nCode\ndef plot_feature_importances(m, df, top=30):\n    fi = pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}).sort_values('imp', ascending=False)\n    fi[:top].plot('cols', 'imp', 'barh', figsize=(12, 8), legend=False)\n    return fi\n\nfi = plot_feature_importances(m, x);\n\n\n\n\n\nWe can see that some features are much more relevant than others. For instance, the year in which the bulldozer was made and its size seem to be the most significant aspects when it comes to determining its selling price, while things such as the transmission mechanism or the day it is being sold barely have an impact.\nWe will remove the least relevant features and retrain our model, leading to a simpler regressor. Therefore, if the performance is similar, it means that it will be able to generalize better. Evaluating the RMSE of the retrained model in the validation set we see that it is not only similar but, actually, a little bit better.\n\n\nCode\nto_keep = fi[fi.imp>0.005].cols\nx_i, valid_x_i = x[to_keep], valid_x[to_keep]\nm = rf(x_i, y)\nm_rmse(m, valid_x_i, valid_y)\n\n\n0.231334\n\n\nBesides feature importance, we can also see which of these features are redundant or provide similar information. Removing redundant features makes our model simpler and more robust, meaning that it will generalize better to unseen data.\n\n\nCode\ndef cluster_columns(df, figsize=(10,6), font_size=12):\n    corr = np.round(scipy.stats.spearmanr(df).correlation, 4)\n    corr_condensed = hc.distance.squareform(1-corr)\n    z = hc.linkage(corr_condensed, method='average')\n    fig = plt.figure(figsize=figsize)\n    hc.dendrogram(z, labels=df.columns, orientation='left', leaf_font_size=font_size)\n    plt.show()\n    \ncluster_columns(x_i)\n\n\n\n\n\nThose features that are merged together at the rightmost part of the plot are the ones that are the most similar. For instance, ‘SaleYear’ and ‘SaleElapsed’ provide the same information but in different formats: the first states the year it was sold and the second tells us how many years have passed since it was sold. Just like with irrelevant features, we can remove some of these redudant ones and re-evaluate our model.\n\n\nCode\nto_drop = ['saleYear', 'ProductGroupDesc', 'fiBaseModel', 'Grouser_Tracks']\nx_ic, valid_x_ic = x_i.drop(to_drop, axis=1), valid_x_i.drop(to_drop, axis=1)\nm = rf(x_ic, y)\nm_rmse(m, valid_x_ic, valid_y)\n\n\n0.232922\n\n\nDropping the least informative features and some of the redundant ones, we have greatly simplified our model while keeping the same performance. This will allow the model to generalize much, much better. We could keep up with the model interpretation and feature engineering, but it is beyond the scope of this lesson. Some other features that we can drop are time-stamp variables, such as MachineID and SalesID, as well as some model identification ones. This is because, with the model in production, when we want to infer the price of a bulldozer that is currently being sold, the time-stamp-related features do not provide any significant information to the random forest, provided that it is completely unable to generalize beyond what it has seen during training. For an in-depth explanation, check the lesson 7 of fastai’s 2020 course.\nWe will proceed now to do the prediction by training a neural network.\n\n\n3 Neural networks\nWhile random forests do great work, they are completely unable to extrapolate to regions beyond the limits of the training data. It may not be the end of the world for some tasks, but it is definitely terrible for some others.\nHowever, as we have seen, those models can be extremely helpful to understand the data and get an idea of the most important features, as they are very easily interpretable. Therefore, we will combine both approaches and take advantage of the feature analysis that we have performed with the random forest. This way, we will get rid of some of the meaningless features straight away before training the network.\n\n\n\n\n\n\nExpand to learn about the training details\n\n\n\n\n\nThe neural network will have to deal with continuous and categorical variables in a completely different way. We will create an embdedding for each categorical variable, while the numerical ones are just input into a fully connected layer. Then, everything is brought together in a dense classifier at the end. Therefore, it is importnat that we split the variables into numerical and categorical and, in fact, categorical variables with high cardinality, like saleElapsed, may be dealt with as numerical ones to prevent massive embeddings.\n\n\n\nLet’s train!\n\n\nCode\nx_ic = x_ic.drop(['SalesID', 'MachineID', 'fiModelDescriptor'], axis=1)\ndf_nn = df[list(x_ic.columns) + [dep_var]] # Keep only useful features\n\ncont_nn, cat_nn = cont_cat_split(df_nn, max_card=9000, dep_var=dep_var)\n\ncont_nn.append('saleElapsed')\ncat_nn.remove('saleElapsed')\ndf_nn.saleElapsed.dtype = int\n\nprocs_nn = [Categorify, FillMissing, Normalize]\nto_nn = TabularPandas(df_nn, procs_nn, cat_nn, cont_nn, splits=splits, y_names=dep_var)\ndls = to_nn.dataloaders(1024)\n\nlearn = tabular_learner(dls, y_range=(8,12), layers=[500,250],\n                        n_out=1, loss_func=F.mse_loss)\n\nlearn.fit_one_cycle(5, 1e-2)\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.070896\n      0.063198\n      00:07\n    \n    \n      1\n      0.056112\n      0.067255\n      00:07\n    \n    \n      2\n      0.049322\n      0.054010\n      00:07\n    \n    \n      3\n      0.043438\n      0.051197\n      00:07\n    \n    \n      4\n      0.040356\n      0.051439\n      00:07\n    \n  \n\n\n\nIn order to compare the random forest with the neural network we have to check what the RMSE is.\n\n\nCode\npreds,targs = learn.get_preds()\nr_mse(preds,targs)\n\n\n\n\n\n0.226801\n\n\nThe neural network provides a much better result than the random forest predicting the sales price of bulldozers. This is, mainly, due to the hard limitation in extrapolation of random forests, which make them struggle in forecasting tasks such as this one where prices evolve through time and we have to make inferences in the future.\nThis has been only one example of how to apply machine learning to tabular data. As you can see, these kind of problems offer a much more engaging relationship in the feature engineering part, provided that we feed the data straight into the classifier.\n\n\n\n\n\nReferences\n\nHoward, Jeremy, and Sylvain Gugger. 2020. “Fastai: A Layered API for Deep Learning.” Information 11 (2). https://doi.org/10.3390/info11020108."
  },
  {
    "objectID": "course/applications/applications-cv.html#multi-label-classification",
    "href": "course/applications/applications-cv.html#multi-label-classification",
    "title": "Typical tasks in computer vision",
    "section": "2.1 Multi-label classification",
    "text": "2.1 Multi-label classification\nWithin image classificaiton we, sometimes, encounter applications in which, rather than assigning a single label to each image, we need to provide a list of labels. This is known as multi-label classification and it is typically applied in situations in which we need to enumerate certain categories that appear in the image.\nI find it pretty intuitive to understand this kind of tasks with the analogy of a kid to whom we ask “what do you see in this image?” and the kid enumerates every single thing it can recognize in it: a tree, a dog, the sun, a lake, grass, a house, etc. Nonetheless, it will not be able to tell us things that it does know yet, such as the brand name of the car, or the name of a constellation in a night sky. In this case, the machine will be our kid and we will tell it exactly which things to identify in the images.\nTo provide an example, we use the PASCAL dataset. Let’s see how it looks like.\n\n\nCode\npath = untar_data(URLs.PASCAL_2007)\ndf = pd.read_csv(path/'train.csv')\ndf.head()\n\n\n\n\n\n\n  \n    \n      \n      fname\n      labels\n      is_valid\n    \n  \n  \n    \n      0\n      000005.jpg\n      chair\n      True\n    \n    \n      1\n      000007.jpg\n      car\n      True\n    \n    \n      2\n      000009.jpg\n      horse person\n      True\n    \n    \n      3\n      000012.jpg\n      car\n      False\n    \n    \n      4\n      000016.jpg\n      bicycle\n      True\n    \n  \n\n\n\n\nWe have a list of images with assigned labels and an indicator telling whether the image belongs to the validation set or not. See that the third image has a label ‘horse person’. It is not a centaur, it’s just two labels: horse and person. Let’s have a look at the image.\n\n\nCode\nf = df.iloc[2, 0]\nimg = Image.open(path/\"train\"/f)\nimg\n\n\n\n\n\nIndeed, there is a horse and three people. Notice that the task does not involve recognizing how many elements of a certain category are there. It is quite close to ticking a checkbox list of categories. In this dataset there are 20 categories:\n\n\nCode\nlabels = set()\nfor L in df.labels.unique(): labels.update(L.split(' '))\nprint(labels)\n\n\n{'chair', 'bicycle', 'tvmonitor', 'train', 'sheep', 'aeroplane', 'diningtable', 'person', 'motorbike', 'cow', 'bus', 'cat', 'pottedplant', 'sofa', 'horse', 'bottle', 'bird', 'car', 'boat', 'dog'}\n\n\nHence, among those, the associated categories to the image are ‘horse’ and ‘person’. Let us have a look at some more examples to get an idea of the kind of images that we encounter.\n\n\nCode\ndef get_x(r): return path/'train'/r['fname']\ndef get_y(r): return r['labels'].split(' ')\n\ndef splitter(df):\n    train = df.index[~df['is_valid']].tolist()\n    valid = df.index[df['is_valid']].tolist()\n    return train, valid\n\ndblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                   splitter=splitter,\n                   get_x=get_x, \n                   get_y=get_y,\n                   item_tfms = RandomResizedCrop(128, min_scale=0.35))\ndls = dblock.dataloaders(df)\n\ndls.show_batch()\n\n\n\n\n\nUnlike in the previous task, where all images had either dogs or pets as main body, in this case, we encounter a wide range of different images going from close portraits to general landscape views with many different objects in them. Nonetheless, we will do the same as in the previous example: take a pre-trained model and adapt it to this specific task.\n\n\n\n\n\n\nExpand to learn about the training procedure\n\n\n\n\n\nIn this case, we take a resnet50, which is larger than the previous resnet34. The architecture is also pre-trained in the Imagenet dataset and we fine tune it for this multi-label classification. The output layer now contains 20 neurons indicating whether each category appears in the sample.\n\n\n\nLet’s train!\n\n\nCode\nlearn = vision_learner(dls, resnet50, metrics=[accuracy_multi])\nlearn.fine_tune(3, base_lr=3e-3, freeze_epochs=4)\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy_multi\n      time\n    \n  \n  \n    \n      0\n      0.953263\n      0.685920\n      0.631952\n      00:03\n    \n    \n      1\n      0.826859\n      0.549115\n      0.749303\n      00:03\n    \n    \n      2\n      0.605951\n      0.196031\n      0.952749\n      00:04\n    \n    \n      3\n      0.358963\n      0.114387\n      0.961833\n      00:04\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy_multi\n      time\n    \n  \n  \n    \n      0\n      0.128015\n      0.102574\n      0.964502\n      00:04\n    \n    \n      1\n      0.111764\n      0.095951\n      0.967072\n      00:04\n    \n    \n      2\n      0.097319\n      0.094602\n      0.967709\n      00:04\n    \n  \n\n\n\nIn less than a minute of training, we are capable of providing all the categories appearing in the given images with an accuracy of ~96%! Let’s have a look at some examples in the validation set.\n\n\nCode\nlearn.show_results()\n\n\n\n\n\n\n\n\n\n\n\n\nThe model does kind of OK! Although it fails in some picture, it generally get’s it right, as we would have expected by the high accuracy! You can explore more predictions by running the cell again and seeing where the model failed."
  },
  {
    "objectID": "course/applications/applications-cv.html#image-segmentation",
    "href": "course/applications/applications-cv.html#image-segmentation",
    "title": "Typical tasks in computer vision",
    "section": "2.2 Image segmentation",
    "text": "2.2 Image segmentation\nIn the image classification tasks that we have tackled so far, we have related classes with whole images, e.g., telling which pet breed appears in an image, or whether it contains any of a bunch of categories like horse and a person. We can go a step further and assign a label to each pixel to identify certain parts of the image. This is known as image segmentation.\nThis technique has numerious applications across very distinct fields. For instance, in autonomous driving we have to tell which parts of the image are road, traffic signs, pedestrians, etc. On a completely different approach, in biomedical imaging, segmentation is used to tell appart healthy tissue from regions affected by certain diseases, such as identifying tumorous cells among healthy ones.\nHere we will show a segmentation example using a subset of the CamVid dataset for autonomous driving. Let us have a look at some examples to get a better understanding of the task at hand.\n\n\nCode\npath = untar_data(URLs.CAMVID_TINY)\ndls = SegmentationDataLoaders.from_label_func(\n    path, bs=8, fnames = get_image_files(path/\"images\"),\n    label_func = lambda o: path/'labels'/f'{o.stem}_P{o.suffix}',\n    codes = np.loadtxt(path/'codes.txt', dtype=str))\n\ndls.show_batch()\n\n\n\n\n\n\n\n    \n      \n      100.18% [2318336/2314212 00:01<00:00]\n    \n    \n\n\n\n\n\nEach pixel in the images is assigned a label indicating whether it is a tree, a traffic sign, a car, a bike, a building, a pedestrian, etc. Here, we see the images overlapped with the color-coded label mask to ease the visualization. The goal is, given an image, generate the color-coding mask, that is, another image. Again, we will leverage a pre-trained model to perform this task.\n\n\n\n\n\n\nExpand to learn about the training procedure\n\n\n\n\n\nJust as in the previous cases, we take the same pre-trained resnet34 and adapt it for our task. In this case, we not only limit ourselves to change the last layer of the network but, also, we also modify its architecture. We use the weights from the pre-trained network and convert it into a U-Net (Ronneberger, Fischer, and Brox 2015).\n\n\n\nLet’s train!\n\n\nCode\nlearn = unet_learner(dls, resnet34)\nlearn.fine_tune(13)\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      3.030844\n      2.215920\n      00:01\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.882831\n      1.702614\n      00:01\n    \n    \n      1\n      1.612940\n      1.320828\n      00:02\n    \n    \n      2\n      1.440053\n      1.191449\n      00:01\n    \n    \n      3\n      1.288506\n      1.073788\n      00:01\n    \n    \n      4\n      1.185435\n      0.932827\n      00:01\n    \n    \n      5\n      1.069924\n      0.805112\n      00:01\n    \n    \n      6\n      0.971954\n      0.763744\n      00:01\n    \n    \n      7\n      0.887134\n      0.744387\n      00:01\n    \n    \n      8\n      0.817554\n      0.754688\n      00:01\n    \n    \n      9\n      0.755276\n      0.707468\n      00:01\n    \n    \n      10\n      0.700168\n      0.690309\n      00:01\n    \n    \n      11\n      0.654143\n      0.690137\n      00:01\n    \n    \n      12\n      0.616357\n      0.685067\n      00:01\n    \n  \n\n\n\nGiven that the dataset is rather small, the training is ridiculously fast. Here, we do not have a metric, such as error rate, that allows us to get an idea of the overall performance. Therefore, we will ask the model to generate the classification mask for some images and see how it goes in a rather qualitative way.\n\n\nCode\nlearn.show_results(max_n=3, figsize=(10, 8))\n\n\n\n\n\n\n\n\n\n\n\n\nOn the left, we see the real color-coding and, on the right, the model prediction. We can see that all the buildings, trees, cars and traffic signs are consistently colored, so the model is doing a great work here. The most difficult part seems to be the identification of road lines as well as accurately defining the shapes. The model has room for improvement and it would certainly perform better with a larger training dataset.\nBeware, though, the model has entirely missed a cyclist in the first image, WATCH OUT!!"
  },
  {
    "objectID": "course/applications/applications-nlp.html",
    "href": "course/applications/applications-nlp.html",
    "title": "Typical tasks in natural language processing",
    "section": "",
    "text": "Note\n\n\n\nIn this notebook we make extensive use of transfer learning, a technique that we will explain in a couple of lessons.\nFurthermore, we use the fastai (Howard and Gugger 2020) library to download the data for the different tasks and easily train our models.\n\n\n\n1 Introduction\nNatural language processing (NLP) is the field of machine learning that handles the interaction with spoken or written language as us, humans, use it. Hence, it is most likely the field that makes more notable the advances in artificial intelligence, as it is our interface with modern machines in our daily lifes. It does not matter that our phone can run the most advanced algorithms if, when we ask for a simple thing, it does not understand us at all or it replies with 70s-robotic-voice. On the other hand, even if the internal processing is not the most advanced, having a smooth interaction makes us feel that whatever artificial intelligence lies within is much more advanced.\nNLP covers many aspects, featuring speech recognition (voice to text), semantic analysis, text generation and text to speech, among others. Here, we will mainly focus on text processing to illustrate a few representative tasks.\nWhile computer vision has been a long established field overcoming human performance, the advances in NLP are much more recent. Contributions in the last few years with algorithms such as ULMFiT, predecessor of the GPT series (GPT, GPT-2 and GPT-3) have brought the field of NLP a step forward. The main idea behind these works consist on, first, training a generative model that ‘understands’ the language involved in the task without any specific goal. Then, we can leverage its knowledge to tackle the specific task at hand. In some cases, we do not even need to train our model any further despite it not being specifically trained for the task!\nWe will dive deeper into generative modeling and the current state of the art in the last lecture of the course :D\n\n\n2 Language modeling\nLanguage models are generative algorithms used to create text given a context. Intuitively, these models learn to reproduce the content they read. They have countless wonderful applications, such as this Trump tweet generator, which writes tweets as if it was Donald Trump.\nWe will illustrate the process of language modeling writing movie reviews using the IMDB dataset. We generate text by asking the model to infer what the next word will be given a text fragment. This way, providing the model with a starting point, we add the predicted word to the text and, recursively, repeat the whole process to write a full text.\n\n\n\n\n\n\nExpand to learn about the training procedure\n\n\n\n\n\nJust like in the computer vision examples, we will leverage transfer learning. As starting point, we take a generative model that has been trained with the text data from wikipedia, which already comes with great knowledge of our world. Such a pre-trained model is highly versatile as, in our application case, movies can feature historical phenomena or specific events and characters that a model with the knowledge from wikipedia will already know. For instance, when the name of an actor appears in a review, the model will already know who it is and other things it has done or, even more, the model will infer the content of the movie out of certain featured names.\n\n\n\nLet’s have a look at the data!\n\n\nCode\npath = untar_data(URLs.IMDB)\nget_imdb = partial(get_text_files, folders=['train', 'test', 'unsup'])\n\ndls_lm = DataBlock(blocks=TextBlock.from_folder(path, is_lm=True),\n                   get_items=get_imdb, splitter=RandomSplitter(0.1)\n                   ).dataloaders(path, path=path, bs=128, seq_len=80)\n\ndls_lm.show_batch(max_n=2)\n\n\n\n\n\n\n\n    \n      \n      100.00% [144441344/144440600 00:35<00:00]\n    \n    \n\n\n\n\n\n\n\n\n\nxxbos ' holes ' was a xxup great movie . xxmaj disney made the right choice . xxmaj every person who i have talked to about it said they xxup loved it . xxmaj everyone casted was fit for the part they had , and xxmaj shia xxmaj labeouf really has a future with acting . xxmaj sigourney xxmaj weaver was perfect for xxmaj the xxmaj warden , she was exactly how i imagined her . everyone who has n't\nthat one can not imagine her as anyone else . xxmaj aamir xxmaj ali xxmaj malik is another actor who plays with the audience , seducing them and disturbing them through the course of the film xxmaj as an xxmaj indian separated from the partition by two generations i ca n't really say that i feel the pain that my mother does when she sees a film such as this , i have heard stories of my grand aunt who\n' holes ' was a xxup great movie . xxmaj disney made the right choice . xxmaj every person who i have talked to about it said they xxup loved it . xxmaj everyone casted was fit for the part they had , and xxmaj shia xxmaj labeouf really has a future with acting . xxmaj sigourney xxmaj weaver was perfect for xxmaj the xxmaj warden , she was exactly how i imagined her . everyone who has n't seen\none can not imagine her as anyone else . xxmaj aamir xxmaj ali xxmaj malik is another actor who plays with the audience , seducing them and disturbing them through the course of the film xxmaj as an xxmaj indian separated from the partition by two generations i ca n't really say that i feel the pain that my mother does when she sees a film such as this , i have heard stories of my grand aunt who was\n\n\nNotice that the text is tokenized including special symbols such as xxbos, indicating the beginning of sentence, xxmaj indicating that the next word starts with a capital letter, and so on. Truth is that the model does not understand words as they are, but rather it uses a representation of the words in a high dimensional mathematical space.\n\n\n\n\n\n\nExpand to learn about the training procedure\n\n\n\n\n\nLooking at the data, we see that the text is split into fragments that are shifted by one word between one another. This is because the target of the text on the left is its next word, that is, the text on the right. We use a recurrent neural network (RNN) to which the text is recursivelly passed word by word and, at every step, the target is the corresponding word on the right. Using the pre-traiend wikipedia RNN, we already have an embedding representing most of the English words and the network knows how to relate them. We just need to fine-tune the RNN in order to specialize it in the movie review context.\nNotice that training these models takes much longer than training the computer vision or tabular data ones. This is because of the nature of the RNN architecture, that loops over each word of the text and… well, Python does not like loops :)\n\n\n\nLet’s train!\n\n\nCode\nlearn = language_model_learner(dls_lm, AWD_LSTM, drop_mult=0.3, metrics=[accuracy, Perplexity()]).to_fp16()\nlearn.fine_tune(10, base_lr=2e-2)\n\n\n\n\n\n\n\n    \n      \n      100.00% [105070592/105067061 00:12<00:00]\n    \n    \n\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/1 00:00<?]\n    \n    \n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      perplexity\n      time\n    \n  \n  \n  \n\n\n    \n      \n      10.97% [289/2634 01:04<08:44 4.4093]\n    \n    \n\n\nKeyboardInterrupt: \n\n\nThe model correctly predicts the next word more than a third of the times. The number may not be outstanding compared with the accuracies that we have obtained in the computer vision tasks, but think about its meaning: the model correctly infers what the next word of any arbitrary review will be one out of every three guesses.\nLet’s now ask it to make some movie reviews for us. We provide it with the start of a sentence and see where it goes. Let’s start with ‘I liked this movie because’:\n\n\nCode\nn_words = 50\nstem = \"I liked this movie because\"\npred = learn.predict(stem, n_words, temperature=0.75)\nprint(pred)\n\n\nSee what happens with ‘Such a terrible movie should never’:\n\n\nCode\nstem = \"Such a terrible movie should never\"\npred = learn.predict(stem, n_words, temperature=0.75)\nprint(pred)\n\n\n\n\n\nSuch a terrible movie should never have been made . The director of this film , Paul Anderson , should be fired . So there are some OTHER reasons to waste any time on this flick . i have to disagree with the other comments . If you want to\n\n\nThese are some hilarious reviews! All of them make sense and are more or less coherent. For instance, when we provide the model with a positive beginning, ‘I liked this movie because’, the movie review tries to explain the reasons why it was so good. On the other hand, when we provide a negative starting point, ‘Such a terrible movie should never’, the review rips the movie beefing its director.\nNotice that, in these two reviews, the model has included names such as Paul Anderson (despite him being an actor and not a director), concepts like camcorder or thriller of the 80’s and it has even emphasized the word ‘OTHER’ with capital letters.\nThe dataset is made out of highly polarized movie reviews ando so the model has an easy time writing positive or negative reviews. Let’s see where it takes us whenever we provide it with an open start like ‘This movie about’:\n\n\nCode\nstem = \"This movie about \"\npred = learn.predict(stem, n_words, temperature=0.75)\nprint(pred)\n\n\n\n\n\nThis movie about a number of things that do n't happen to us , most of them are not exciting . The main problem with the movie is the fact that it is more about life than about the life of others . The main character , Bayliss , is\n\n\n\n\nCode\nstem = \"This movie about \"\npred = learn.predict(stem, n_words, temperature=0.75)\nprint(pred)\n\n\n\n\n\nThis movie about a small town , Iowa , is about a young man ( bill Paxton ) who is right about to go to school with a girl he met and he falls in love with her ( deborah Murray ) . She is also a photographer ,\n\n\nWith the same starting point, the first review is negative straight away, but the second remains neutral for the limited size that we have provided.\nNow that we have this powerful language model, we can use it knowledge to address other tasks involving movie reviews, so we will save the main body of the model for later.\n\n\nCode\nlearn.save_encoder(\"imdb_encoder\")\n\n\n\n\n3 Text classification\nOne of the most extended applications of NLP is text classification, which consists on assigning categories to pieces of text. This is highly related with “understanding” texts in artificial intelligence pipelines and data mining. For instance, we can use these classifiers to automatically sort scientific publications into their respective fields, e.g. condensed matter, neurology, …, and similar tasks. We can also use these models to find whether customer feedback is positive or negative at a large scale, separate fake news from real ones, or even tell the native language of the writer from a text in English.\nIn order to illustrate a case, we will continue with the same application example as before, leveraging the IMDB dataset, to assess whether movie reviews are positive or negative. Notice that, despite using the same dataset, the task is completely different. During the training of the langauge model, the target was the same bit of thext shifted by one word. Now, the target is a label indicating whether the review is positive or negative, as we see below.\n\n\nCode\npath = untar_data(URLs.IMDB)\n\ndls_clas = DataBlock(\n    blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab), CategoryBlock),\n    get_y = parent_label,\n    get_items=partial(get_text_files, folders=['train', 'test']),\n    splitter=GrandparentSplitter(valid_name='test')\n).dataloaders(path, path=path, bs=128, seq_len=72)\n\ndls_clas.show_batch(max_n=5)\n\n\n\n\n  \n    \n      \n      text\n      category\n    \n  \n  \n    \n      0\n      xxbos xxmaj match 1 : xxmaj tag xxmaj team xxmaj table xxmaj match xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley vs xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley started things off with a xxmaj tag xxmaj team xxmaj table xxmaj match against xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit . xxmaj according to the rules of the match , both opponents have to go through tables in order to get the win . xxmaj benoit and xxmaj guerrero heated up early on by taking turns hammering first xxmaj spike and then xxmaj bubba xxmaj ray . a xxmaj german xxunk by xxmaj benoit to xxmaj bubba took the wind out of the xxmaj dudley brother . xxmaj spike tried to help his brother , but the referee restrained him while xxmaj benoit and xxmaj guerrero\n      pos\n    \n    \n      1\n      xxbos xxmaj titanic directed by xxmaj james xxmaj cameron presents a fictional love story on the historical setting of the xxmaj titanic . xxmaj the plot is simple , xxunk , or not for those who love plots that twist and turn and keep you in suspense . xxmaj the end of the movie can be figured out within minutes of the start of the film , but the love story is an interesting one , however . xxmaj kate xxmaj winslett is wonderful as xxmaj rose , an aristocratic young lady betrothed by xxmaj cal ( billy xxmaj zane ) . xxmaj early on the voyage xxmaj rose meets xxmaj jack ( leonardo dicaprio ) , a lower class artist on his way to xxmaj america after winning his ticket aboard xxmaj titanic in a poker game . xxmaj if he wants something , he goes and gets it\n      pos\n    \n    \n      2\n      xxbos xxrep 3 * xxmaj warning - this review contains \" plot spoilers , \" though nothing could \" spoil \" this movie any more than it already is . xxmaj it really xxup is that bad . xxrep 3 * \\n\\n xxmaj before i begin , xxmaj i 'd like to let everyone know that this definitely is one of those so - incredibly - bad - that - you - fall - over - laughing movies . xxmaj if you 're in a lighthearted mood and need a very hearty laugh , this is the movie for you . xxmaj now without further ado , my review : \\n\\n xxmaj this movie was found in a bargain bin at wal - mart . xxmaj that should be the first clue as to how good of a movie it is . xxmaj secondly , it stars the lame action\n      neg\n    \n    \n      3\n      xxbos xxmaj jim xxmaj carrey is back to much the same role that he played in xxmaj the xxmaj mask , a timid guy who is trying to get ahead in the world but who seems to be plagued with bad luck . xxmaj even when he tries to help a homeless guy from being harassed by a bunch of hoodlums ( and of course they have to be xxmaj mexican , obviously ) , his good will towards his fellow man backfires . xxmaj in that case , it was n't too hard to predict that he was about to have a handful of angry hoodlums , but i like that the movie suggests that things like that should n't be ignored . xxmaj i 'm reminded of the episode of xxmaj michael xxmaj moore 's brilliant xxmaj the xxmaj awful xxmaj truth , when they had a man\n      pos\n    \n    \n      4\n      xxbos xxup the xxup shop xxup around xxup the xxup corner is one of the sweetest and most feel - good romantic comedies ever made . xxmaj there 's just no getting around that , and it 's hard to actually put one 's feeling for this film into words . xxmaj it 's not one of those films that tries too hard , nor does it come up with the oddest possible scenarios to get the two protagonists together in the end . xxmaj in fact , all its charm is innate , contained within the characters and the setting and the plot … which is highly believable to boot . xxmaj it 's easy to think that such a love story , as beautiful as any other ever told , * could * happen to you … a feeling you do n't often get from other romantic comedies\n      pos\n    \n  \n\n\n\nLet’s take the previous language model that we had trained to generate movie reviews, which knows quite a lot of the world thanks to wikipedia, and quite a bit more of movies, as starting point for our classifier.\n\n\n\n\n\n\nExpand to learn about the training procedure\n\n\n\n\n\nGiven the language model, we now add a fully connected layer at the back. The language model acts as feature extractor of the text, which feeds a dense classifier that outputs the probability of belonging to each class: positive or negative. Just as in computer vision, we start by freezing the pre-trained part of the model and then we proceed to unfreeze it once the training has advanced. In this case, however, we will gradually unfreeze the different layers of the model, from back to front, rather than unfreezing it all at once. Additionally, we will be using discriminative learning rates for the whole process.This process of taking a generic language model (wikipedia), fine-tunning it to our task (movie reviews) and, finally, using it as a classifier is the core of ULMFiT, as previously hinted.\n\n\n\nLet’s train!\n\n\nCode\nlearn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, \n                                metrics=accuracy).to_fp16()\nlearn = learn.load_encoder('imdb_encoder')\n\nlearn.fit_one_cycle(1, 2e-2)\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.356537\n      0.200132\n      0.920920\n      00:25\n    \n  \n\n\n\n\n\nCode\nlearn.freeze_to(-2)\nlearn.fit_one_cycle(1, slice(1e-2/(2.6**4), 1e-2))\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.277847\n      0.178657\n      0.931400\n      00:30\n    \n  \n\n\n\n\n\nCode\nlearn.freeze_to(-3)\nlearn.fit_one_cycle(1, slice(5e-3/(2.6**4), 5e-3))\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.211942\n      0.158949\n      0.941560\n      00:38\n    \n  \n\n\n\n\n\nCode\nlearn.unfreeze()\nlearn.fit_one_cycle(2, slice(1e-3/(2.6**4), 1e-3))\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.181801\n      0.154084\n      0.942280\n      00:46\n    \n    \n      1\n      0.159143\n      0.157385\n      0.944000\n      00:47\n    \n  \n\n\n\nWe reach an accuracy of ~94% in less than 4 minutes of training. This is mainly thanks to using our pre-trained language model as a feature extractor, which can extract rich information from the movie reviews to create a wonderful classifier.\nLet’s see some examples.\n\n\nCode\nlearn.show_results(max_n=4)\n\n\n\n\n\n\n\n  \n    \n      \n      text\n      category\n      category_\n    \n  \n  \n    \n      0\n      xxbos xxmaj there 's a sign on xxmaj the xxmaj lost xxmaj highway that says : \\n\\n * major xxup spoilers xxup ahead * \\n\\n ( but you already knew that , did n't you ? ) \\n\\n xxmaj since there 's a great deal of people that apparently did not get the point of this movie , xxmaj i 'd like to contribute my interpretation of why the plot makes perfect sense . xxmaj as others have pointed out , one single viewing of this movie is not sufficient . xxmaj if you have the xxup dvd of xxup md , you can \" cheat \" by looking at xxmaj david xxmaj lynch 's \" top 10 xxmaj hints to xxmaj unlocking xxup md \" ( but only upon second or third viewing , please . ) ;) \\n\\n xxmaj first of all , xxmaj mulholland xxmaj drive is\n      pos\n      pos\n    \n    \n      1\n      xxbos xxmaj back in the mid / late 80s , an xxup oav anime by title of \" bubblegum xxmaj crisis \" ( which i think is a military slang term for when technical equipment goes haywire ) made its debut on video , taking inspiration from \" blade xxmaj runner \" , \" the xxmaj terminator \" and maybe even \" robocop \" , with a little dash of xxmaj batman / xxmaj bruce xxmaj wayne - xxmaj iron xxmaj man / xxmaj tony xxmaj stark and xxmaj charlie 's xxmaj angel 's girl power thrown in for good measure . 8 episodes long , the overall story was that in 21st century xxmaj tokyo , xxmaj japan , year xxunk - xxunk , living machines called xxmaj boomers were doing manual labor and sometimes cause problems . a special , xxup swat like branch of law enforcers ,\n      pos\n      pos\n    \n    \n      2\n      xxbos xxmaj if anyone ever assembles a compendium on modern xxmaj american horror that is truly worth it 's salt , there will * have * to be an entry for xxup sf xxmaj brownrigg 's xxunk xxunk in xxmaj asylum xxmaj horror . xxmaj every time i watch this movie i am impressed by the complete economy of the film , from the compact , totally self - contained plot with a puzzling beginning and an all too horrible ending , the engaging performances by what was essentially a group of non - professional actors , and a xxunk sense of dread and claustrophobia that effectively consumes the xxunk with a certain inevitability which is all the more terrifying because the viewers know what is going on long before the xxunk ] , with the only question being when are they going to wake up & smell the coffee\n      pos\n      pos\n    \n    \n      3\n      xxbos xxmaj tony xxmaj hawk 's xxmaj pro xxmaj skater 2x , is n't much different at all from the previous games ( excluding xxmaj tony xxmaj hawk 3 ) . xxmaj the only thing new that is featured in xxmaj tony xxmaj hawk 's xxmaj pro xxmaj skater 2x , is the new selection of levels , and tweaked out graphics . xxmaj tony xxmaj hawk 's xxmaj pro xxmaj skater 2x offers a new career mode , and that is the 2x career . xxmaj the 2x career is basically xxmaj tony xxmaj hawk 1 career , because there is only about five challenges per level . xxmaj if you missed xxmaj tony xxmaj hawk 1 and 2 , i suggest that you buy xxmaj tony xxmaj hawk 's xxmaj pro xxmaj skater 2x , but if you have played the first two games , you should still\n      pos\n      pos\n    \n  \n\n\n\nWe can even get the classifier to tell whether our reviews are positive or negative.\n\nreview = \"This movie is fucking awful\"\nlearn.predict(review)\n\n\n\n\n('neg', TensorText(0), TensorText([0.9409, 0.0591]))\n\n\n\nreview = \"Such a wonderful movie!!!\"\nlearn.predict(review)\n\n\n\n\n('pos', TensorText(1), TensorText([9.0573e-04, 9.9909e-01]))\n\n\nFrom left to right, we see the category ‘neg’ or ‘pos’, its numerical equivalent (0 or 1) and the tensor telling the machine confidence for each class. In both cases, the model is pretty certain of the class it is predicting: 0.941 negative and 0.999 positive for each review, respectively. Let’s try with something that is less obvious.\n\nreview = (\"At first, this movie looked great and eganging. I was having a great time. Nonetheless,\"+\n          \" the last half turned out to be complete boring and worthless.\")\nlearn.predict(review)\n\n\n\n\n('neg', TensorText(0), TensorText([0.9985, 0.0015]))\n\n\nHere, we introduced a turning point in the review and the model got it pretty well. Let’s try with something a bit more neutral or ambiguous.\n\nreview = \"The movie is about a penguin's life. I guess it is somewhat informative, at least.\"\nlearn.predict(review)\n\n\n\n\n('pos', TensorText(1), TensorText([0.2826, 0.7174]))\n\n\nWe can see that in this last case, the model is much less confident about this being a positive review and the truth is that it can be interpreted in both ways. The movie can fall in the “not bad” category and we say that it is somewhat informative. However, it could also be that the movie is terrible to watch but that, at least, we get to learn something.\n\n\n4 Other natural language applications\nAs previously mentioned, NLP covers a massive range of applications. Directly related with what we’ve done so far, we could do some text regression tasks in which we aim to predict, for instance, a movie score out of the review. The process would be analogous to fine-tuning the text classifier.\nHowever, there are other completely different tasks that we can tackle. For example, we can translate a text to a different language, we can summarize it, we can extract a conceptual graph, etc.\n\n\n5 Sequence modeling\nSo far, we have seen computer vision applications exclusively involving images, and natural language processing applications involving only text. However, there are many tasks that involve various kinds of data at once. For example, we may be interested in writing captions that describe images. This is a language modeling task that takes an image as starting point. Conversely, we may be interested in generating images given a description, which is a computer vision generative task with a text as starting point.\nThis is performed by considering the data as a sequence of pieces of information all together. Just like the tokenization that we have performed for the text processing, we can come up with ways to tokenize our images. Combining both approaches, our models can then process the series of tokens regardless of their origin. There have recently been huge advances in this field. For example, flamingo is a chatbot that can process images, seamlesly integrating text and images in a conversation. Perhaps even more surprisingly, diffusion models such as DALLE-E can create high resolution images from arbitrary prompts.\nWe can download and play around with one of these models! For example, you can follow this tutorial to use the OpenAI API or this tutorial to use the huggingface stable diffusion models.\n\n\nCode\ndef generate_image(prompt):\n    \"Generates an image with the DALL-E2 API\"\n    response = openai.Image.create(\n        prompt=prompt, n=1, size=\"1024x1024\", response_format=\"b64_json\"\n    )\n    img = b64decode(response['data'][0]['b64_json'])\n    return Image.open(io.BytesIO(img))\n\n\nYou can use these references to draw inspiration for better prompts. Below, you will find some prompts provided by the students and the resulting images. Use with responsability!\n\ndescription = \"Cute and adorable walrus playing the banjo\"\ngenerate_image(description)\n\n\n\n\n\ndescription = \"a white siamese cat\"\ngenerate_image(description)\n\n\n\n\n\ndescription = \"Quantum master in barcelona in Gaudi trencadis\"\ngenerate_image(description)\n\n\n\n\n\ndescription = \"Shrek hyper realistic\"\ngenerate_image(description)\n\n\n\n\n\ndescription = \"gigachad homer hyper realistic\"\ngenerate_image(description)\n\n\n\n\n\ndescription = \"purple compass with the shape of a horse, neon light, perfect lighting\"\ngenerate_image(description)\n\n\n\n\n\ndescription = \"duck beaver fusion, creepy, horror, dark\"\ngenerate_image(description)\n\n\n\n\n\ndescription = \"asian Leonardo Dicaprio, steampunk\"\ngenerate_image(description)\n\n\n\n\n\ndescription = \"Pooh riding a comunist horse, for the people, military propaganda\"\ngenerate_image(description)\n\n\n\n\n\ndescription = \"Pooh riding a comunist horse, for the people, soviet poster\"\ngenerate_image(description)\n\n\n\n\n\n\n\n\n\nReferences\n\nHoward, Jeremy, and Sylvain Gugger. 2020. “Fastai: A Layered API for Deep Learning.” Information 11 (2). https://doi.org/10.3390/info11020108."
  },
  {
    "objectID": "course/applications/application-genqc.html",
    "href": "course/applications/application-genqc.html",
    "title": "Compiling Unitaries Using Diffusion Models",
    "section": "",
    "text": "Quantum computing relies on efficiently translating quantum operations into viable physical realizations on existing quantum hardware. Recently, diffusion models — a powerful class of generative models in machine learning — have demonstrated exceptional performance on this task.\nIn this tutorial notebook, we will demonstrate how to use this method to synthesize arbitrary unitaries into a cudaq.kernel, effectively decomposing them into sequences of quantum gates, a process commonly known as unitary compilation.\nOur focus will be on generating circuits using a pre-trained diffusion model. Specifically, we will illustrate how to compile different unitaries into discrete quantum gates, based on the methods presented in (Fürrutter et al., 2024), and continuous quantum gates, based on (Fürrutter et al., 2025)."
  },
  {
    "objectID": "course/applications/application-genqc.html#diffusion-model-pipeline",
    "href": "course/applications/application-genqc.html#diffusion-model-pipeline",
    "title": "Compiling Unitaries Using Diffusion Models",
    "section": "Diffusion model pipeline",
    "text": "Diffusion model pipeline\nGenerative diffusion models (DMs) have recently delivered remarkable results across a wide range of applications, from image generation to protein folding. In this work, we leverage DMs to generate quantum circuits based on a user specified unitary matrix and a text prompt that defines the allowed gate set, effectively using DMs as unitary compilers. The method is thoroughly explained in Ref. (Fürrutter et al., 2024). The following figure provides an overview of the proposed circuit generation pipeline:\n\n\n\nQuantum circuit generation pipeline. Figure adapted from (Fürrutter et al., 2024)\n\n\nThe pipeline consists of 3 main components:\n1) Circuit encoding: Like any neural network, diffusion models operate with continuous inputs and outputs. However, since the circuits we consider are composed of discrete gates (i.e., with no continuous parameters), we develop a mapping that transforms each gate into a continuous vector. This allows us to represent a given circuit as a three-dimensional tensor, as illustrated. Crucially, this mapping is invertible: when the DM generates continuous tensors, we can apply the inverse map to convert them back into the circuit form. An overview of these steps is provided in the figure below:\n\n\n\nQuantum circuit encoding. Figure adapted from (Fürrutter et al., 2024)\n\n\n2) Conditioning: The user’s input (the set of available gates and the unitary to compile) is also transformed into a continuous tensor by two neural networks. For the gate set description, where the input is a text prompt (e.g., “Compile using [‘x’, ‘h’]”), we utilize a pre-trained language model. For the unitary, we employ a neural network that is trained jointly with the diffusion model.\n3) Unitary compilation: The generation procedure follows the typical DM process: the model is given a fully noisy tensor which is iteratively de-noised until reaching a clean sample based on the given conditioning (the desired unitary and gate set). The tensors generated by the DM are then mapped to circuits via the inverse encoding procedure. To learn more about the practical implementation of diffusion models we recommend this tutorial.\nIn the following, we will use cudaq and genQC to perform all these steps and go from a desired unitary matrix \\(U\\) to a quantum circuit that we can execute using CUDA-Q.\n\nExtension to discrete-continuous quantum circuits: Recently, in Fürrutter et al. (2025), the authors introduced the next generation of compilation models, utilizing not only discrete gates but also continuous quantum gates in a multimodal diffusion model. We refer interested readers to the paper for more advanced details on the method. Moreover, we will also cover its use in the sections below."
  },
  {
    "objectID": "course/applications/application-genqc.html#setup-and-load-models",
    "href": "course/applications/application-genqc.html#setup-and-load-models",
    "title": "Compiling Unitaries Using Diffusion Models",
    "section": "Setup and load models",
    "text": "Setup and load models\nFirst, we make sure we have a compatible version of genQC (github.com/FlorianFuerrutter/genQC) installed.\n\n!pip install genQC==0.2.3 torch --break-system-packages -q\n\n\nimport cudaq\nimport torch\nimport numpy as np\nimport genQC\nimport os\n\nos.environ['HF_HUB_DISABLE_PROGRESS_BARS'] = '1'\n\nimport genQC.utils.misc_utils as util\nfrom genQC.pipeline.diffusion_pipeline import DiffusionPipeline\nfrom genQC.pipeline.multimodal_diffusion_pipeline \\\n            import MultimodalDiffusionPipeline_ParametrizedCompilation\n\nfrom genQC.platform.tokenizer.circuits_tokenizer import CircuitTokenizer\nfrom genQC.platform.simulation import Simulator, CircuitBackendType\nfrom genQC.scheduler.scheduler_dpm import DPMScheduler\n\nfrom genQC.inference.sampling \\\n            import decode_tensors_to_backend, generate_compilation_tensors\nfrom genQC.inference.evaluation_helper import get_unitaries\nfrom genQC.inference.eval_metrics import UnitaryInfidelityNorm\n\n\ndevice = util.infer_torch_device() # Use CUDA if we have a GPU\nprint(device)\n\n[INFO]: Cuda device has a capability of 8.6 (>= 8), allowing tf32 matmul.\ncuda\n\n\nWe set a fixed seed for reproducibility.\n\n# We set a seed to pytorch, numpy and python. \n# Note: This will also set deterministic cuda algorithms, possibly at the cost of reduced performance!\nutil.set_seed(0)\n\nFor evaluation, we also need to specify the cudaq circuit simulator backend.\n\nsimulator = Simulator(CircuitBackendType.CUDAQ, \n                      target='qpp-cpu')  # Target for cudaq, note that cpu is faster for low qubit kernels\n\n\nLoad discrete model\nFirst, we load pre-trained model weights for a discrete model directly from Hugging Face: Floki00/qc_unitary_3qubit and setup the DM pipeline. For details of the model we refer to the paper Fürrutter et al., 2024. Note that this model is only trained on 3 qubit unitaries up to 12 gates.\n\ndiscrete_pipeline = DiffusionPipeline.from_pretrained(\n            repo_id=\"Floki00/qc_unitary_3qubit\", # Download model from Hugging Face\n            device=device)\n\n[INFO]: `genQC.models.unet_qc.QC_Compilation_UNet` instantiated from given `config` on cuda.\n[INFO]: Loading model from `/home/gorka/.cache/huggingface/hub/models--Floki00--qc_unitary_3qubit/snapshots/e12810ea65a9a88832fadd34834894e8174658a5/model.pt` onto device: cuda.\n[INFO]: `genQC.models.unet_qc.QC_Compilation_UNet`. Freeze model: True\n[INFO]: `genQC.models.frozen_open_clip.CachedFrozenOpenCLIPEmbedder` instantiated from given `config` on cuda.\n[INFO]: `genQC.models.frozen_open_clip.CachedFrozenOpenCLIPEmbedder`. Found no key `save_type` in `config`. No state dict loaded.\n[INFO]: `genQC.models.frozen_open_clip.CachedFrozenOpenCLIPEmbedder`. Freeze model: True\n\n\nThe loaded discrete model is trained with the discrete gate set:\n\ndiscrete_pipeline.gate_pool\n\n['h', 'cx', 'z', 'x', 'ccx', 'swap']\n\n\nwhich we need in order to define the discrete_vocabulary and create a CircuitTokenizer, allowing us to decode tokenized circuits generated by the model.\n\ndiscrete_vocabulary = {g:i+1 for i, g in enumerate(discrete_pipeline.gate_pool)} \ndiscrete_tokenizer  = CircuitTokenizer(discrete_vocabulary)\ndiscrete_tokenizer.vocabulary\n\n{'h': 1, 'cx': 2, 'z': 3, 'x': 4, 'ccx': 5, 'swap': 6}\n\n\nSet parameters the model was trained on. Note that these are fixed and depend on the pre-trained model.\n\n# These parameters are specific to our pre-trained model.\ndiscrete_system_size   = 3\ndiscrete_max_gates     = 12\n\nSet inference parameters\nThen, we setup diffusion model inference parameters. These can be changed in principle, as they are sample hyperparameters.\n\ntimesteps = 40\ndiscrete_pipeline.scheduler.set_timesteps(timesteps)\n\n\n\nCreate helper functions\nTo facilitate the sampling, evaluating and plotting multiple circuits for different unitaries, we create some helper functions here.\n\nA function to check that a matrix U is indeed unitary, i.e. \\(U^{\\dagger} U = U U^{\\dagger} = I\\).\n\n\ndef verify_unitary(U: torch.Tensor):\n    \"\"\"Check if unitary.\"\"\"\n    assert torch.allclose(U.adjoint() @ U, torch.eye(2**num_of_qubits, dtype=U.dtype))  \n    assert torch.allclose(U @ U.adjoint(), torch.eye(2**num_of_qubits, dtype=U.dtype))\n\n\nA function to sample the DM and return generated kernels with coresponding infidelities.\n\n\ndef sample_kernels_and_evaluate(U: torch.Tensor, \n                                prompt: str, \n                                num_of_qubits: int, \n                                samples: int,\n                                return_tensors: bool = False):\n    \"\"\"\n    Sample the DM and return generated kernels with coresponding infidelities.\n    \"\"\"\n\n    # 1) Check if unitary\n    verify_unitary(U)\n\n    # 2) Generate tensor representations using the DM based on the prompt and U.\n    U = U.to(torch.complex64)\n    \n    # Sample discrete model\n    out_tensor = generate_compilation_tensors(discrete_pipeline, \n                              prompt=prompt, \n                              U=U, \n                              samples=samples,      # How many circuits we sample per unitary\n                              system_size=discrete_system_size, \n                              num_of_qubits=num_of_qubits, \n                              max_gates=discrete_max_gates,\n                              g=10.0,               # classifier-free-guidance (CFG) scale\n                              no_bar=False,         # show progress bar\n                              auto_batch_size=256,  # for less GPU memory usage limit batch size \n                              tensor_prod_pad=False, \n                              enable_params=False,\n                             )\n    tokenizer = discrete_tokenizer\n    params    = None\n        \n\n    \n    # 3) Convert tensors to kernels \n    generated_kernels, _, generated_tensors = decode_tensors_to_backend(simulator=simulator, \n                                                     tokenizer=tokenizer, \n                                                     tensors=out_tensor, \n                                                     params=params,\n                                                     return_tensors=True)\n\n    # 4) Evaluate the kernels and return the unitaries\n    generated_us = get_unitaries(simulator, generated_kernels, num_qubits=num_of_qubits)\n\n    # 5) Calculate the infidelities to the target U\n    infidelities = UnitaryInfidelityNorm.distance(\n                    approx_U=torch.from_numpy(np.stack(generated_us)).to(torch.complex128), \n                    target_U=U.unsqueeze(0).to(torch.complex128))\n\n    if return_tensors:\n        return generated_kernels, infidelities, generated_tensors\n    return generated_kernels, infidelities\n\n\nA function to plot the topk best generated kernels.\n\n\ndef plot_topk_kernels(generated_kernels: list, \n                      infidelities: torch.Tensor, \n                      num_of_qubits:int, \n                      topk: int):\n    \"\"\"\n    Plot the topk best generated kernels.\n    \"\"\"\n\n    # Get topk indices\n    best_indices = np.argsort(infidelities)[:topk]\n\n    input_state = [0] * (2**num_of_qubits)\n    input_state[0] = 1\n\n    # Print the circuits\n    for i, best_index in enumerate(best_indices):\n        kernel = generated_kernels[best_index].kernel\n        thetas = generated_kernels[best_index].params\n        \n        print(f\"Circuit has an infidelity of {infidelities[best_index].item():0.1e}.\")\n        print(cudaq.draw(kernel, input_state, thetas))"
  },
  {
    "objectID": "course/applications/application-genqc.html#unitary-compilation",
    "href": "course/applications/application-genqc.html#unitary-compilation",
    "title": "Compiling Unitaries Using Diffusion Models",
    "section": "Unitary compilation",
    "text": "Unitary compilation\nWe start by defining the unitaries we want to compile and then sample the corresponding DM. Note that these models have been trained to compile unitaries that arise from circuits composed of the gates contained in their vocabulary. While these are universal gate sets, i.e. they can perform universal computation, they can only do so with an arbitrary-precision for an infinite number of gates. Because the number of gates in the models are restricted to some max_gates (12 for the discrete model and 32 for the continuous one), we can only expect the models to generate unitaries under this constraint. We will consider here the compilation of such unitaries. Nonetheless, stay tuned for bigger and better models!\n\nRandom unitary\nLet’s start with a random 3-qubit unitary.\n\nnum_of_qubits = 3\n\nWe can define our arbitrary unitary U directly as a complex torch.tensor:\n\nU = torch.tensor([[0.70710678, 0., 0., 0., 0.70710678, 0., 0., 0.],\n               [0., -0.70710678, 0., 0., 0., -0.70710678, 0., 0.],\n               [-0.70710678, 0., 0., 0., 0.70710678, 0., 0., 0.],\n               [0., 0.70710678, 0., 0., 0., -0.70710678, 0., 0.],\n               [0., 0., 0.70710678, 0., 0., 0., 0., 0.70710678],\n               [0., 0., 0., 0.70710678, 0., 0., 0.70710678, 0.],\n               [0., 0., -0.70710678, 0., 0., 0., 0., 0.70710678],\n               [0., 0., 0., -0.70710678, 0., 0., 0.70710678, 0.]],              \n              dtype=torch.complex128)\n\n\n\nDiscrete model\nThe loaded discrete model was trained on the gate set ['h', 'cx', 'z', 'x', 'ccx', 'swap']. Specifically, it was trained to generate circuits using any arbitrary subset of this gate set. Therefore, during inference, we can instruct the model to compile the unitary using any of these subsets. However, it is crucial to follow the prompt structure Compile using [...], as the model was trained with this specific format.\nFor example, let’s consider a scenario where we compile the unitary without using the x gate:\n\n# Notice how the x gate is missing from the prompt since this is a restriction we set\nprompt = \"Compile using: ['h', 'cx', 'z', 'ccx', 'swap']\"\n\nNow, we call the diffusion model pipeline to generate encoded circuits based on the specified conditions: prompt and U.\n\ngenerated_kernels, infidelities = sample_kernels_and_evaluate(\n                                          U=U, \n                                          prompt=prompt, \n                                          num_of_qubits=num_of_qubits, \n                                          samples=128)\n\n\n\n\n[INFO]: (generate_comp_tensors) Generated 128 tensors\n\n\nNext, we plot the best three circuits in the following cell.\n\nplot_topk_kernels(generated_kernels, infidelities, num_of_qubits, topk=3)\n\nCircuit has an infidelity of 3.4e-08.\n     ╭───╮╭───╮        ╭───╮╭───╮     \nq0 : ┤ x ├┤ z ├─────●──┤ z ├┤ z ├──●──\n     ╰─┬─╯╰───╯   ╭─┴─╮├───┤╰───╯╭─┴─╮\nq1 : ──●────────╳─┤ x ├┤ z ├─────┤ x ├\n       │  ╭───╮ │ ╰─┬─╯╰───╯     ╰─┬─╯\nq2 : ──●──┤ h ├─╳───●──────────────●──\n          ╰───╯                       \n\nCircuit has an infidelity of 3.4e-08.\n     ╭───╮╭───╮                  \nq0 : ┤ x ├┤ z ├─────●─────────●──\n     ╰─┬─╯╰───╯   ╭─┴─╮╭───╮╭─┴─╮\nq1 : ──●────────╳─┤ x ├┤ z ├┤ x ├\n       │  ╭───╮ │ ╰─┬─╯╰───╯╰─┬─╯\nq2 : ──●──┤ h ├─╳───●─────────●──\n          ╰───╯                  \n\nCircuit has an infidelity of 3.4e-08.\n     ╭───╮╭───╮                  \nq0 : ┤ x ├┤ z ├─────●─────────●──\n     ╰─┬─╯╰───╯   ╭─┴─╮╭───╮╭─┴─╮\nq1 : ──●────────╳─┤ x ├┤ z ├┤ x ├\n       │  ╭───╮ │ ╰─┬─╯╰───╯╰─┬─╯\nq2 : ──●──┤ h ├─╳───●─────────●──\n          ╰───╯"
  },
  {
    "objectID": "course/applications/application-genqc.html#choosing-the-circuit-you-need",
    "href": "course/applications/application-genqc.html#choosing-the-circuit-you-need",
    "title": "Compiling Unitaries Using Diffusion Models",
    "section": "Choosing the circuit you need",
    "text": "Choosing the circuit you need\nAs mentioned earlier, one of the key advantages of using diffusion models (DMs) as a unitary compiler is the ability to rapidly sample many circuits. However, as is common in machine learning, the model has a certain accuracy, meaning not all generated circuits are expected to exactly compile the specified unitary. In this section, we will evaluate how many of the generated circuits are indeed correct and then perform post-selection to identify circuits that successfully perform the desired unitary operation.\nLet’s revisit the random unitary from before, but this time we closer inspect what the model generates.\n\nnum_of_qubits = 3\nprompt = f\"Compile {num_of_qubits} qubits using: ['h', 'cx', 'z', 'x', 'ccx', 'swap']\"\n\nU = torch.tensor([[0.70710678, 0., 0., 0., 0.70710678, 0., 0., 0.],\n               [0., -0.70710678, 0., 0., 0., -0.70710678, 0., 0.],\n               [-0.70710678, 0., 0., 0., 0.70710678, 0., 0., 0.],\n               [0., 0.70710678, 0., 0., 0., -0.70710678, 0., 0.],\n               [0., 0., 0.70710678, 0., 0., 0., 0., 0.70710678],\n               [0., 0., 0., 0.70710678, 0., 0., 0.70710678, 0.],\n               [0., 0., -0.70710678, 0., 0., 0., 0., 0.70710678],\n               [0., 0., 0., -0.70710678, 0., 0., 0.70710678, 0.]],              \n              dtype=torch.complex128)\n\n\ngenerated_kernels, infidelities, generated_tensors = \\\n                sample_kernels_and_evaluate(\n                              U=U, \n                              prompt=prompt, \n                              num_of_qubits=num_of_qubits, \n                              samples=128,\n                              return_tensors=True)\n\n\n\n\n[INFO]: (generate_comp_tensors) Generated 128 tensors\n\n\nFirst, we plot a histogram of the infidelities of our generated circuits.\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(7, 4))\nplt.title(\n    f\"Distribution of infidelities for {len(infidelities)} generated circuits\",\n    fontsize=12)\nplt.ylabel(\"Number of circuits\", fontsize=14)\nplt.xlabel(\"Unitary infidelity\", fontsize=14)\nplt.hist(infidelities, bins=30)\nplt.show()\n\n\n\n\nAs we see above, we now have around 15 kernels that compile the desired unitary! This is particularly valuable when dealing with hardware constraints, where, for instance, we might want to avoid using certain qubits or specific gates. One practical example is finding the circuit with the fewest CNOT gates (also known as cx). In our discrete_vocabulary definition above, we identified that cx corresponds to the label 2 in our tokenized tensors.\n\ncx_token = discrete_vocabulary[\"ccx\"]\ncx_token\n\n5\n\n\nLet’s use this information to search for the circuit that minimizes the number of cx gates:\n\n# First, we remove possible duplicates and only pick distinct circuits\n_, idx_unique = torch.unique(generated_tensors, dim=0, return_inverse=True)\nunique_tensors = generated_tensors[idx_unique]\nunique_infidelities = infidelities[idx_unique]\nunique_kernels = [generated_kernels[idx] for idx in idx_unique]\n\n# Then, find the correct circuits\nidx_correct = torch.argwhere(unique_infidelities < 0.01).flatten()\ncorrect_tensors = unique_tensors[idx_correct]\nprint(\n    f\"The model generated {correct_tensors.shape[0]} distinct circuits with infidelity < 0.01.\"\n)\n\n# Now let's flatten the last two dimensions (related to the actual circuit) and find out how many 2's (i.e. cx) gates each circuit has:\nnum_cx = (correct_tensors.flatten(1, 2) == cx_token).sum(1)\nprint(\"These circuits have this number of cx gates:\", num_cx.tolist())\n\nThe model generated 31 distinct circuits with infidelity < 0.01.\nThese circuits have this number of cx gates: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n\n\nAs we can see, the diffusion model (DM) uses 0 to 3 CNOT gates to compile the unitary. We can now print a few of these circuits to select the one that best suits our needs, or to study whether there are any interesting patterns the model employs for this specific unitary.\nFor instance, we can sort the circuits by their cx count and plot them.\n\n# Get the correct kernels\ncorrect_kernels = [unique_kernels[idx] for idx in idx_correct]\n\n# Order the gates by cx count\ncorrect_kernels_cx_sorted = [\n    (correct_kernels[idx], num_cx[idx]) for idx in torch.argsort(num_cx)\n]\n\n# Draw a few of these circuits\ninput_state = [0] * (2**num_of_qubits)\ninput_state[0] = 1\n\nfor correct_kernel, num_cx_gates in correct_kernels_cx_sorted[:3]:\n    kernel = correct_kernel.kernel\n    thetas = correct_kernel.params\n\n    print(f\"Generated circuit with {num_cx_gates} cx:\")\n    print(cudaq.draw(kernel, input_state, thetas))\n\nGenerated circuit with 1 cx:\n     ╭───╮╭───╮╭───╮           ╭───╮        \nq0 : ┤ x ├┤ h ├┤ x ├─╳───────╳─┤ h ├────────\n     ╰─┬─╯╰───╯╰─┬─╯ │ ╭───╮ │ ├───┤        \nq1 : ──●─────────●───╳─┤ x ├─╳─┤ z ├─╳──────\n       │  ╭───╮╭───╮   ╰───╯   ╰───╯ │ ╭───╮\nq2 : ──●──┤ h ├┤ z ├─────────────────╳─┤ z ├\n          ╰───╯╰───╯                   ╰───╯\n\nGenerated circuit with 1 cx:\n     ╭───╮╭───╮     ╭───╮              \nq0 : ┤ x ├┤ z ├──╳──┤ x ├─╳──╳──╳───╳──\n     ╰─┬─╯├───┤  │  ╰─┬─╯ │  │  │   │  \nq1 : ──●──┤ h ├──╳────●───┼──┼──┼───╳──\n       │  ├───┤╭───╮      │  │  │ ╭───╮\nq2 : ──●──┤ h ├┤ z ├──────╳──╳──╳─┤ h ├\n          ╰───╯╰───╯              ╰───╯\n\nGenerated circuit with 1 cx:\n     ╭───╮╭───╮                           \nq0 : ┤ x ├┤ z ├─────────╳───────╳──╳───╳──\n     ╰─┬─╯├───┤         │       │  │   │  \nq1 : ──●──┤ h ├──────╳──╳───●───┼──┼───╳──\n       │  ├───┤╭───╮ │    ╭─┴─╮ │  │ ╭───╮\nq2 : ──●──┤ x ├┤ h ├─╳────┤ x ├─╳──╳─┤ h ├\n          ╰───╯╰───╯      ╰───╯      ╰───╯\n\n\n\n\nprint(cudaq.__version__)\nprint(\"genQC Version\", genQC.__version__)\n\nCUDA-Q Version 0.11.0 (https://github.com/NVIDIA/cuda-quantum f5cc3bb1d85abcf1f642f4ddd20ad08bc1d4d200)\ngenQC Version 0.2.3"
  },
  {
    "objectID": "course/index.html",
    "href": "course/index.html",
    "title": "Structure of the lectures",
    "section": "",
    "text": "flowchart TD\nsubgraph one[ ]\n A(linear models)--> A1(Linear regression)\n A --> A2(Polynomial Regression)\n A --> A3(Logistic Regression)\n A --> A4(Perceptron)\nend\nstyle one fill:#82c4c3,stroke-width:0px;\nclassDef boxes stroke-width:0px;\nclass A,A1,A2,A3,A4 boxes;\n\n\n\n\n\n\n\n\n\n\n\n\nflowchart TD\nsubgraph three[ ]\n C(A Probabilistic View on Machine Learning)--> C1(Review Probability)\n C-->C2(Likelihood)\n C-->C3(Kullback Leibler Divergence)\n C-->C4(Linear Regression) \nend\nstyle three fill:#f6d887,stroke-width:0px;\nclassDef boxes stroke-width:0px;\nclass C,C1,C2,C3,C4 boxes;\n\n\n\n\n\n\n\n\n\n\n\n\nflowchart TD\nsubgraph two[ ]\n B(Neural Networks)--> B1(Perceptron)\n B-->B2(Deep Neural Networks)\n B-->B3(Automatic Differentiation)\nend\nstyle two fill:#82c4c3,stroke-width:0px;\nclassDef boxes stroke-width:0px;\nclass B,B1,B2,B3,B4 boxes;"
  },
  {
    "objectID": "course/deep_learning/neural_networks_from_scratch.html",
    "href": "course/deep_learning/neural_networks_from_scratch.html",
    "title": "NN from scratch",
    "section": "",
    "text": "In the next few lessons we will dive into the world of neural networks and deep learning. We will first start with the basics, understanding how a very simple neural network works and the different algorithms that exist for training them."
  },
  {
    "objectID": "course/deep_learning/neural_networks_from_scratch.html#sigmoid-perceptron-update",
    "href": "course/deep_learning/neural_networks_from_scratch.html#sigmoid-perceptron-update",
    "title": "NN from scratch",
    "section": "1.1 Sigmoid perceptron update",
    "text": "1.1 Sigmoid perceptron update\nThe training of a perceptron consists on the iterative update of its parameters, \\(W\\) and \\(\\mathbf{b}\\), in order to minimize the loss function \\(L\\). Here, we will consider the mean-squared error:\n\\[\\begin{equation}\n\\begin{split}\nL & = \\frac{1}{N}\\sum_{i=1}^n L_i\\\\\nL_i & = \\frac{1}{2}(y_i - \\hat{y}_i)^2,\n\\end{split}\n\\end{equation}\\]\nwhere \\(\\hat{y}_i\\) is our prediction and \\(y_i\\) the ground truth.\nWe update the weights \\(W\\) and biases \\(\\mathbf{b}\\) with a gradient descent procedure:\n\\[\\begin{equation}\n\\begin{split}\nW & \\leftarrow W - \\eta \\frac{\\partial L}{\\partial W} = W - \\frac{\\eta}{n} \\sum_{i=1}^N \\frac{\\partial L_i}{\\partial W}\\\\\n\\mathbf{b} & \\leftarrow \\mathbf{b} - \\eta \\frac{\\partial L}{\\partial \\mathbf{b}} = W - \\frac{\\eta}{n} \\sum_{i=1}^n \\frac{\\partial L_i}{\\partial \\mathbf{b}},\n\\end{split}\n\\end{equation}\\]\nwhere \\(\\eta\\) is the learning rate.\nIn this case, we can obtain analytical expressions for the gradients:\n\\[\\begin{equation}\n\\begin{split}\n\\frac{\\partial L_i}{\\partial W} & = (y_i - \\hat{y}_i)\\frac{\\partial y_i}{\\partial W}\\\\\n\\frac{\\partial L_i}{\\partial \\mathbf{b}} & = (y_i - \\hat{y}_i)\\frac{\\partial y_i}{\\partial \\mathbf{b}}\\\\\ny_i & = \\sigma(z_i) \\\\\nz_i & = x_i^T W + \\mathbf{b}\n\\end{split}\n\\end{equation}\\]\nWith the chain rule we have:\n\\[\\begin{equation}\n\\begin{split}\n\\frac{\\partial y_i}{\\partial W} & = \\frac{\\partial \\sigma(z_i)}{\\partial z_i}\\frac{\\partial z_i}{\\partial W} \\\\\n\\frac{\\partial y_i}{\\partial \\mathbf{b}} & = \\frac{\\partial \\sigma(z_i)}{\\partial z_i}\\frac{\\partial z_i}{\\partial \\mathbf{b}}\n\\end{split}\n\\end{equation}\\]\n\n\n\n\n\n\nExercise\n\n\n\nShow that update rule for weights \\(W\\) and bias \\(\\mathbf{b}\\) is:\n\\[\\begin{equation}\n\\begin{split}\nW & \\leftarrow W - \\frac{\\eta }{n}\\sum_{i=1}^n (y_i - \\hat{y}_i) y_i(1-y_i)x_i\\\\\n\\mathbf{b} & \\leftarrow \\mathbf{b} - \\frac{\\eta }{n}\\sum_{i=1}^n (y_i - \\hat{y}_i) y_i(1-y_i)\n\\end{split}\n\\end{equation}\\]"
  },
  {
    "objectID": "course/deep_learning/neural_networks_from_scratch.html#network-definition",
    "href": "course/deep_learning/neural_networks_from_scratch.html#network-definition",
    "title": "NN from scratch",
    "section": "2.1 Network definition",
    "text": "2.1 Network definition\nIn a neural network with a single hidden layer, we have two perceptrons: one between the input and the hidden layer, and one between the hidden layer and the output.\nThe input layer has the same size as the number of features in our data, i.e., \\(m\\) neurons. Then, the hidden layer has \\(h\\) neurons, and the output layer has as many neurons as classes \\(c\\). In a regression task, \\(c=1\\) as we predict a single scalar. Thus, the first weigth matrix \\(W_1\\) has shape \\(m\\times h\\) and , and the second weight matrix \\(W_2\\) has shape \\(h\\times c\\). In this case, we only consider biases in the hidden layer \\(\\mathbf{b}_1\\) which is a vector with \\(h\\) entries.\n\n2.1.1 Feed-forward pass\nNow, let’s go through the feed-forward pass of the training data \\(X\\in\\mathbb{R}^{n\\times m}\\) through our network.\n\nThe input goes through the first linear layer \\(\\mathbf{h} \\leftarrow X^TW_1 + \\mathbf{b}_1\\) with shapes \\([n, m] \\times [m, h] = [n, h]\\)\nThen, we apply the activation function \\(\\hat{\\mathbf{h}} \\leftarrow \\sigma(\\mathbf{h})\\) with shape \\([n, h]\\)\nThen, we apply the second linear layer \\(\\mathbf{g} \\leftarrow \\mathbf{h}^TW_{2}\\) with shapes \\([n, h] \\times [h, c] = [n, c]\\)\nFinally, we apply the activation function \\(\\hat{\\mathbf{y}} \\leftarrow \\sigma(\\mathbf{g})\\) with shape \\([n, c]\\)\n\n\n\n2.1.2 Parameter update\nWe will use the MSE loss function denoted in matrix rerpesentation as \\(L = \\frac{1}{2n}||Y - \\hat{Y}||^2\\),\nThe parameter update rule is \\[\\begin{equation}\n\\begin{split}\n  W_1 & = W_1 - \\frac{\\eta}{n}\\frac{\\partial L}{\\partial W_1} \\\\\n  \\mathbf{b}_1 & = \\mathbf{b}_1 - \\frac{\\eta}{n}\\frac{\\partial L}{\\partial \\mathbf{b}_1} \\\\\n  W_2 & = W_2 - \\frac{\\eta}{n}\\frac{\\partial L}{\\partial W_2}.\n\\end{split}\n\\end{equation}\\]\nLet us calculate gradients of the loss function with respect to \\(W_1\\), \\(\\mathbf{b}_1\\) and \\(W_2\\) using the chain rule:\n\\[\\begin{equation}\n\\begin{split}\n\\frac{\\partial L}{\\partial W_{2}} & = \\frac{\\partial L}{\\partial \\hat{Y}}\\frac{\\partial \\hat{Y}}{\\partial \\mathbf{g}}\\frac{\\partial \\mathbf{g}}{\\partial W_{2}} \\\\\n\\frac{\\partial L}{\\partial W_{1}} & = \\frac{\\partial L}{\\partial \\hat{Y}}\\frac{\\partial \\hat{Y}}{\\partial \\mathbf{g}}\\frac{\\partial \\mathbf{g}}{\\partial \\hat{\\mathbf{h}}}\\frac{\\partial \\hat{\\mathbf{h}}}{\\partial \\mathbf{h}}\\frac{\\partial \\mathbf{h}}{\\partial W_{1}}\\\\\n\\frac{\\partial L}{\\partial \\mathbf{b}_{1}} & = \\frac{\\partial L}{\\partial \\hat{Y}}\\frac{\\partial \\hat{Y}}{\\partial \\mathbf{g}}\\frac{\\partial \\mathbf{g}}{\\partial \\hat{\\mathbf{h}}}\\frac{\\partial \\hat{\\mathbf{h}}}{\\partial \\mathbf{h}}\\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{b}_{1}}\n\\end{split}\n\\end{equation}\\]\nWe can write down every term:\n\\[\\begin{equation}\n\\begin{split}\n\\frac{\\partial L}{\\partial \\hat{Y}} & = \\hat{Y} - Y \\\\\n\\frac{\\partial \\hat{Y}}{\\partial \\mathbf{g}} & = \\hat{Y}(1-\\hat{Y}) \\\\\n\\frac{\\partial \\mathbf{g}}{\\partial W_{2}} & = \\hat{\\mathbf{h}} \\\\\n\\frac{\\partial \\mathbf{g}}{\\partial \\hat{\\mathbf{h}}} & = W_{2} \\\\\n\\frac{\\partial \\hat{\\mathbf{h}}}{\\partial \\mathbf{h}} & = \\hat{\\mathbf{h}}(1-\\hat{\\mathbf{h}}) \\\\\n\\frac{\\partial \\mathbf{h}}{\\partial W_1} & = X\\\\\n\\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{b}_1} & = \\mathbb{1}\n\\end{split}\n\\end{equation}\\]\n\n\n\n\n\n\nExercise\n\n\n\nShow explicitly that \\(Q_1\\), and \\(Q_2\\) read: \\[\\begin{equation}\n\\begin{split}\nQ_2 & \\equiv \\frac{\\partial L}{\\partial \\hat{Y}}\\frac{\\partial \\hat{Y}}{\\partial \\mathbf{g}} = (\\hat{Y}-Y)\\hat{Y}(1-\\hat{Y}) \\\\\nQ_1 & \\equiv \\frac{\\partial L}{\\partial \\hat{Y}}\\frac{\\partial \\hat{Y}}{\\partial \\mathbf{g}}\\frac{\\partial \\mathbf{g}}{\\partial \\hat{\\mathbf{h}}}\\frac{\\partial \\hat{\\mathbf{h}}}{\\partial \\mathbf{h}} = Q_2 W_{2}\\hat{\\mathbf{h}}(1-\\hat{\\mathbf{h}})\n\\end{split}\n\\end{equation}\\]\n\n\n\n\n\n\n\n\nExercise\n\n\n\nShow that update rules for weights \\(W_1\\) and \\(W_2\\) are\n\\(W_2 = W_2 - \\frac{\\eta}{n}\\hat{\\mathbf{h}}^TQ_2\\)\n\\(B_1 = B_1 - \\frac{\\eta}{n}Q_1\\)\n\\(W_1 = W_1 - \\frac{\\eta}{n}X^TQ_1\\)\nHint 1: Operations in \\((\\hat{Y}-Y)Y(1-Y)\\) are element-wise multiplications.\nHint 2: Operations in \\(\\hat{\\mathbf{h}}(1-\\hat{\\mathbf{h}})\\) are element-wise multiplications.\nHint 3: The resulting weight updates must have the same dimension as the weight matrices."
  },
  {
    "objectID": "course/deep_learning/neural_networks_from_scratch.html#universal-approximation-theorem",
    "href": "course/deep_learning/neural_networks_from_scratch.html#universal-approximation-theorem",
    "title": "NN from scratch",
    "section": "2.2 Universal approximation theorem",
    "text": "2.2 Universal approximation theorem\nWhile the previous may seem a trivial model, it has been proven that this model is indeed a universal approximator. In particular, the following theorem proves it (informal version):\n\nA neural network with a single hidden layer and enough hidden neurons, using a suitable nonlinear activation function, can approximate any continuous function on a compact domain (for example, any function on \\([0,1]^n\\)) as closely as we want.\n\nThis means that in principle we could use the previous model for anything, even replicate an LLM. But of course, the previous is a theoretical result, an such a model is not practical: the width would be enourmous and training it could be a nightmare… We will see how to remedy this later this course. For now, let’s see how to train such model with python."
  },
  {
    "objectID": "course/deep_learning/neural_networks_from_scratch.html#activation-functions",
    "href": "course/deep_learning/neural_networks_from_scratch.html#activation-functions",
    "title": "NN from scratch",
    "section": "3.1 Activation functions",
    "text": "3.1 Activation functions\nSo far we have been using softmax \\(\\sigma(z) = \\frac{1}{1+e^{-x}}\\) activation function only. The other activation functions are:\n\n\n\n0_lo8wlkwReDcXkts0.png\n\n\nLoss function should be calculated accordignly to the given activation function!"
  },
  {
    "objectID": "course/deep_learning/neural_networks_from_scratch.html#stochastic-gradient-descent-sgd",
    "href": "course/deep_learning/neural_networks_from_scratch.html#stochastic-gradient-descent-sgd",
    "title": "NN from scratch",
    "section": "4.1 Stochastic gradient descent (SGD)",
    "text": "4.1 Stochastic gradient descent (SGD)\nIn the standard gradient descent, we compute the gradient of the cost function with respect to the parameters for the entire training dataset. In most cases, it is extremely slow and even intractable for datasets that don’t even fit in memory. It also doesn’t allow us to update our model online, i.e. with new examples on-the-fly.\nIn SGD gradient descent, we use mini-batches comprised of a few training samples, and the model’s parameters are updated based on the average loss across the samples in each mini-batch. This way, SGD is able to make faster progress through the training dataset, and it can also make use of vectorized operations, which can make the training process more efficient."
  },
  {
    "objectID": "course/deep_learning/neural_networks_from_scratch.html#momentum",
    "href": "course/deep_learning/neural_networks_from_scratch.html#momentum",
    "title": "NN from scratch",
    "section": "4.2 Momentum",
    "text": "4.2 Momentum\nMomentum optimization is an algorithm that can be used to improve SGD. It works by adding a fraction \\(\\gamma\\) of the previous parameter update to the current one, which helps the model make faster progress in the right direction and avoid getting stuck in local minima. This fraction is called the momentum coefficient, and it is a hyperparameter that can be adjusted according to the problem.\nThe momentum algorithm accumulates a history of the past gradients and continues to move in their direction:\n\\[\\begin{equation}\n\\begin{split}\ng_t &=  \\frac{\\partial L(\\theta_{t-1})}{\\partial \\theta}\\\\\nv_t &= \\gamma v_{t-1} - \\eta g_t \\\\\n\\theta &= \\theta + v_t,\n\\end{split}\n\\end{equation}\\] where \\(t\\) enumerates training epoch, \\(\\theta\\) are the trainable parameters of the Neural Network, \\(\\gamma\\) is the momentum coefficient and \\(\\eta\\) is the learning rate.\nThe velocity \\(v\\) accumulates the gradient of the loss function \\(L\\); the larger \\(\\gamma\\) with respect to \\(\\eta\\), the more previous gradients affect the current direction. In the standard SGD algorithm, the update size depended on the gradient and the learning rate. With momentum, it also depends on how large and how aligned consecutive gradients are. In addition to speeding up training, momentum optimization can also help the model to generalize better to new data."
  },
  {
    "objectID": "course/deep_learning/neural_networks_from_scratch.html#adaptative-gradient-adagrad",
    "href": "course/deep_learning/neural_networks_from_scratch.html#adaptative-gradient-adagrad",
    "title": "NN from scratch",
    "section": "4.3 Adaptative Gradient (Adagrad)",
    "text": "4.3 Adaptative Gradient (Adagrad)\nAdaptative Gradient algorithm (Duchi, Hazan, and Singer 2011) is based on the idea of adapting the learning rate to the parameters, performing larger updates for infrequent and smaller updates for frequent parameters.\nThe AdaGrad algorithm works by accumulating the squares of the gradients for each parameter, and then scaling the learning rate for each parameter by the inverse square root of this sum. This has the effect of reducing the learning rate for parameters that have been updated frequently, and increasing the learning rate for parameters that have been updated infrequently.\nThe update rule for AdaGrad algorithm reads\n\\[\\begin{equation}\n\\begin{split}\n\\theta_{t+1} & = \\theta_t + \\Delta\\theta,\n\\end{split}\n\\end{equation}\\]\n\\[\\begin{equation}\n\\begin{split}\n\\Delta \\theta &= - \\frac{\\eta}{\\sqrt{diag( \\epsilon\\mathbb{1} + G_t )}} \\odot g_t,\\\\\ng_t &= \\frac{\\partial L(\\theta_{t-1})}{\\partial \\theta}\\\\\nG_t &= \\sum_{\\tau = 1}^{t} g_\\tau g_\\tau^T.\n\\end{split}\n\\end{equation}\\] where \\(\\odot\\) means element-wise multiplication. The \\(\\epsilon \\ll 0\\) is a regularizing parameter, preventing from division by 0.\nAdagrad eliminates the need to manually tune the learning rate, i.e. initially \\(\\eta \\ll 1\\), and it is effectively adapted during training process. Algorithm is quite sensitive to the choice of the initial learning rate, and it may require careful tuning to achieve good results."
  },
  {
    "objectID": "course/deep_learning/neural_networks_from_scratch.html#adaptive-moment-estimation-adam",
    "href": "course/deep_learning/neural_networks_from_scratch.html#adaptive-moment-estimation-adam",
    "title": "NN from scratch",
    "section": "4.4 Adaptive Moment Estimation (Adam)",
    "text": "4.4 Adaptive Moment Estimation (Adam)\nAdam algorithm (Kingma and Ba 2014) combines the ideas of momentum optimization and Adagrad to make more stable updates and achieve faster convergence.\nLike momentum optimization, Adam uses an exponentially decaying average of the previous gradients to determine the direction of the update. This helps the model to make faster progress in the right direction and avoid oscillations. Like AdaGrad, Adam also scales the learning rate for each parameter based on the inverse square root of an exponentially decaying average of the squared gradients. This has the effect of reducing the learning rate for parameters that have been updated frequently, and increasing the learning rate for parameters that have been updated infrequently.\nAdam uses Exponentially Modified Moving Average for gradients and its square:\n\\[\\begin{equation}\n\\begin{split}\ng_t &= \\frac{\\partial L(\\theta_{t-1})}{\\partial \\theta}\\\\\nm_t &= \\beta_1 m_{t-1} + (1-\\beta_1)g_t \\\\\nv_t &= \\beta_2 v_{t-1} + (1-\\beta_2)g_t^2.\n\\end{split}\n\\end{equation}\\]\nThe update rule for the parameters reads:\n\\[\\begin{equation}\n\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon}\\hat{m}_t,\n\\end{equation}\\] where \\[\\begin{equation}\n\\begin{split}\n\\hat{m}_t &= \\frac{m_t}{1-\\beta^t_1}\\\\\n\\hat{v}_t &= \\frac{v_t}{1-\\beta^t_2},\n\\end{split}\n\\end{equation}\\] are bias-corrected first and second gradient moments estimates.\nAuthors suggest to set \\(\\beta_1 = 0.9\\), \\(\\beta_2 = 0.999\\), \\(\\eta = 10^{-8}\\).\n\n\n\n\n\n\nExercise\n\n\n\nBased on the full gradient loop we used above, implement the SGD algorithm over the MNIST dataset. A reminder of what the code should look like:\n\nSet batch size, learning rate, epochs and any other training hyperparameter.\nLoop over training epochs:\n\nLoop over number of batches:\n\nSample a random batch from the training set. For instance: batch = X_train[np.random.randint(0, X_train.shape[0], batch_size)]\nForward pass, backward, network update.\n\nTrack the test and training loss\n\n\nBonus: Implement the Adam optimizer in the previous loop."
  },
  {
    "objectID": "course/deep_learning/regularization_techniques.html#weight-decay-l1-and-l2-regularization",
    "href": "course/deep_learning/regularization_techniques.html#weight-decay-l1-and-l2-regularization",
    "title": "NN regularization",
    "section": "2.1 Weight decay (L1 and L2 regularization)",
    "text": "2.1 Weight decay (L1 and L2 regularization)\nWe already say this method, when performing polynomial regression, although in the context of neural networks we typically refer to it as weight decay rather than weight regularization. As we saw, this method involves adding a penalty to the cost function during training to discourage the model from learning excessively large weights. These regularization techniques are based on the idea that large weights can lead to overfitting, as they may allow the model to fit the training data too closely. L1 and L2 regularization are methods for adding a penalty term to the cost function during training to discourage the model from learning excessively large weights. L1 regularization:\n\n2.1.1 L1 regularization\nL1 regularization, also known as Lasso regularization, adds a penalty term to the cost function that is proportional to the absolute value of the weights. The L1 regularization term has the form:\n\\[\\begin{equation}\nL_1 = \\lambda  \\sum |W|\n\\end{equation}\\] where \\(\\lambda\\) is the regularization parameter, and \\(W\\) is the weight matrix.\nThe effect of L1 regularization is to push the weights towards zero, which can help to reduce overfitting by decreasing the complexity of the model. L1 regularization can also be useful for feature selection, as it tends to drive the weights of unimportant features to zero, effectively removing them from the model.\nNow that we know pytorch, let’s use it to create our own loss function!\n\nclass LassoLoss(nn.Module):\n    def __init__(self, base_criterion: nn.Module, model: nn.Module, l1_lambda: float = 1e-4):\n        super().__init__()\n        \n        self.base = base_criterion\n        \n        self.params = [p for n,p in model.named_parameters() if p.requires_grad and not n.endswith(\".bias\") and p.ndim > 1]\n        \n        self.l1_lambda = l1_lambda\n\n    def forward(self, outputs, targets):\n        base_loss = self.base(outputs, targets)\n        l1 = torch.zeros((), device=base_loss.device)\n        for p in self.params:\n            l1 = l1 + p.abs().sum()\n        return base_loss + self.l1_lambda * l1\n\n\nn_epochs = 10\nmodel = FullyConnected(28 * 28, 500, 10).to(DEVICE)\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n\nloss_lasso = LassoLoss(base_criterion = torch.nn.CrossEntropyLoss(),\n                       model = model,\n                       l1_lambda = 1e-4)\n\ntrained_model_lasso = training_loop(model, loss_lasso, n_epochs, train_loader, val_loader, device = DEVICE)\n\n\n\n\nValidation Loss: 1.0706, Accuracy: 0.8966\n\n\n\n\n\n\n\n\nExercise\n\n\n\nFrom the previous we can see that the validation loss is bigger compared to the previous training, although accuracy is comparable. What happened?\n\n\nBeyond our purpose of improving our validation accuracy, our goal was to reduce the average value of the weights. Did we succeed? Let’s take a look a it:\n\nweight_og_model = torch.concatenate([l for l in [trained_model.linear_1.weight.flatten(), \n                                                 trained_model.linear_1.weight.flatten()]]).detach().cpu()\n\nweight_lasso_model = torch.concatenate([l for l in [trained_model_lasso.linear_1.weight.flatten(), \n                                                    trained_model_lasso.linear_1.weight.flatten()]]).detach().cpu()\n\n\nplt.hist(weight_og_model, bins = 100, label = f'OG model - Avg. = {weight_og_model.abs().mean():.4f}', density = True)\nplt.hist(weight_lasso_model, bins = 100, label = f'Lasso model - Avg. = {weight_lasso_model.abs().mean():.4f}', alpha = 0.4, density = True)\nplt.legend()\n\n;\n\n''\n\n\n\n\n\n\n\n2.1.2 L2 regularization\nL2 regularization, also known as \\({\\it Ridge}\\) regularization, adds a penalty term to the cost function that is proportional to the square of the weights. The L2 regularization term has the form:\n\\[\\begin{equation}\nL_2 = \\lambda  \\sum W^2\n\\end{equation}\\]\nwhere again \\(\\lambda\\) is the regularization parameter, and \\(W\\) are weights of the model.\nThe effect of L2 regularization is the similar to the L1 one: decrease the weights towards zero, which can help to reduce overfitting by decreasing the complexity of the model.\nHowever, unlike L1 regularization, L2 regularization does not lead to the complete removal of weights, as it only shrinks the weights rather than setting them to zero.\nIn general, L2 regularization is more commonly used than L1 regularization, as it tends to be more stable and easier to optimize. However, L1 regularization can be useful in situations where it is important to select a subset of features, as it has the ability to drive some weights to zero. Let’s see that in code:\n\nclass RidgeLoss(nn.Module):\n    def __init__(self, base_criterion: nn.Module, model: nn.Module, l1_lambda: float = 1e-4):\n        super().__init__()\n        \n        self.base = base_criterion\n        \n        self.params = [p for n,p in model.named_parameters() if p.requires_grad and not n.endswith(\".bias\") and p.ndim > 1]\n        \n        self.l1_lambda = l1_lambda\n\n    def forward(self, outputs, targets):\n        base_loss = self.base(outputs, targets)\n        l1 = torch.zeros((), device=base_loss.device)\n        for p in self.params:\n            l1 = l1 + p.square().sum()\n        return base_loss + self.l1_lambda * l1\n\n\nn_epochs = 10\nmodel = FullyConnected(28 * 28, 500, 10).to(DEVICE)\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n\nloss_ridge = RidgeLoss(base_criterion = torch.nn.CrossEntropyLoss(),\n                       model = model,\n                       l1_lambda = 1e-4)\n\ntrained_model_ridge = training_loop(model, loss_lasso, n_epochs, train_loader, val_loader, device = DEVICE)\n\n\n\n\nValidation Loss: 1.0605, Accuracy: 0.8962\n\n\n\nweight_og_model = torch.concatenate([l for l in [trained_model.linear_1.weight.flatten(), \n                                                 trained_model.linear_1.weight.flatten()]]).detach().cpu()\n\nweight_ridge_model = torch.concatenate([l for l in [trained_model_ridge.linear_1.weight.flatten(), \n                                                    trained_model_ridge.linear_1.weight.flatten()]]).detach().cpu()\n\n\nplt.hist(weight_og_model, bins = 100, label = f'OG model - Avg. = {weight_og_model.abs().mean():.4f}', density = True)\nplt.hist(weight_ridge_model, bins = 100, label = f'Lasso model - Avg. = {weight_ridge_model.abs().mean():.4f}', alpha = 0.4, density = True)\nplt.legend()\n\n;\n\n''\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nAs we can see in the previous plot, not much change… Increase the \\(\\lambda\\) parameter of the Ridge regularization and see what happens with: 1) the loss; 2) the weights.\n\n\n\n\n\n\n\n\nExercise\n\n\n\nLet’s come up with a completely new loss function: implement both the Ridge and Lasso regularizations but with a twist: the former will only apply to the first layer and the former to the second. Use the same loss function construction as above.\n\n\n\n### Your code here\n\n\n## Rather than training your model, which may take some time, you can test the loss on a non-trained model:\n\n# We first get a batch\nimages, targets = next(iter(train_loader))\n\n# Define the model and loss\nmodel = FullyConnected(28 * 28, 500, 10).to(DEVICE)\nloss_mixed = MixedL1L2Loss(base_criterion = nn.CrossEntropyLoss(), model = model, l1_lambda=1e-4, l2_lambda=1e-3)\n\n# Do forward pass\noutputs = model(images.to(DEVICE))\n\n# Compute the loss\nloss_mixed(outputs, targets.to(DEVICE))\n\ntensor(3.0156, device='cuda:0', grad_fn=<AddBackward0>)"
  },
  {
    "objectID": "course/deep_learning/regularization_techniques.html#dropout",
    "href": "course/deep_learning/regularization_techniques.html#dropout",
    "title": "NN regularization",
    "section": "2.2 Dropout",
    "text": "2.2 Dropout\nAnother popular technique to prevent overfitting in neural networks is dropout (Hinton et al. 2012) While ridge and lasso constrain the magnitude of model parameters, dropout acts directly on the network’s activations: during training, each neuron is randomly “dropped out” (i.e., temporarily set to zero) with a fixed probability. This prevents the network from relying too heavily on any single neuron or feature and encourages the development of redundant, robust representations. At inference time, all neurons are active, but their outputs are scaled to account for the missing activations during training. In essence, dropout can be viewed as a stochastic regularizer that approximates training an ensemble of many smaller subnetworks, improving generalization without adding explicit parameter penalties.\nOppose to what regularization through L1 or L2, dropout acts directly on the model. Let’s see how to do this in pytorch:\n\nclass FullyConnected_dropout(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, dropout_prob = 0.2):\n        super().__init__()\n        self.linear_1 = nn.Linear(input_size, hidden_size)\n        self.linear_2 = nn.Linear(hidden_size, output_size, bias=False)\n\n        self.dropout = nn.Dropout(p = dropout_prob)\n        \n    def forward(self, x):\n        x = x.reshape(-1, 28 * 28)\n        z = self.linear_1(x)\n        x = F.relu(z)\n        # After computing, we apply the dropout layer:\n        x = self.dropout(x)\n        z = self.linear_2(x)\n        return z\n\nLet’s now train the new model:\n\nn_epochs = 10\nmodel = FullyConnected_dropout(28 * 28, 500, 10, dropout_prob=0.2).to(DEVICE)\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n\ntrained_model_dropout = training_loop(model, criterion, n_epochs, train_loader, val_loader, device = DEVICE)\n\n\n\n\nValidation Loss: 0.4138, Accuracy: 0.8830\n\n\n\n\n\n\n\n\nTip\n\n\n\nVery important: dropout is only useful during training. During inference, we don’t want our neurons to randomly shut down, as this could affect the accuracy of the output. To solve this, pytorch’s models have to modes: model.train(), which sets the model’s layers into training mode (e.g. dropout is considered); or model.eval(), which sets the model to inference mode, so dropout and other layer properties are not considered.\n\n\nLet’s see the difference between the two modes:\n\nfor i in range(3):\n    pred = trained_model_dropout(next(iter(val_loader))[0].to(DEVICE))\n    print(pred[:4,:4])\n\ntensor([[ 0.5062, -4.7286, -1.7473,  0.6674],\n        [-1.5978, -1.3038,  2.4630,  6.5667],\n        [ 2.2861, -5.4669,  5.9600,  4.6127],\n        [-2.7885,  2.0934, -1.2175,  0.7194]], device='cuda:0',\n       grad_fn=<SliceBackward0>)\ntensor([[ 0.4503, -4.5496, -1.8042,  0.8437],\n        [-0.0629, -2.2050,  1.7746,  7.6969],\n        [ 0.3124, -4.2750,  5.9855,  4.3752],\n        [-2.8338,  1.8375, -1.4099,  0.6517]], device='cuda:0',\n       grad_fn=<SliceBackward0>)\ntensor([[-0.3726, -3.7200, -1.7582,  0.7903],\n        [ 0.5540, -2.1269,  1.7473,  5.9036],\n        [ 1.4706, -4.9557,  5.8581,  4.6562],\n        [-3.0979,  1.9791, -1.6882,  1.1849]], device='cuda:0',\n       grad_fn=<SliceBackward0>)\n\n\n\ntrained_model_dropout.eval()\nfor i in range(3):\n    pred = trained_model_dropout(next(iter(val_loader))[0].to(DEVICE))\n    print(pred[:4,:4])\n\ntensor([[ 0.3722, -4.6152, -1.9534,  0.9229],\n        [-0.7638, -1.5321,  1.8037,  7.0875],\n        [ 1.3205, -4.8303,  6.0047,  4.7763],\n        [-3.0167,  2.0212, -1.4367,  0.9608]], device='cuda:0',\n       grad_fn=<SliceBackward0>)\ntensor([[ 0.3722, -4.6152, -1.9534,  0.9229],\n        [-0.7638, -1.5321,  1.8037,  7.0875],\n        [ 1.3205, -4.8303,  6.0047,  4.7763],\n        [-3.0167,  2.0212, -1.4367,  0.9608]], device='cuda:0',\n       grad_fn=<SliceBackward0>)\ntensor([[ 0.3722, -4.6152, -1.9534,  0.9229],\n        [-0.7638, -1.5321,  1.8037,  7.0875],\n        [ 1.3205, -4.8303,  6.0047,  4.7763],\n        [-3.0167,  2.0212, -1.4367,  0.9608]], device='cuda:0',\n       grad_fn=<SliceBackward0>)\n\n\nAs you can see, but setting the model into evaluation mode we transform it into a deterministic model, just as expect!"
  },
  {
    "objectID": "course/deep_learning/regularization_techniques.html#batch-normalization",
    "href": "course/deep_learning/regularization_techniques.html#batch-normalization",
    "title": "NN regularization",
    "section": "2.3 Batch normalization:",
    "text": "2.3 Batch normalization:\nBatch normalization (Ioffe and Szegedy 2015) is a technique that normalizes the activations of each mini-batch to stabilize and speed up the training of deep neural networks.\nInstead of computing normalization statistics over the entire training set, which would be impractical during stochastic optimization, batch normalization operates on each mini-batch. This helps keep the distribution of activations consistent across layers and reduces overfitting.\nDuring training, for each mini-batch, the layer computes the mean and standard deviation of the activations and normalizes them as:\n\\[\\begin{equation}\n\\hat{x} = \\frac{x - \\mu_{\\text{batch}}}{\\sigma_{\\text{batch}}}\n\\end{equation}\\]\nwhere \\(\\mu_{\\text{batch}}\\) and \\(\\sigma_{\\text{batch}}\\) are the mean and standard deviation of the mini-batch activations.\nThe layer maintains running averages of these statistics, which are updated during training. At evaluation time, these running averages are used instead of the batch statistics, ensuring stable behavior and consistent outputs between training and inference. It is hence very important to set model.eval(), because if not the result will change depending on, for instance, the input size!\nIn pytorch, batch normalization is implemented as a layer, similar to what we did with Dropout, although here we will also need to specifiy the input size:\n\nclass FullyConnected_batchnorm(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, dropout_p=0.5):\n        super().__init__()\n        self.linear_1 = nn.Linear(input_size, hidden_size)\n        self.bn_1     = nn.BatchNorm1d(hidden_size)   # batch normalization\n        self.linear_2 = nn.Linear(hidden_size, output_size, bias=False)\n\n    def forward(self, x):\n        x = x.reshape(x.size(0), -1)          \n        x = self.linear_1(x)\n        # We now implement the batch normalization to the activations (i.e. before the activation function!)\n        x = self.bn_1(x)                 \n        x = F.relu(x)\n        x = self.linear_2(x)\n        return x\n\n\n# import torch\n\n# Train the model for a maximum of 100 epochs\n#for epoch in range(100):\n  # Train the model for one epoch\n#  train(model, train_data, optimizer)\n\n  # Evaluate the model on the validation set\n#  val_loss = evaluate(model, val_data)\n\n  # If the validation loss has not improved in the last 10 epochs, stop training\n#  if val_loss > best_val_loss:\n#    best_val_loss = val_loss\n#    patience = 0\n#  else:\n#    patience += 1\n#    if patience == 10:\n#      break"
  },
  {
    "objectID": "course/deep_learning/nn_with_pytorch.html#the-magic-behind-gradients",
    "href": "course/deep_learning/nn_with_pytorch.html#the-magic-behind-gradients",
    "title": "NN with PyTorch",
    "section": "1.1 The magic behind gradients",
    "text": "1.1 The magic behind gradients\nWhen we set requires_grad=True, PyTorch builds a computational graph that records every operation we perform. When we call .backward(), PyTorch traverses this graph in reverse, from the object we call it from to the beginning, applying the chain rule from calculus to compute exact derivatives (not numerical approximations!).\nLet’s see this by repeating the previous example, although this way we’ll do it step-by-step in order to make everything very explicit and easy to visualize.\n\n# Let's trace through the computation step by step\nx = 2 * a_tensor\ny = x + b_tensor\nz = torch.sum(y)  # This is the previous `result` \n\nz.backward()\n\nThe computational graph looks like:\n  a_tensor → [mul by 2] → x ────┐\n                                ├→ [add] → y → [sum] → z\n  b_tensor ─────────────────────┘\nWhen we call z.backward(), PyTorch traverses the graph backwards to compute the derivatives:\n\n\\(\\frac{dz}{dy} = [1, 1, 1]\\) (derivative of sum)\n\\(\\frac{dy}{dx} = [1, 1, 1]\\) (derivative of addition w.r.t a)\n\\(\\frac{dy}{db} = [1, 1, 1]\\) (derivative of addition w.r.t. b)\n\\(\\frac{dx}{da} = [2, 2, 2]\\) (derivative of multiplication by 2)\n\nBy chain rule: \\(\\frac{dz}{da} = \\frac{dz}{dy}\\frac{dy}{dx}\\frac{dx}{da} = [2, 2, 2]\\) and \\(\\frac{dz}{db} = \\frac{dz}{dy}\\frac{dy}{db} = [1, 1, 1]\\)\n\nfrom torchviz import make_dot\n\nz = torch.sum(2 * a_tensor + b_tensor)\nmake_dot(z, params={'a': a_tensor, 'b': b_tensor})"
  },
  {
    "objectID": "course/deep_learning/nn_with_pytorch.html#task-and-data",
    "href": "course/deep_learning/nn_with_pytorch.html#task-and-data",
    "title": "NN with PyTorch",
    "section": "2.1 Task and data",
    "text": "2.1 Task and data\nLet’s start by the task and the data. We will use again the MNIST dataset, which is composed of hand-written digit images from 0 to 9. The task will be to classify those images into their respective digits.\n\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\nfrom torch.utils.data import DataLoader, random_split\n\ntorch.manual_seed(7)\n\n\nmnist_train = MNIST(root=\"data\", train=True, download=True, transform=ToTensor())\nmnist_test = MNIST(root=\"data\", train=False, download=True, transform=ToTensor())\n\nprint(mnist_train)\nprint(mnist_test)\n\nDataset MNIST\n    Number of datapoints: 60000\n    Root location: data\n    Split: Train\n    StandardTransform\nTransform: ToTensor()\nDataset MNIST\n    Number of datapoints: 10000\n    Root location: data\n    Split: Test\n    StandardTransform\nTransform: ToTensor()\n\n\nIn machine learning, it is very important that we become familiar with the data that we are dealing with. In this case, we may plot some example images.\n\n\nCode\nimport matplotlib.pyplot as plt\n\ndef plot_image(ax, image: torch.Tensor, label: int | None = None):\n    \"Plot a single image.\"\n    ax.imshow(image.squeeze(), cmap=\"gray\")\n    if label is not None:\n        ax.set_title(f\"Pred: {label}\")\n\ndef plot_examples(dataset):\n    \"Plot 5 examples from the MNIST dataset.\"\n    _, axes = plt.subplots(1, 5, figsize=(12, 3))\n    for i, ax in enumerate(axes):\n        image, label = dataset[i]\n        plot_image(ax, image)\n    plt.show()\n\nplot_examples(mnist_train)\n\n\n\n\n\nThe images are \\(28 \\times 28\\) pixels in grayscale, and the labels are a single scalar.\n\nimage, label = mnist_train[0]\nimage.shape, label\n\n(torch.Size([1, 28, 28]), 5)\n\n\nNow let’s split the training set into training and validation. This will allow us to evaluate the model’s generalization capabilities during training and tune its hyper-parameters.\n\ntrain_data, validation_data = random_split(mnist_train, [55000, 5000])\n\nFinally, we will create the data loaders for the training, validation, and testing data sets. These objects will take care of spliting the data into batches, given that 60000 images may be too much to process at once.\n\nbatch_size = 128\ntrain_loader = DataLoader(train_data, batch_size, shuffle=True)\nval_loader = DataLoader(validation_data, batch_size, shuffle=False)\ntest_loader = DataLoader(mnist_test, batch_size, shuffle=False)"
  },
  {
    "objectID": "course/deep_learning/nn_with_pytorch.html#performance-measure",
    "href": "course/deep_learning/nn_with_pytorch.html#performance-measure",
    "title": "NN with PyTorch",
    "section": "2.2 Performance measure",
    "text": "2.2 Performance measure\nOpposite to what we did in the previous notebook, where we considered a simplified regression scenario with an MSE loss, we will here properly set a classification problem with ten classes (digits from 0 to 9). Therefore, we will use the cross-entropy loss function \\[\\mathcal{L}_{\\text{CE}} = -\\frac{1}{n}\\sum_i^n \\mathbf{y}_i^T\\log(f(\\mathbf{x}_i))\\,,\\] where \\(\\mathbf{y}_i\\) is the one-hot-encoding vector of the true label, and \\(f(\\mathbf{x}_i)\\) provides the predicted probability for sample \\(\\mathbf{x}_i\\) to belong to each of the classes.\n\ndef cross_entropy_loss(predictions, targets):\n    \"\"\"Compute the cross-entropy loss between predictions and targets for a given batch.\"\"\"\n    target_preds = predictions[torch.arange(len(predictions)), targets]\n    return -torch.mean(torch.log(target_preds))\n\nBesides the loss function, we can compute other performance indicators that may not need to be differentiable, like the accuracy or the error rate.\n\ndef accuracy(predictions, targets):\n    \"\"\"Compute the accuracy of predictions given the true targets.\"\"\"\n    return (predictions.argmax(dim=1) == targets).float().mean()"
  },
  {
    "objectID": "course/deep_learning/nn_with_pytorch.html#model",
    "href": "course/deep_learning/nn_with_pytorch.html#model",
    "title": "NN with PyTorch",
    "section": "2.3 Model",
    "text": "2.3 Model\nThe last ingredient for our learning task is a model that will encode the program to solve the task. In this case, we will start with a simple fully-connected neural network. In these networks, we distinguish between three types of layers:\n\nThe input layer contains the data values. In this case, it will be the pixel values.\nThe output layer contains the desired output. In this case, the probability for each class.\nThe hidden layers are all the layers between the input and output layers.\n\nIndividual neurons perform simple calculations based on the signal received from by the neurons from the preceding layer. Typically, they perform a linear transformation followed by a non-linear activation function \\(\\xi\\) of the form.\n\\[\\begin{split}\n    z &= \\mathbf{\\omega}^T \\mathbf{x} + b = \\sum_i \\omega_i x_i + b\\\\\n    x &= \\xi(z)\\,.\n\\end{split}\\]\nHere, \\(\\mathbf{x}\\) denotes the activations of the neurons in the preceding layer, and the connection strength between each of those neurons is encoded in the weight vector \\(\\mathbf{\\omega}\\). The neuron incorporates a bias \\(b\\), and the resulting value of the linear transformation \\(z\\) is known as the logit. Finally, the resulting activation of the neuron \\(x\\) is determined by applying the non-linear activation function \\(\\xi\\).\nWe will start by initializing the parameters for our linear operations.\n\ninput_size = 28 * 28\nhidden_size = 500\nn_classes = 10\n\n# Input to hidden\nW1 = torch.randn(input_size, hidden_size) / torch.sqrt(torch.tensor(input_size))\nW1.requires_grad_()\nb = torch.zeros(hidden_size, requires_grad=True)\n\n# Hidden to output\nW2 = torch.randn(hidden_size, n_classes) / torch.sqrt(torch.tensor(hidden_size))\nW2.requires_grad_();\n\nThe activation functions can take any form, so long as it is non-linear, and they can be used to obtain the desired output. In this case, we will use the rectified linear unit (ReLU) activation function in the hidden layer \\[\\text{ReLU}(z) = \\max(0, z)\\,,\\] and a softmax activation function in the output layer to normalize the logits as a probability distribution \\[\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_k e^{z_k}}\\,.\\]\n\ndef relu(x):\n    \"Rectified linear unit activation function.\"\n    return torch.maximum(x, torch.tensor(0.0))\n\ndef softmax(x):\n    \"Softmax activation function.\"\n    return torch.exp(x) / torch.exp(x).sum(axis=-1, keepdim=True)\n\nNow we can define our model.\n\ndef model(x):\n    \"Neural network model.\"\n    x = x.reshape(-1, 28 * 28)  # Flatten the image\n    z = x @ W1 + b  # First linear transformation\n    x = relu(z)  # Hidden layer activation\n    z = x @ W2  # Second linear transformation\n    return softmax(z)  # Output layer activation"
  },
  {
    "objectID": "course/deep_learning/nn_with_pytorch.html#training",
    "href": "course/deep_learning/nn_with_pytorch.html#training",
    "title": "NN with PyTorch",
    "section": "2.4 Training",
    "text": "2.4 Training\nWe have all the necessary ingredients to train a machine learning model for digit recognition. Let’s put everything together in a training loop.\nThe typical learning procedure is:\n\nFor every training batch\n\nEvaluate the model\nCompute the loss\nCompute the gradients of the parameters\nUpdate the parameters\n\nFor every validation batch\n\nEvaluate the model\nCompute the loss\n\nRepeat 1 and 2 for every training epoch\n\n\nfrom tqdm.auto import tqdm\n\nlearning_rate = 0.1\nn_epochs = 40\n\ntraining_loss = []\nvalidation_loss = []\n\nfor _ in tqdm(range(n_epochs)):\n    epoch_loss = 0\n    for images, labels in train_loader:\n        preds = model(images)\n        loss = cross_entropy_loss(preds, labels)\n        loss.backward()\n\n        # Now we perform the gradient descent step. We make sure torch does not compute any further gradient here.\n        with torch.no_grad():\n            # Update parameters\n            W1 -= W1.grad * learning_rate\n            b -= b.grad * learning_rate\n            W2 -= W2.grad * learning_rate\n            # Reset gradients\n            W1.grad.zero_()\n            b.grad.zero_()\n            W2.grad.zero_()\n\n        epoch_loss += loss.item()\n\n    training_loss.append(epoch_loss / len(train_loader))\n\n    # Computing the validation loss, we don't want any gradients computed here neither.\n    with torch.no_grad():\n        epoch_loss = 0\n        val_preds, val_targets = [], []\n        for images, labels in val_loader:\n            preds = model(images)\n            loss = cross_entropy_loss(preds, labels)\n            \n            epoch_loss += loss.item()\n            val_preds.append(preds)\n            val_targets.append(labels)\n\n        val_acc = accuracy(torch.cat(val_preds), torch.cat(val_targets))\n        validation_loss.append(epoch_loss / len(val_loader))\n\n    print(f\"Training Loss: {training_loss[-1]:.4f}, Validation Loss: {validation_loss[-1]:.4f}, Accuracy: {val_acc:.4f}\")\n\n\n\n\nTraining Loss: 0.4941, Validation Loss: 0.3212, Accuracy: 0.9180\nTraining Loss: 0.2675, Validation Loss: 0.2702, Accuracy: 0.9310\nTraining Loss: 0.2153, Validation Loss: 0.2213, Accuracy: 0.9418\nTraining Loss: 0.1807, Validation Loss: 0.1928, Accuracy: 0.9492\nTraining Loss: 0.1556, Validation Loss: 0.1698, Accuracy: 0.9578\nTraining Loss: 0.1365, Validation Loss: 0.1552, Accuracy: 0.9612\nTraining Loss: 0.1218, Validation Loss: 0.1428, Accuracy: 0.9612\nTraining Loss: 0.1096, Validation Loss: 0.1353, Accuracy: 0.9646\nTraining Loss: 0.1000, Validation Loss: 0.1259, Accuracy: 0.9674\nTraining Loss: 0.0914, Validation Loss: 0.1155, Accuracy: 0.9670\nTraining Loss: 0.0838, Validation Loss: 0.1101, Accuracy: 0.9698\nTraining Loss: 0.0775, Validation Loss: 0.1039, Accuracy: 0.9716\nTraining Loss: 0.0718, Validation Loss: 0.1006, Accuracy: 0.9724\nTraining Loss: 0.0668, Validation Loss: 0.0953, Accuracy: 0.9724\nTraining Loss: 0.0625, Validation Loss: 0.0948, Accuracy: 0.9736\nTraining Loss: 0.0585, Validation Loss: 0.0897, Accuracy: 0.9748\nTraining Loss: 0.0548, Validation Loss: 0.0868, Accuracy: 0.9758\nTraining Loss: 0.0514, Validation Loss: 0.0841, Accuracy: 0.9742\nTraining Loss: 0.0486, Validation Loss: 0.0817, Accuracy: 0.9756\nTraining Loss: 0.0456, Validation Loss: 0.0829, Accuracy: 0.9764\nTraining Loss: 0.0431, Validation Loss: 0.0819, Accuracy: 0.9768\nTraining Loss: 0.0408, Validation Loss: 0.0815, Accuracy: 0.9754\nTraining Loss: 0.0386, Validation Loss: 0.0800, Accuracy: 0.9760\nTraining Loss: 0.0367, Validation Loss: 0.0785, Accuracy: 0.9762\nTraining Loss: 0.0346, Validation Loss: 0.0805, Accuracy: 0.9752\nTraining Loss: 0.0329, Validation Loss: 0.0763, Accuracy: 0.9766\nTraining Loss: 0.0313, Validation Loss: 0.0753, Accuracy: 0.9774\nTraining Loss: 0.0297, Validation Loss: 0.0728, Accuracy: 0.9770\nTraining Loss: 0.0282, Validation Loss: 0.0726, Accuracy: 0.9782\nTraining Loss: 0.0272, Validation Loss: 0.0716, Accuracy: 0.9786\nTraining Loss: 0.0257, Validation Loss: 0.0709, Accuracy: 0.9784\nTraining Loss: 0.0245, Validation Loss: 0.0713, Accuracy: 0.9790\nTraining Loss: 0.0234, Validation Loss: 0.0719, Accuracy: 0.9798\nTraining Loss: 0.0224, Validation Loss: 0.0694, Accuracy: 0.9788\nTraining Loss: 0.0213, Validation Loss: 0.0707, Accuracy: 0.9798\nTraining Loss: 0.0203, Validation Loss: 0.0704, Accuracy: 0.9792\nTraining Loss: 0.0196, Validation Loss: 0.0693, Accuracy: 0.9786\nTraining Loss: 0.0187, Validation Loss: 0.0726, Accuracy: 0.9790\nTraining Loss: 0.0180, Validation Loss: 0.0684, Accuracy: 0.9792\nTraining Loss: 0.0172, Validation Loss: 0.0682, Accuracy: 0.9790\n\n\n\nplt.plot(training_loss, label=\"Training Loss\")\nplt.plot(validation_loss, label=\"Validation Loss\")\nplt.xlabel(\"Training epoch\")\nplt.ylabel(\"Cross-entropy loss\")\nplt.grid()\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7d6bcce99ee0>"
  },
  {
    "objectID": "installation_guide.html",
    "href": "installation_guide.html",
    "title": "Installation guide",
    "section": "",
    "text": "In this notebook, we will guide you through the installation of the necessary tools for this course."
  },
  {
    "objectID": "installation_guide.html#installing-python-and-jupyter-notebooks",
    "href": "installation_guide.html#installing-python-and-jupyter-notebooks",
    "title": "Installation guide",
    "section": "Installing Python and Jupyter notebooks",
    "text": "Installing Python and Jupyter notebooks\nThere are different options here, the recommended ones are Anaconda (see installation guide here) or Visual Studio Code (see guide here)."
  },
  {
    "objectID": "installation_guide.html#lectures_ml-library",
    "href": "installation_guide.html#lectures_ml-library",
    "title": "Installation guide",
    "section": "lectures_ml library",
    "text": "lectures_ml library\n\nNote: you can skip this step if you will be working via Google Colab\n\nFirst, we will install the library lectures_ml, which provides access to different functions and tools that will be used throughout our course. In your terminal (in case you are on Windows, make sure you are in the Anaconda terminal, not the Windows shell), run the following:\npip install git+https://github.com/BorjaRequena/Machine-Learning-Course.git\nThis may take a bit, as this will also install many other libraries (e.g. pytorch, scikit_learn, etc) that will be used during the course. To test that everything went fine, you can run the following cell. Note that you need to restart the kernel after installing the libraries for this cell to work:\n\n# Test cell, run this to see if the libraries are correctly installed. sklearn sometimes gives some errors.\n#If it didn't install correctly but the others did, install it via pip: pip install scikit-learn\n\ntry:\n    import lectures_ml\n    print('lectures_ml good!')\nexcept ImportError:\n    raise AssertionError(\"Library 'lectures_ml' is not installed.\")\n\ntry:\n    import torch\n    print('torch good!')\nexcept ImportError:\n    raise AssertionError(\"Library 'torch' is not installed.\")\n\ntry:\n    import sklearn\n    print('sklearn good!')\nexcept ImportError:\n    raise AssertionError(\"Library 'sklearn' is not installed. Install manually if needed via pip install scikit-learn\")\n\nlectures_ml good!\ntorch good!\nsklearn good!"
  },
  {
    "objectID": "installation_guide.html#ml4phys_uibk_w25-repo",
    "href": "installation_guide.html#ml4phys_uibk_w25-repo",
    "title": "Installation guide",
    "section": "ML4Phys_UIBK_W25 repo",
    "text": "ML4Phys_UIBK_W25 repo\nThis repository contains the notebooks that we will follow throughout the course. You have two ways of accessing it:\n\nIf you are planning to work via Google Colab, you can go to the course’s webpage and follow the links redirecting to Colab at the beginning of each section’s page.\nIf you will be working locally, clone the repository using:\n\ngit clone https://github.com/gorkamunoz/ML4Phys_UIBK_W25\n\nThis repository will be update regularly with each week’s course content. Hence, be sure to git pull the repository before each lecture.\n\nImportant: if you edit some of the notebooks (e.g. by doing the exercises), you will end up having some git conflicts. To avoid this, we recommend you to copy+paste this repo in a different folder, and call it e.g. ML4Phys_student. Use this new notebook to do the exercises, while keeping the original untouched. Follow the procedure above to pull from the original repo. Whenever a new notebook appears, paste it in your “student” folder. This is a “dirty” solution, git indeed gives you access to tools to this properly."
  },
  {
    "objectID": "installation_guide.html#fastai-library",
    "href": "installation_guide.html#fastai-library",
    "title": "Installation guide",
    "section": "fastai library",
    "text": "fastai library\nAside of the libraries in the requirements of lectures_ml, we will also need fastai. You can install it using pip install fastai."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning in Classical and Quantum Physics",
    "section": "",
    "text": "Welcome to the webpage of the ML for Physics Course at the UIBK. Here you will find most resources of the course, from explanatory notebooks to code snippets that will help us explore the wild world of ML."
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Machine Learning in Classical and Quantum Physics",
    "section": "Course description",
    "text": "Course description\nThis course gives an introduction to machine learning and deep learning: starting from linear linear models al the way up to state of the art generative models. The material covers the following topics:\n\nWhat is actually machine learning?\nBasics of ML: From linear models to logistic regression\nML applications: from computer vision to Physics\nBasics of deep learning: from neural networks to Transformers\nUnsupervised learning and interpretable ML\nReinforcement Learning\nGenerative modelling: from Boltzmann machines to diffusion models\n\nThe course combines theory and practice in the form of jupyter notebooks with python. We make extensive use of specific librairies such as numpy, PyTorch and fastai."
  },
  {
    "objectID": "index.html#evaluation",
    "href": "index.html#evaluation",
    "title": "Machine Learning in Classical and Quantum Physics",
    "section": "Evaluation",
    "text": "Evaluation\nHomeworks (50%)\nThere will be 3 homeworks, each equally contributing to the final mark. To know more about the homeworks, visit the Codabench platform. The day after the submission of each of the homeworks, your group will do a short presentation (5 mins.) about the methods you developed. The mark will calculated from your performance above the baseline (60%), the revision of the code + your presentations (35%) and an extra 5% based on your ranking’s position (first position gets the full 5% :), and we decrease linearly).\n\nImportant: to get accepted in the competition, your Codabench username must end with “_UIBK25”.\n\nFinal Project (30%)\nThe last weeks of the course you will work in groups on a final project, the topic of which will be made public at later stages. You will present your findings in a 20 minutes presentation in the last days of the course. The topics of the final projects will be decided later, based on the number of course participants.\nExam (20%)\nA short written exam, reviewing the main concepts taught in the course."
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "Machine Learning in Classical and Quantum Physics",
    "section": "Resources",
    "text": "Resources\n\nImportant: follow this installation guide to install the main resources of this course.\n\n\nSome of the content of this course has been adapted from the book Machine Learning for the Quantum Sciences by A. Dawid et al., which serves as a gentle introduction to ML but also to its applications in quantum sciences.\nThe book Neural Networks and Deep Learning by Nielsen offers a nice hands-on introduction to the world of ML\nIf you are already fluent in Python, the course Practical Deep Learning for Coders is for you. Indeed, we will extensively use some of the tools developed therein, as for instance the library fastai.\nFor the Reinforcement Learning part of this course, the book Reinforcement Learning: An Introduction is the go-to resource"
  },
  {
    "objectID": "index.html#previous-contributors",
    "href": "index.html#previous-contributors",
    "title": "Machine Learning in Classical and Quantum Physics",
    "section": "Previous contributors",
    "text": "Previous contributors\nPart of the content of this course was originally developed for by Borja Requena, Alexandre Dauphin, Marcin Płodzień and Paolo Stornati for the Master in Quantum Science and Technology Barcelona. The original course content can be found here."
  }
]